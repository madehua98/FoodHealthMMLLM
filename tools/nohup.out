import nvidia_smi failedimport nvidia_smi failedimport nvidia_smi failed

import nvidia_smi failedimport nvidia_smi failedimport nvidia_smi failedimport nvidia_smi failed

import nvidia_smi failed



True
True
True
True
False
True
True
True
1
1
1
1
1
1
1
0
/ML-A800/home/guoshuyue/anaconda3/envs/datacomp1/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ML-A800/home/guoshuyue/anaconda3/envs/datacomp1/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ML-A800/home/guoshuyue/anaconda3/envs/datacomp1/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ML-A800/home/guoshuyue/anaconda3/envs/datacomp1/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ML-A800/home/guoshuyue/anaconda3/envs/datacomp1/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ML-A800/home/guoshuyue/anaconda3/envs/datacomp1/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ML-A800/home/guoshuyue/anaconda3/envs/datacomp1/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ML-A800/home/guoshuyue/anaconda3/envs/datacomp1/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/madehua/code/FoodHealthMMLLM/tools/img_filter_food.py", line 122, in <module>
    checkpoint = torch.load(model_load_path)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/datacomp1/lib/python3.10/site-packages/torch/serialization.py", line 1026, in load
    return _load(opened_zipfile,
  File "/ML-A800/home/guoshuyue/anaconda3/envs/datacomp1/lib/python3.10/site-packages/torch/serialization.py", line 1438, in _load
    result = unpickler.load()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/datacomp1/lib/python3.10/site-packages/torch/serialization.py", line 1408, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/ML-A800/home/guoshuyue/anaconda3/envs/datacomp1/lib/python3.10/site-packages/torch/serialization.py", line 1382, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/ML-A800/home/guoshuyue/anaconda3/envs/datacomp1/lib/python3.10/site-packages/torch/serialization.py", line 391, in default_restore_location
    result = fn(storage, location)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/datacomp1/lib/python3.10/site-packages/torch/serialization.py", line 266, in _cuda_deserialize
    device = validate_cuda_device(location)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/datacomp1/lib/python3.10/site-packages/torch/serialization.py", line 250, in validate_cuda_device
    raise RuntimeError('Attempting to deserialize object on a CUDA '
RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
FasterViT(
  (patch_embed): PatchEmbed(
    (proj): Identity()
    (conv_down): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (4): BatchNorm2d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (levels): ModuleList(
    (0): FasterViTLayer(
      (blocks): ModuleList(
        (0): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): Identity()
        )
        (1): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.011)
        )
        (2): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.022)
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
    )
    (1): FasterViTLayer(
      (blocks): ModuleList(
        (0): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.033)
        )
        (1): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.044)
        )
        (2): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.056)
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
    )
    (2): FasterViTLayer(
      (blocks): ModuleList(
        (0): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.067)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (1): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.078)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (2): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.089)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.089)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (3): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.100)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (4): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.111)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.111)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (5): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.122)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.122)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (6): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.133)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (7): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.144)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.144)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
      (global_tokenizer): TokenInitializer(
        (pos_embed): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (to_global_feature): Sequential(
          (pos): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pool): AvgPool2d(kernel_size=5, stride=3, padding=0)
        )
      )
    )
    (3): FasterViTLayer(
      (blocks): ModuleList(
        (0): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.156)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.167)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.178)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.189)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (head): Linear(in_features=768, out_features=2, bias=True)
)
FasterViT(
  (patch_embed): PatchEmbed(
    (proj): Identity()
    (conv_down): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (4): BatchNorm2d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (levels): ModuleList(
    (0): FasterViTLayer(
      (blocks): ModuleList(
        (0): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): Identity()
        )
        (1): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.011)
        )
        (2): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.022)
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
    )
    (1): FasterViTLayer(
      (blocks): ModuleList(
        (0): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.033)
        )
        (1): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.044)
        )
        (2): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.056)
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
    )
    (2): FasterViTLayer(
      (blocks): ModuleList(
        (0): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.067)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (1): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.078)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (2): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.089)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.089)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (3): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.100)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (4): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.111)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.111)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (5): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.122)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.122)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (6): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.133)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (7): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.144)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.144)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
      (global_tokenizer): TokenInitializer(
        (pos_embed): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (to_global_feature): Sequential(
          (pos): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pool): AvgPool2d(kernel_size=5, stride=3, padding=0)
        )
      )
    )
    (3): FasterViTLayer(
      (blocks): ModuleList(
        (0): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.156)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.167)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.178)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.189)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (head): Linear(in_features=768, out_features=2, bias=True)
)
FasterViT(
  (patch_embed): PatchEmbed(
    (proj): Identity()
    (conv_down): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (4): BatchNorm2d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (levels): ModuleList(
    (0): FasterViTLayer(
      (blocks): ModuleList(
        (0): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): Identity()
        )
        (1): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.011)
        )
        (2): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.022)
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
    )
    (1): FasterViTLayer(
      (blocks): ModuleList(
        (0): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.033)
        )
        (1): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.044)
        )
        (2): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.056)
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
    )
    (2): FasterViTLayer(
      (blocks): ModuleList(
        (0): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.067)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (1): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.078)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (2): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.089)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.089)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (3): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.100)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (4): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.111)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.111)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (5): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.122)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.122)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (6): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.133)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (7): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.144)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.144)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
      (global_tokenizer): TokenInitializer(
        (pos_embed): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (to_global_feature): Sequential(
          (pos): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pool): AvgPool2d(kernel_size=5, stride=3, padding=0)
        )
      )
    )
    (3): FasterViTLayer(
      (blocks): ModuleList(
        (0): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.156)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.167)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.178)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.189)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (head): Linear(in_features=768, out_features=2, bias=True)
)
FasterViT(
  (patch_embed): PatchEmbed(
    (proj): Identity()
    (conv_down): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (4): BatchNorm2d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (levels): ModuleList(
    (0): FasterViTLayer(
      (blocks): ModuleList(
        (0): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): Identity()
        )
        (1): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.011)
        )
        (2): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.022)
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
    )
    (1): FasterViTLayer(
      (blocks): ModuleList(
        (0): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.033)
        )
        (1): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.044)
        )
        (2): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.056)
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
    )
    (2): FasterViTLayer(
      (blocks): ModuleList(
        (0): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.067)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (1): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.078)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (2): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.089)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.089)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (3): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.100)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (4): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.111)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.111)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (5): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.122)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.122)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (6): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.133)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (7): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.144)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.144)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
      (global_tokenizer): TokenInitializer(
        (pos_embed): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (to_global_feature): Sequential(
          (pos): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pool): AvgPool2d(kernel_size=5, stride=3, padding=0)
        )
      )
    )
    (3): FasterViTLayer(
      (blocks): ModuleList(
        (0): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.156)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.167)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.178)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.189)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (head): Linear(in_features=768, out_features=2, bias=True)
)
FasterViT(
  (patch_embed): PatchEmbed(
    (proj): Identity()
    (conv_down): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (4): BatchNorm2d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (levels): ModuleList(
    (0): FasterViTLayer(
      (blocks): ModuleList(
        (0): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): Identity()
        )
        (1): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.011)
        )
        (2): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.022)
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
    )
    (1): FasterViTLayer(
      (blocks): ModuleList(
        (0): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.033)
        )
        (1): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.044)
        )
        (2): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.056)
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
    )
    (2): FasterViTLayer(
      (blocks): ModuleList(
        (0): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.067)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (1): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.078)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (2): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.089)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.089)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (3): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.100)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (4): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.111)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.111)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (5): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.122)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.122)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (6): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.133)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (7): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.144)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.144)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
      (global_tokenizer): TokenInitializer(
        (pos_embed): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (to_global_feature): Sequential(
          (pos): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pool): AvgPool2d(kernel_size=5, stride=3, padding=0)
        )
      )
    )
    (3): FasterViTLayer(
      (blocks): ModuleList(
        (0): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.156)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.167)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.178)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.189)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (head): Linear(in_features=768, out_features=2, bias=True)
)
FasterViT(
  (patch_embed): PatchEmbed(
    (proj): Identity()
    (conv_down): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (4): BatchNorm2d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (levels): ModuleList(
    (0): FasterViTLayer(
      (blocks): ModuleList(
        (0): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): Identity()
        )
        (1): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.011)
        )
        (2): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.022)
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
    )
    (1): FasterViTLayer(
      (blocks): ModuleList(
        (0): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.033)
        )
        (1): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.044)
        )
        (2): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.056)
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
    )
    (2): FasterViTLayer(
      (blocks): ModuleList(
        (0): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.067)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (1): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.078)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (2): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.089)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.089)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (3): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.100)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (4): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.111)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.111)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (5): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.122)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.122)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (6): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.133)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (7): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.144)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.144)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
      (global_tokenizer): TokenInitializer(
        (pos_embed): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (to_global_feature): Sequential(
          (pos): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pool): AvgPool2d(kernel_size=5, stride=3, padding=0)
        )
      )
    )
    (3): FasterViTLayer(
      (blocks): ModuleList(
        (0): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.156)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.167)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.178)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.189)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (head): Linear(in_features=768, out_features=2, bias=True)
)
FasterViT(
  (patch_embed): PatchEmbed(
    (proj): Identity()
    (conv_down): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (4): BatchNorm2d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (levels): ModuleList(
    (0): FasterViTLayer(
      (blocks): ModuleList(
        (0): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): Identity()
        )
        (1): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.011)
        )
        (2): ConvBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.022)
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
    )
    (1): FasterViTLayer(
      (blocks): ModuleList(
        (0): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.033)
        )
        (1): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.044)
        )
        (2): ConvBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_path): DropPath(drop_prob=0.056)
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
    )
    (2): FasterViTLayer(
      (blocks): ModuleList(
        (0): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.067)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (1): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.078)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (2): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.089)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.089)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (3): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.100)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (4): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.111)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.111)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (5): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.122)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.122)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (6): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.133)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
        (7): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.144)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (hat_attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
          )
          (hat_mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (hat_drop_path): DropPath(drop_prob=0.144)
          (hat_pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=384, bias=False)
            )
          )
          (upsampler): Upsample(size=7, mode='nearest')
        )
      )
      (downsample): Downsample(
        (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
        (reduction): Sequential(
          (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        )
      )
      (global_tokenizer): TokenInitializer(
        (pos_embed): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (to_global_feature): Sequential(
          (pos): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (pool): AvgPool2d(kernel_size=5, stride=3, padding=0)
        )
      )
    )
    (3): FasterViTLayer(
      (blocks): ModuleList(
        (0): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.156)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.167)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.178)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.189)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): HAT(
          (pos_embed): PosEmbMLPSwinv1D(
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=768, bias=False)
            )
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (pos_emb_funct): PosEmbMLPSwinv2D(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (head): Linear(in_features=768, out_features=2, bias=True)
)
  0%|          | 0/3949 [00:00<?, ?it/s]  0%|          | 0/3949 [00:00<?, ?it/s]  0%|          | 0/3949 [00:00<?, ?it/s]  0%|          | 0/3949 [00:00<?, ?it/s]  0%|          | 0/3949 [00:00<?, ?it/s]  0%|          | 0/3949 [00:00<?, ?it/s]  0%|          | 0/3949 [00:00<?, ?it/s]  0%|          | 1/3949 [00:00<18:36,  3.54it/s]  0%|          | 1/3949 [00:00<19:30,  3.37it/s]  0%|          | 1/3949 [00:00<20:22,  3.23it/s]  0%|          | 1/3949 [00:00<20:37,  3.19it/s]  0%|          | 1/3949 [00:00<20:52,  3.15it/s]  0%|          | 1/3949 [00:00<21:18,  3.09it/s]  0%|          | 1/3949 [00:00<21:54,  3.00it/s]  0%|          | 2/3949 [00:00<19:44,  3.33it/s]  0%|          | 2/3949 [00:00<19:52,  3.31it/s]  0%|          | 2/3949 [00:00<20:27,  3.21it/s]  0%|          | 2/3949 [00:00<20:42,  3.18it/s]  0%|          | 2/3949 [00:00<21:24,  3.07it/s]  0%|          | 2/3949 [00:00<21:28,  3.06it/s]  0%|          | 2/3949 [00:00<22:32,  2.92it/s]  0%|          | 3/3949 [00:00<19:35,  3.36it/s]  0%|          | 3/3949 [00:00<19:37,  3.35it/s]  0%|          | 3/3949 [00:00<21:02,  3.13it/s]  0%|          | 3/3949 [00:00<21:31,  3.05it/s]  0%|          | 3/3949 [00:01<22:55,  2.87it/s]  0%|          | 3/3949 [00:01<22:37,  2.91it/s]  0%|          | 3/3949 [00:01<23:12,  2.83it/s]  0%|          | 4/3949 [00:01<18:52,  3.48it/s]  0%|          | 4/3949 [00:01<19:30,  3.37it/s]  0%|          | 4/3949 [00:01<20:29,  3.21it/s]  0%|          | 4/3949 [00:01<21:09,  3.11it/s]  0%|          | 4/3949 [00:01<21:27,  3.06it/s]  0%|          | 4/3949 [00:01<22:09,  2.97it/s]  0%|          | 4/3949 [00:01<22:35,  2.91it/s]  0%|          | 5/3949 [00:01<19:26,  3.38it/s]  0%|          | 5/3949 [00:01<20:05,  3.27it/s]  0%|          | 5/3949 [00:01<19:55,  3.30it/s]  0%|          | 5/3949 [00:01<21:36,  3.04it/s]  0%|          | 5/3949 [00:01<21:11,  3.10it/s]  0%|          | 5/3949 [00:01<21:57,  2.99it/s]  0%|          | 5/3949 [00:01<22:11,  2.96it/s]  0%|          | 6/3949 [00:01<20:00,  3.29it/s]  0%|          | 6/3949 [00:01<20:08,  3.26it/s]  0%|          | 6/3949 [00:01<20:39,  3.18it/s]  0%|          | 6/3949 [00:01<21:43,  3.02it/s]  0%|          | 6/3949 [00:01<21:58,  2.99it/s]  0%|          | 6/3949 [00:02<21:55,  3.00it/s]  0%|          | 6/3949 [00:02<22:27,  2.93it/s]  0%|          | 7/3949 [00:02<18:51,  3.48it/s]  0%|          | 7/3949 [00:02<20:18,  3.24it/s]  0%|          | 7/3949 [00:02<20:01,  3.28it/s]  0%|          | 7/3949 [00:02<21:47,  3.02it/s]  0%|          | 7/3949 [00:02<22:03,  2.98it/s]  0%|          | 7/3949 [00:02<22:31,  2.92it/s]  0%|          | 8/3949 [00:02<18:42,  3.51it/s]  0%|          | 7/3949 [00:02<22:54,  2.87it/s]  0%|          | 8/3949 [00:02<19:37,  3.35it/s]  0%|          | 8/3949 [00:02<20:13,  3.25it/s]  0%|          | 9/3949 [00:02<17:36,  3.73it/s]  0%|          | 8/3949 [00:02<22:37,  2.90it/s]  0%|          | 8/3949 [00:02<23:06,  2.84it/s]  0%|          | 9/3949 [00:02<19:42,  3.33it/s]  0%|          | 9/3949 [00:02<19:38,  3.34it/s]  0%|          | 8/3949 [00:02<22:45,  2.89it/s]  0%|          | 8/3949 [00:02<22:55,  2.86it/s]  0%|          | 10/3949 [00:02<18:33,  3.54it/s]  0%|          | 9/3949 [00:02<22:03,  2.98it/s]  0%|          | 10/3949 [00:02<19:18,  3.40it/s]  0%|          | 9/3949 [00:03<22:46,  2.88it/s]  0%|          | 9/3949 [00:03<22:28,  2.92it/s]  0%|          | 10/3949 [00:03<20:24,  3.22it/s]  0%|          | 9/3949 [00:03<22:46,  2.88it/s]  0%|          | 11/3949 [00:03<19:56,  3.29it/s]  0%|          | 10/3949 [00:03<22:19,  2.94it/s]  0%|          | 10/3949 [00:03<21:43,  3.02it/s]  0%|          | 11/3949 [00:03<20:32,  3.20it/s]  0%|          | 11/3949 [00:03<20:20,  3.23it/s]  0%|          | 10/3949 [00:03<21:50,  3.01it/s]  0%|          | 10/3949 [00:03<21:53,  3.00it/s]  0%|          | 12/3949 [00:03<19:57,  3.29it/s]  0%|          | 11/3949 [00:03<21:43,  3.02it/s]  0%|          | 11/3949 [00:03<21:13,  3.09it/s]  0%|          | 12/3949 [00:03<20:48,  3.15it/s]  0%|          | 12/3949 [00:03<20:33,  3.19it/s]  0%|          | 11/3949 [00:03<21:35,  3.04it/s]  0%|          | 11/3949 [00:03<22:15,  2.95it/s]  0%|          | 13/3949 [00:03<20:40,  3.17it/s]  0%|          | 13/3949 [00:03<20:15,  3.24it/s]  0%|          | 12/3949 [00:03<21:14,  3.09it/s]  0%|          | 12/3949 [00:03<21:55,  2.99it/s]  0%|          | 13/3949 [00:03<20:22,  3.22it/s]  0%|          | 12/3949 [00:04<21:49,  3.01it/s]  0%|          | 12/3949 [00:04<22:41,  2.89it/s]  0%|          | 14/3949 [00:04<20:04,  3.27it/s]  0%|          | 14/3949 [00:04<19:45,  3.32it/s]  0%|          | 14/3949 [00:04<19:59,  3.28it/s]  0%|          | 13/3949 [00:04<21:59,  2.98it/s]  0%|          | 13/3949 [00:04<22:08,  2.96it/s]  0%|          | 13/3949 [00:04<22:22,  2.93it/s]  0%|          | 13/3949 [00:04<22:24,  2.93it/s]  0%|          | 15/3949 [00:04<18:27,  3.55it/s]  0%|          | 15/3949 [00:04<20:18,  3.23it/s]  0%|          | 15/3949 [00:04<19:59,  3.28it/s]  0%|          | 14/3949 [00:04<22:08,  2.96it/s]  0%|          | 14/3949 [00:04<22:50,  2.87it/s]  0%|          | 16/3949 [00:04<17:16,  3.79it/s]  0%|          | 14/3949 [00:04<22:35,  2.90it/s]  0%|          | 14/3949 [00:04<23:10,  2.83it/s]  0%|          | 16/3949 [00:04<20:20,  3.22it/s]  0%|          | 16/3949 [00:04<20:02,  3.27it/s]  0%|          | 17/3949 [00:04<16:21,  4.01it/s]  0%|          | 15/3949 [00:05<22:27,  2.92it/s]  0%|          | 15/3949 [00:05<23:31,  2.79it/s]  0%|          | 17/3949 [00:05<19:45,  3.32it/s]  0%|          | 15/3949 [00:05<23:15,  2.82it/s]  0%|          | 15/3949 [00:05<23:45,  2.76it/s]  0%|          | 18/3949 [00:05<16:39,  3.93it/s]  0%|          | 17/3949 [00:05<20:03,  3.27it/s]  0%|          | 16/3949 [00:05<22:16,  2.94it/s]  0%|          | 16/3949 [00:05<22:57,  2.85it/s]  0%|          | 18/3949 [00:05<20:21,  3.22it/s]  0%|          | 19/3949 [00:05<16:44,  3.91it/s]  0%|          | 16/3949 [00:05<22:41,  2.89it/s]  0%|          | 16/3949 [00:05<22:46,  2.88it/s]  0%|          | 18/3949 [00:05<20:35,  3.18it/s]  0%|          | 17/3949 [00:05<22:09,  2.96it/s]  1%|          | 20/3949 [00:05<17:17,  3.79it/s]  0%|          | 17/3949 [00:05<23:05,  2.84it/s]  0%|          | 19/3949 [00:05<20:19,  3.22it/s]  0%|          | 17/3949 [00:05<22:16,  2.94it/s]  0%|          | 17/3949 [00:05<22:34,  2.90it/s]  0%|          | 19/3949 [00:05<20:16,  3.23it/s]  1%|          | 21/3949 [00:05<17:00,  3.85it/s]  0%|          | 18/3949 [00:06<22:27,  2.92it/s]  1%|          | 20/3949 [00:06<21:02,  3.11it/s]  0%|          | 18/3949 [00:06<23:30,  2.79it/s]  1%|          | 20/3949 [00:06<20:17,  3.23it/s]  0%|          | 18/3949 [00:06<22:37,  2.90it/s]  0%|          | 18/3949 [00:06<22:10,  2.95it/s]  1%|          | 22/3949 [00:06<15:52,  4.12it/s]  1%|          | 23/3949 [00:06<15:25,  4.24it/s]  0%|          | 19/3949 [00:06<23:16,  2.81it/s]  1%|          | 21/3949 [00:06<21:26,  3.05it/s]  1%|          | 21/3949 [00:06<20:37,  3.17it/s]  0%|          | 19/3949 [00:06<22:17,  2.94it/s]  0%|          | 19/3949 [00:06<22:39,  2.89it/s]  0%|          | 19/3949 [00:06<23:56,  2.74it/s]  1%|          | 24/3949 [00:06<15:59,  4.09it/s]  1%|          | 22/3949 [00:06<21:22,  3.06it/s]  1%|          | 20/3949 [00:06<22:58,  2.85it/s]  1%|          | 22/3949 [00:06<20:26,  3.20it/s]  1%|          | 20/3949 [00:06<22:20,  2.93it/s]  1%|          | 20/3949 [00:06<23:16,  2.81it/s]  1%|          | 20/3949 [00:06<22:41,  2.89it/s]  1%|          | 25/3949 [00:06<16:09,  4.05it/s]  1%|          | 23/3949 [00:07<20:12,  3.24it/s]  1%|          | 23/3949 [00:07<21:11,  3.09it/s]  1%|          | 21/3949 [00:07<22:50,  2.87it/s]  1%|          | 21/3949 [00:07<23:04,  2.84it/s]  1%|          | 26/3949 [00:07<16:32,  3.95it/s]  1%|          | 21/3949 [00:07<22:33,  2.90it/s]  1%|          | 21/3949 [00:07<22:44,  2.88it/s]  1%|          | 24/3949 [00:07<19:59,  3.27it/s]  1%|          | 24/3949 [00:07<20:09,  3.25it/s]  1%|          | 22/3949 [00:07<22:29,  2.91it/s]  1%|          | 27/3949 [00:07<17:08,  3.81it/s]  1%|          | 22/3949 [00:07<22:25,  2.92it/s]  1%|          | 22/3949 [00:07<22:40,  2.89it/s]  1%|          | 22/3949 [00:07<23:27,  2.79it/s]  1%|          | 25/3949 [00:07<19:45,  3.31it/s]  1%|          | 25/3949 [00:07<20:25,  3.20it/s]  1%|          | 28/3949 [00:07<17:38,  3.70it/s]  1%|          | 23/3949 [00:07<22:48,  2.87it/s]  1%|          | 23/3949 [00:07<22:27,  2.91it/s]  1%|          | 23/3949 [00:07<22:36,  2.89it/s]  1%|          | 23/3949 [00:07<23:36,  2.77it/s]  1%|          | 26/3949 [00:07<19:10,  3.41it/s]  1%|          | 26/3949 [00:08<21:30,  3.04it/s]  1%|          | 29/3949 [00:08<18:03,  3.62it/s]  1%|          | 24/3949 [00:08<22:05,  2.96it/s]  1%|          | 24/3949 [00:08<22:00,  2.97it/s]  1%|          | 24/3949 [00:08<22:32,  2.90it/s]  1%|          | 24/3949 [00:08<22:37,  2.89it/s]  1%|          | 27/3949 [00:08<20:13,  3.23it/s]  1%|          | 27/3949 [00:08<21:13,  3.08it/s]  1%|          | 30/3949 [00:08<19:00,  3.44it/s]  1%|          | 25/3949 [00:08<21:41,  3.01it/s]  1%|          | 25/3949 [00:08<21:45,  3.01it/s]  1%|          | 25/3949 [00:08<21:33,  3.03it/s]  1%|          | 28/3949 [00:08<20:03,  3.26it/s]  1%|          | 25/3949 [00:08<22:55,  2.85it/s]  1%|          | 28/3949 [00:08<20:27,  3.19it/s]  1%|          | 31/3949 [00:08<19:15,  3.39it/s]  1%|          | 26/3949 [00:08<21:35,  3.03it/s]  1%|          | 26/3949 [00:08<21:42,  3.01it/s]  1%|          | 26/3949 [00:08<22:20,  2.93it/s]  1%|          | 29/3949 [00:08<20:00,  3.27it/s]  1%|          | 26/3949 [00:08<22:01,  2.97it/s]  1%|          | 29/3949 [00:08<20:31,  3.18it/s]  1%|          | 32/3949 [00:09<19:42,  3.31it/s]  1%|          | 27/3949 [00:09<21:11,  3.09it/s]  1%|          | 27/3949 [00:09<21:45,  3.00it/s]  1%|          | 30/3949 [00:09<20:13,  3.23it/s]  1%|          | 27/3949 [00:09<22:13,  2.94it/s]  1%|          | 27/3949 [00:09<21:53,  2.99it/s]  1%|          | 30/3949 [00:09<21:06,  3.09it/s]  1%|          | 33/3949 [00:09<20:28,  3.19it/s]  1%|          | 28/3949 [00:09<21:01,  3.11it/s]  1%|          | 28/3949 [00:09<21:35,  3.03it/s]  1%|          | 31/3949 [00:09<20:16,  3.22it/s]  1%|          | 28/3949 [00:09<21:49,  2.99it/s]  1%|          | 28/3949 [00:09<21:27,  3.04it/s]  1%|          | 34/3949 [00:09<19:19,  3.38it/s]  1%|          | 31/3949 [00:09<20:50,  3.13it/s]  1%|          | 29/3949 [00:09<21:48,  3.00it/s]  1%|          | 32/3949 [00:09<19:55,  3.28it/s]  1%|          | 29/3949 [00:09<21:48,  3.00it/s]  1%|          | 35/3949 [00:09<19:14,  3.39it/s]  1%|          | 29/3949 [00:09<22:19,  2.93it/s]  1%|          | 32/3949 [00:09<20:47,  3.14it/s]  1%|          | 29/3949 [00:09<23:13,  2.81it/s]  1%|          | 30/3949 [00:10<21:21,  3.06it/s]  1%|          | 33/3949 [00:10<20:16,  3.22it/s]  1%|          | 36/3949 [00:10<19:34,  3.33it/s]  1%|          | 33/3949 [00:10<19:45,  3.30it/s]  1%|          | 30/3949 [00:10<22:57,  2.84it/s]  1%|          | 30/3949 [00:10<22:37,  2.89it/s]  1%|          | 30/3949 [00:10<23:07,  2.82it/s]  1%|          | 31/3949 [00:10<21:22,  3.05it/s]  1%|          | 34/3949 [00:10<19:37,  3.32it/s]  1%|          | 34/3949 [00:10<19:33,  3.34it/s]  1%|          | 37/3949 [00:10<19:50,  3.29it/s]  1%|          | 31/3949 [00:10<21:50,  2.99it/s]  1%|          | 31/3949 [00:10<21:52,  2.99it/s]  1%|          | 31/3949 [00:10<22:28,  2.91it/s]  1%|          | 32/3949 [00:10<21:18,  3.06it/s]  1%|          | 35/3949 [00:10<19:44,  3.30it/s]  1%|          | 38/3949 [00:10<19:10,  3.40it/s]  1%|          | 35/3949 [00:10<20:08,  3.24it/s]  1%|          | 32/3949 [00:10<22:02,  2.96it/s]  1%|          | 32/3949 [00:10<21:55,  2.98it/s]  1%|          | 32/3949 [00:10<22:33,  2.89it/s]  1%|          | 36/3949 [00:11<19:54,  3.28it/s]  1%|          | 33/3949 [00:11<21:54,  2.98it/s]  1%|          | 39/3949 [00:11<19:18,  3.37it/s]  1%|          | 36/3949 [00:11<19:42,  3.31it/s]  1%|          | 33/3949 [00:11<21:41,  3.01it/s]  1%|          | 33/3949 [00:11<21:52,  2.98it/s]  1%|          | 33/3949 [00:11<22:48,  2.86it/s]  1%|          | 37/3949 [00:11<19:56,  3.27it/s]  1%|          | 37/3949 [00:11<19:08,  3.41it/s]  1%|          | 40/3949 [00:11<19:24,  3.36it/s]  1%|          | 34/3949 [00:11<21:41,  3.01it/s]  1%|          | 34/3949 [00:11<21:33,  3.03it/s]  1%|          | 34/3949 [00:11<21:30,  3.03it/s]  1%|          | 34/3949 [00:11<22:10,  2.94it/s]  1%|          | 38/3949 [00:11<20:01,  3.26it/s]  1%|          | 41/3949 [00:11<19:25,  3.35it/s]  1%|          | 38/3949 [00:11<19:55,  3.27it/s]  1%|          | 35/3949 [00:11<22:02,  2.96it/s]  1%|          | 35/3949 [00:11<21:15,  3.07it/s]  1%|          | 35/3949 [00:11<21:38,  3.01it/s]  1%|          | 35/3949 [00:11<21:25,  3.04it/s]  1%|          | 39/3949 [00:12<21:19,  3.05it/s]  1%|          | 42/3949 [00:12<20:40,  3.15it/s]  1%|          | 39/3949 [00:12<20:24,  3.19it/s]  1%|          | 36/3949 [00:12<21:16,  3.07it/s]  1%|          | 36/3949 [00:12<21:28,  3.04it/s]  1%|          | 36/3949 [00:12<20:18,  3.21it/s]  1%|          | 36/3949 [00:12<22:21,  2.92it/s]  1%|          | 37/3949 [00:12<19:57,  3.27it/s]  1%|          | 40/3949 [00:12<21:21,  3.05it/s]  1%|          | 43/3949 [00:12<20:47,  3.13it/s]  1%|          | 40/3949 [00:12<21:12,  3.07it/s]  1%|          | 37/3949 [00:12<21:01,  3.10it/s]  1%|          | 37/3949 [00:12<20:09,  3.23it/s]  1%|          | 38/3949 [00:12<19:44,  3.30it/s]  1%|          | 37/3949 [00:12<21:47,  2.99it/s]  1%|          | 41/3949 [00:12<21:06,  3.09it/s]  1%|          | 41/3949 [00:12<22:16,  2.92it/s]  1%|          | 44/3949 [00:12<22:39,  2.87it/s]  1%|          | 38/3949 [00:12<20:29,  3.18it/s]  1%|          | 38/3949 [00:12<19:48,  3.29it/s]  1%|          | 39/3949 [00:12<19:10,  3.40it/s]  1%|          | 38/3949 [00:12<21:32,  3.03it/s]  1%|          | 42/3949 [00:13<22:16,  2.92it/s]  1%|          | 45/3949 [00:13<21:40,  3.00it/s]  1%|          | 42/3949 [00:13<22:04,  2.95it/s]  1%|          | 39/3949 [00:13<20:31,  3.18it/s]  1%|          | 39/3949 [00:13<20:07,  3.24it/s]  1%|          | 40/3949 [00:13<19:17,  3.38it/s]  1%|          | 39/3949 [00:13<22:10,  2.94it/s]  1%|          | 43/3949 [00:13<21:07,  3.08it/s]  1%|          | 46/3949 [00:13<21:11,  3.07it/s]  1%|          | 43/3949 [00:13<21:15,  3.06it/s]  1%|          | 40/3949 [00:13<19:40,  3.31it/s]  1%|          | 40/3949 [00:13<21:14,  3.07it/s]  1%|          | 41/3949 [00:13<20:07,  3.24it/s]  1%|          | 40/3949 [00:13<21:50,  2.98it/s]  1%|          | 44/3949 [00:13<20:20,  3.20it/s]  1%|          | 47/3949 [00:13<21:02,  3.09it/s]  1%|          | 44/3949 [00:13<21:22,  3.04it/s]  1%|          | 41/3949 [00:13<20:31,  3.17it/s]  1%|          | 41/3949 [00:13<21:17,  3.06it/s]  1%|          | 42/3949 [00:13<20:57,  3.11it/s]  1%|          | 41/3949 [00:13<21:37,  3.01it/s]  1%|          | 45/3949 [00:13<20:20,  3.20it/s]  1%|          | 45/3949 [00:14<20:24,  3.19it/s]  1%|          | 48/3949 [00:14<20:32,  3.16it/s]  1%|          | 42/3949 [00:14<21:41,  3.00it/s]  1%|          | 42/3949 [00:14<22:04,  2.95it/s]  1%|          | 43/3949 [00:14<21:31,  3.02it/s]  1%|          | 42/3949 [00:14<21:32,  3.02it/s]  1%|          | 49/3949 [00:14<19:31,  3.33it/s]  1%|          | 46/3949 [00:14<19:32,  3.33it/s]  1%|          | 46/3949 [00:14<20:55,  3.11it/s]  1%|          | 43/3949 [00:14<21:58,  2.96it/s]  1%|          | 43/3949 [00:14<21:59,  2.96it/s]  1%|          | 47/3949 [00:14<18:43,  3.47it/s]  1%|▏         | 50/3949 [00:14<19:22,  3.35it/s]  1%|          | 47/3949 [00:14<20:28,  3.18it/s]  1%|          | 43/3949 [00:14<21:44,  2.99it/s]  1%|          | 44/3949 [00:14<23:07,  2.81it/s]  1%|          | 44/3949 [00:14<21:49,  2.98it/s]  1%|          | 44/3949 [00:14<22:20,  2.91it/s]  1%|▏         | 51/3949 [00:14<19:08,  3.39it/s]  1%|          | 48/3949 [00:14<19:22,  3.35it/s]  1%|          | 45/3949 [00:14<21:41,  3.00it/s]  1%|          | 48/3949 [00:14<20:33,  3.16it/s]  1%|          | 44/3949 [00:14<21:48,  2.98it/s]  1%|          | 45/3949 [00:15<21:13,  3.07it/s]  1%|          | 49/3949 [00:15<19:38,  3.31it/s]  1%|▏         | 52/3949 [00:15<20:10,  3.22it/s]  1%|          | 46/3949 [00:15<21:05,  3.08it/s]  1%|          | 49/3949 [00:15<21:03,  3.09it/s]  1%|          | 45/3949 [00:15<24:27,  2.66it/s]  1%|          | 45/3949 [00:15<22:18,  2.92it/s]  1%|          | 46/3949 [00:15<20:54,  3.11it/s]  1%|▏         | 50/3949 [00:15<19:48,  3.28it/s]  1%|▏         | 53/3949 [00:15<20:18,  3.20it/s]  1%|          | 47/3949 [00:15<21:11,  3.07it/s]  1%|          | 46/3949 [00:15<22:26,  2.90it/s]  1%|          | 46/3949 [00:15<20:58,  3.10it/s]  1%|▏         | 50/3949 [00:15<21:05,  3.08it/s]  1%|          | 47/3949 [00:15<20:18,  3.20it/s]  1%|▏         | 51/3949 [00:15<20:24,  3.18it/s]  1%|▏         | 54/3949 [00:15<20:52,  3.11it/s]  1%|          | 47/3949 [00:15<21:31,  3.02it/s]  1%|          | 48/3949 [00:15<21:26,  3.03it/s]  1%|          | 47/3949 [00:15<21:20,  3.05it/s]  1%|▏         | 51/3949 [00:15<22:06,  2.94it/s]  1%|          | 48/3949 [00:16<19:52,  3.27it/s]  1%|▏         | 52/3949 [00:16<20:28,  3.17it/s]  1%|          | 49/3949 [00:16<21:13,  3.06it/s]  1%|▏         | 55/3949 [00:16<21:25,  3.03it/s]  1%|          | 48/3949 [00:16<21:53,  2.97it/s]  1%|▏         | 52/3949 [00:16<20:47,  3.12it/s]  1%|          | 48/3949 [00:16<21:11,  3.07it/s]  1%|          | 49/3949 [00:16<21:10,  3.07it/s]  1%|▏         | 53/3949 [00:16<19:39,  3.30it/s]  1%|▏         | 56/3949 [00:16<20:38,  3.14it/s]  1%|▏         | 50/3949 [00:16<21:40,  3.00it/s]  1%|▏         | 53/3949 [00:16<20:45,  3.13it/s]  1%|          | 49/3949 [00:16<21:35,  3.01it/s]  1%|          | 49/3949 [00:16<22:50,  2.85it/s]  1%|▏         | 50/3949 [00:16<21:21,  3.04it/s]  1%|▏         | 54/3949 [00:16<20:05,  3.23it/s]  1%|▏         | 57/3949 [00:16<20:20,  3.19it/s]  1%|▏         | 54/3949 [00:16<20:20,  3.19it/s]  1%|▏         | 51/3949 [00:16<21:29,  3.02it/s]  1%|▏         | 50/3949 [00:16<22:01,  2.95it/s]  1%|▏         | 50/3949 [00:16<22:31,  2.89it/s]  1%|▏         | 55/3949 [00:17<19:22,  3.35it/s]  1%|▏         | 58/3949 [00:17<19:16,  3.36it/s]  1%|▏         | 51/3949 [00:17<22:17,  2.92it/s]  1%|▏         | 55/3949 [00:17<20:37,  3.15it/s]  1%|▏         | 52/3949 [00:17<21:49,  2.98it/s]  1%|▏         | 51/3949 [00:17<22:34,  2.88it/s]  1%|▏         | 59/3949 [00:17<18:07,  3.58it/s]  1%|▏         | 51/3949 [00:17<22:33,  2.88it/s]  1%|▏         | 56/3949 [00:17<19:47,  3.28it/s]  1%|▏         | 52/3949 [00:17<22:10,  2.93it/s]  1%|▏         | 56/3949 [00:17<20:24,  3.18it/s]  1%|▏         | 53/3949 [00:17<21:46,  2.98it/s]  2%|▏         | 60/3949 [00:17<17:57,  3.61it/s]  1%|▏         | 57/3949 [00:17<19:24,  3.34it/s]  1%|▏         | 52/3949 [00:17<22:58,  2.83it/s]  1%|▏         | 52/3949 [00:17<22:43,  2.86it/s]  1%|▏         | 57/3949 [00:17<19:48,  3.28it/s]  1%|▏         | 53/3949 [00:17<22:26,  2.89it/s]  2%|▏         | 61/3949 [00:17<17:00,  3.81it/s]  1%|▏         | 54/3949 [00:17<22:02,  2.95it/s]  1%|▏         | 58/3949 [00:17<19:43,  3.29it/s]  2%|▏         | 62/3949 [00:18<16:01,  4.04it/s]  1%|▏         | 53/3949 [00:18<22:47,  2.85it/s]  1%|▏         | 53/3949 [00:18<23:05,  2.81it/s]  1%|▏         | 58/3949 [00:18<20:36,  3.15it/s]  1%|▏         | 54/3949 [00:18<22:25,  2.89it/s]  1%|▏         | 59/3949 [00:18<19:35,  3.31it/s]  1%|▏         | 55/3949 [00:18<22:39,  2.86it/s]  2%|▏         | 63/3949 [00:18<16:44,  3.87it/s]  1%|▏         | 54/3949 [00:18<22:29,  2.89it/s]  1%|▏         | 54/3949 [00:18<22:24,  2.90it/s]  1%|▏         | 59/3949 [00:18<20:23,  3.18it/s]  1%|▏         | 55/3949 [00:18<21:53,  2.96it/s]  1%|▏         | 56/3949 [00:18<21:23,  3.03it/s]  2%|▏         | 60/3949 [00:18<20:31,  3.16it/s]  2%|▏         | 64/3949 [00:18<18:45,  3.45it/s]  1%|▏         | 55/3949 [00:18<22:04,  2.94it/s]  1%|▏         | 55/3949 [00:18<22:16,  2.91it/s]  1%|▏         | 56/3949 [00:18<20:28,  3.17it/s]  2%|▏         | 60/3949 [00:18<21:03,  3.08it/s]  1%|▏         | 57/3949 [00:18<21:12,  3.06it/s]  2%|▏         | 61/3949 [00:18<21:14,  3.05it/s]  1%|▏         | 57/3949 [00:18<19:18,  3.36it/s]  1%|▏         | 56/3949 [00:18<21:27,  3.02it/s]  2%|▏         | 65/3949 [00:18<19:35,  3.30it/s]  1%|▏         | 56/3949 [00:19<22:11,  2.92it/s]  2%|▏         | 61/3949 [00:19<21:13,  3.05it/s]  1%|▏         | 58/3949 [00:19<21:29,  3.02it/s]  1%|▏         | 58/3949 [00:19<18:58,  3.42it/s]  2%|▏         | 62/3949 [00:19<21:54,  2.96it/s]  1%|▏         | 57/3949 [00:19<21:22,  3.04it/s]  2%|▏         | 66/3949 [00:19<20:24,  3.17it/s]  1%|▏         | 57/3949 [00:19<21:56,  2.96it/s]  2%|▏         | 62/3949 [00:19<21:07,  3.07it/s]  1%|▏         | 59/3949 [00:19<20:45,  3.12it/s]  1%|▏         | 59/3949 [00:19<18:49,  3.44it/s]  2%|▏         | 63/3949 [00:19<22:00,  2.94it/s]  1%|▏         | 58/3949 [00:19<21:20,  3.04it/s]  1%|▏         | 58/3949 [00:19<21:20,  3.04it/s]  2%|▏         | 67/3949 [00:19<20:47,  3.11it/s]  2%|▏         | 63/3949 [00:19<21:35,  3.00it/s]  2%|▏         | 60/3949 [00:19<20:27,  3.17it/s]  2%|▏         | 60/3949 [00:19<19:23,  3.34it/s]  2%|▏         | 64/3949 [00:19<21:13,  3.05it/s]  1%|▏         | 59/3949 [00:19<20:45,  3.12it/s]  1%|▏         | 59/3949 [00:19<20:57,  3.09it/s]  2%|▏         | 68/3949 [00:20<21:00,  3.08it/s]  2%|▏         | 64/3949 [00:20<21:51,  2.96it/s]  2%|▏         | 61/3949 [00:20<20:29,  3.16it/s]  2%|▏         | 61/3949 [00:20<19:47,  3.27it/s]  2%|▏         | 60/3949 [00:20<20:40,  3.13it/s]  2%|▏         | 65/3949 [00:20<21:20,  3.03it/s]  2%|▏         | 69/3949 [00:20<20:28,  3.16it/s]  2%|▏         | 60/3949 [00:20<21:46,  2.98it/s]  2%|▏         | 62/3949 [00:20<20:05,  3.22it/s]  2%|▏         | 62/3949 [00:20<19:33,  3.31it/s]  2%|▏         | 65/3949 [00:20<22:05,  2.93it/s]  2%|▏         | 61/3949 [00:20<20:12,  3.21it/s]  2%|▏         | 70/3949 [00:20<20:35,  3.14it/s]  2%|▏         | 61/3949 [00:20<21:05,  3.07it/s]  2%|▏         | 66/3949 [00:20<22:18,  2.90it/s]  2%|▏         | 63/3949 [00:20<20:05,  3.22it/s]  2%|▏         | 63/3949 [00:20<20:07,  3.22it/s]  2%|▏         | 66/3949 [00:20<22:12,  2.91it/s]  2%|▏         | 62/3949 [00:20<20:23,  3.18it/s]  2%|▏         | 62/3949 [00:20<20:32,  3.15it/s]  2%|▏         | 67/3949 [00:20<21:35,  3.00it/s]  2%|▏         | 71/3949 [00:20<20:46,  3.11it/s]  2%|▏         | 64/3949 [00:21<20:09,  3.21it/s]  2%|▏         | 64/3949 [00:21<20:00,  3.24it/s]  2%|▏         | 67/3949 [00:21<21:39,  2.99it/s]  2%|▏         | 63/3949 [00:21<20:43,  3.12it/s]  2%|▏         | 63/3949 [00:21<20:41,  3.13it/s]  2%|▏         | 72/3949 [00:21<20:58,  3.08it/s]  2%|▏         | 68/3949 [00:21<21:44,  2.98it/s]  2%|▏         | 65/3949 [00:21<20:10,  3.21it/s]  2%|▏         | 65/3949 [00:21<20:04,  3.22it/s]  2%|▏         | 68/3949 [00:21<21:31,  3.01it/s]  2%|▏         | 64/3949 [00:21<20:23,  3.18it/s]  2%|▏         | 64/3949 [00:21<20:51,  3.11it/s]  2%|▏         | 73/3949 [00:21<21:38,  2.98it/s]  2%|▏         | 69/3949 [00:21<22:17,  2.90it/s]  2%|▏         | 66/3949 [00:21<20:51,  3.10it/s]  2%|▏         | 66/3949 [00:21<20:31,  3.15it/s]  2%|▏         | 69/3949 [00:21<21:48,  2.97it/s]  2%|▏         | 65/3949 [00:21<20:49,  3.11it/s]  2%|▏         | 65/3949 [00:21<20:56,  3.09it/s]  2%|▏         | 70/3949 [00:21<21:20,  3.03it/s]  2%|▏         | 74/3949 [00:21<21:16,  3.04it/s]  2%|▏         | 67/3949 [00:22<20:48,  3.11it/s]  2%|▏         | 67/3949 [00:22<21:32,  3.00it/s]  2%|▏         | 70/3949 [00:22<21:23,  3.02it/s]  2%|▏         | 66/3949 [00:22<20:58,  3.09it/s]  2%|▏         | 66/3949 [00:22<21:06,  3.07it/s]  2%|▏         | 75/3949 [00:22<21:01,  3.07it/s]  2%|▏         | 71/3949 [00:22<21:18,  3.03it/s]  2%|▏         | 68/3949 [00:22<20:25,  3.17it/s]  2%|▏         | 68/3949 [00:22<21:38,  2.99it/s]  2%|▏         | 71/3949 [00:22<21:24,  3.02it/s]  2%|▏         | 67/3949 [00:22<20:44,  3.12it/s]  2%|▏         | 67/3949 [00:22<20:37,  3.14it/s]  2%|▏         | 72/3949 [00:22<21:09,  3.05it/s]  2%|▏         | 76/3949 [00:22<21:21,  3.02it/s]  2%|▏         | 69/3949 [00:22<20:42,  3.12it/s]  2%|▏         | 69/3949 [00:22<21:06,  3.06it/s]  2%|▏         | 72/3949 [00:22<21:33,  3.00it/s]  2%|▏         | 68/3949 [00:22<20:46,  3.11it/s]  2%|▏         | 68/3949 [00:22<20:51,  3.10it/s]  2%|▏         | 73/3949 [00:22<21:13,  3.04it/s]  2%|▏         | 77/3949 [00:22<21:38,  2.98it/s]  2%|▏         | 70/3949 [00:23<20:40,  3.13it/s]  2%|▏         | 70/3949 [00:23<20:40,  3.13it/s]  2%|▏         | 69/3949 [00:23<20:27,  3.16it/s]  2%|▏         | 73/3949 [00:23<21:56,  2.94it/s]  2%|▏         | 69/3949 [00:23<20:50,  3.10it/s]  2%|▏         | 74/3949 [00:23<21:47,  2.96it/s]  2%|▏         | 71/3949 [00:23<20:08,  3.21it/s]  2%|▏         | 78/3949 [00:23<22:39,  2.85it/s]  2%|▏         | 71/3949 [00:23<20:51,  3.10it/s]  2%|▏         | 70/3949 [00:23<20:27,  3.16it/s]  2%|▏         | 70/3949 [00:23<20:47,  3.11it/s]  2%|▏         | 74/3949 [00:23<22:24,  2.88it/s]  2%|▏         | 72/3949 [00:23<20:42,  3.12it/s]  2%|▏         | 75/3949 [00:23<22:19,  2.89it/s]  2%|▏         | 72/3949 [00:23<20:52,  3.09it/s]  2%|▏         | 79/3949 [00:23<22:55,  2.81it/s]  2%|▏         | 71/3949 [00:23<20:36,  3.14it/s]  2%|▏         | 71/3949 [00:23<20:48,  3.11it/s]  2%|▏         | 75/3949 [00:23<22:07,  2.92it/s]  2%|▏         | 73/3949 [00:23<20:05,  3.21it/s]  2%|▏         | 76/3949 [00:23<21:29,  3.00it/s]  2%|▏         | 73/3949 [00:23<20:23,  3.17it/s]  2%|▏         | 72/3949 [00:24<20:52,  3.10it/s]  2%|▏         | 80/3949 [00:24<23:11,  2.78it/s]  2%|▏         | 72/3949 [00:24<20:45,  3.11it/s]  2%|▏         | 76/3949 [00:24<22:06,  2.92it/s]  2%|▏         | 74/3949 [00:24<19:33,  3.30it/s]  2%|▏         | 74/3949 [00:24<20:22,  3.17it/s]  2%|▏         | 77/3949 [00:24<22:14,  2.90it/s]  2%|▏         | 73/3949 [00:24<20:59,  3.08it/s]  2%|▏         | 81/3949 [00:24<22:55,  2.81it/s]  2%|▏         | 73/3949 [00:24<21:33,  3.00it/s]  2%|▏         | 75/3949 [00:24<19:33,  3.30it/s]  2%|▏         | 77/3949 [00:24<23:00,  2.80it/s]  2%|▏         | 75/3949 [00:24<20:11,  3.20it/s]  2%|▏         | 78/3949 [00:24<23:10,  2.78it/s]  2%|▏         | 74/3949 [00:24<20:52,  3.09it/s]  2%|▏         | 82/3949 [00:24<22:45,  2.83it/s]  2%|▏         | 74/3949 [00:24<20:16,  3.18it/s]  2%|▏         | 76/3949 [00:24<19:49,  3.26it/s]  2%|▏         | 76/3949 [00:24<19:29,  3.31it/s]  2%|▏         | 78/3949 [00:24<23:14,  2.78it/s]  2%|▏         | 75/3949 [00:25<20:36,  3.13it/s]  2%|▏         | 75/3949 [00:25<20:37,  3.13it/s]  2%|▏         | 79/3949 [00:25<24:23,  2.64it/s]  2%|▏         | 77/3949 [00:25<20:11,  3.20it/s]  2%|▏         | 83/3949 [00:25<23:34,  2.73it/s]  2%|▏         | 77/3949 [00:25<19:34,  3.30it/s]  2%|▏         | 79/3949 [00:25<23:13,  2.78it/s]  2%|▏         | 76/3949 [00:25<20:31,  3.14it/s]  2%|▏         | 76/3949 [00:25<20:36,  3.13it/s]  2%|▏         | 80/3949 [00:25<23:12,  2.78it/s]  2%|▏         | 78/3949 [00:25<20:03,  3.22it/s]  2%|▏         | 78/3949 [00:25<21:22,  3.02it/s]  2%|▏         | 84/3949 [00:25<24:19,  2.65it/s]  2%|▏         | 80/3949 [00:25<22:23,  2.88it/s]  2%|▏         | 77/3949 [00:25<21:12,  3.04it/s]  2%|▏         | 77/3949 [00:25<20:05,  3.21it/s]  2%|▏         | 79/3949 [00:25<19:40,  3.28it/s]  2%|▏         | 81/3949 [00:25<23:03,  2.80it/s]  2%|▏         | 79/3949 [00:25<20:19,  3.17it/s]  2%|▏         | 85/3949 [00:25<23:10,  2.78it/s]  2%|▏         | 78/3949 [00:26<20:21,  3.17it/s]  2%|▏         | 81/3949 [00:26<22:43,  2.84it/s]  2%|▏         | 78/3949 [00:26<20:26,  3.16it/s]  2%|▏         | 80/3949 [00:26<19:51,  3.25it/s]  2%|▏         | 80/3949 [00:26<20:04,  3.21it/s]  2%|▏         | 82/3949 [00:26<23:11,  2.78it/s]  2%|▏         | 86/3949 [00:26<23:39,  2.72it/s]  2%|▏         | 79/3949 [00:26<20:26,  3.16it/s]  2%|▏         | 82/3949 [00:26<22:30,  2.86it/s]  2%|▏         | 79/3949 [00:26<20:39,  3.12it/s]  2%|▏         | 81/3949 [00:26<20:24,  3.16it/s]  2%|▏         | 81/3949 [00:26<19:40,  3.28it/s]  2%|▏         | 83/3949 [00:26<23:00,  2.80it/s]  2%|▏         | 80/3949 [00:26<20:22,  3.16it/s]  2%|▏         | 87/3949 [00:26<23:37,  2.72it/s]  2%|▏         | 83/3949 [00:26<22:45,  2.83it/s]  2%|▏         | 80/3949 [00:26<20:54,  3.08it/s]  2%|▏         | 82/3949 [00:26<20:35,  3.13it/s]  2%|▏         | 82/3949 [00:26<20:08,  3.20it/s]  2%|▏         | 84/3949 [00:26<23:04,  2.79it/s]  2%|▏         | 81/3949 [00:26<19:45,  3.26it/s]  2%|▏         | 84/3949 [00:27<22:30,  2.86it/s]  2%|▏         | 81/3949 [00:27<20:43,  3.11it/s]  2%|▏         | 88/3949 [00:27<24:13,  2.66it/s]  2%|▏         | 83/3949 [00:27<20:15,  3.18it/s]  2%|▏         | 83/3949 [00:27<20:26,  3.15it/s]  2%|▏         | 82/3949 [00:27<19:50,  3.25it/s]  2%|▏         | 85/3949 [00:27<24:17,  2.65it/s]  2%|▏         | 82/3949 [00:27<20:26,  3.15it/s]  2%|▏         | 84/3949 [00:27<19:49,  3.25it/s]  2%|▏         | 84/3949 [00:27<21:07,  3.05it/s]  2%|▏         | 85/3949 [00:27<23:11,  2.78it/s]  2%|▏         | 89/3949 [00:27<24:35,  2.62it/s]  2%|▏         | 83/3949 [00:27<19:11,  3.36it/s]  2%|▏         | 86/3949 [00:27<23:55,  2.69it/s]  2%|▏         | 85/3949 [00:27<19:37,  3.28it/s]  2%|▏         | 85/3949 [00:27<20:29,  3.14it/s]  2%|▏         | 83/3949 [00:27<21:35,  2.98it/s]  2%|▏         | 90/3949 [00:27<23:37,  2.72it/s]  2%|▏         | 86/3949 [00:27<23:38,  2.72it/s]  2%|▏         | 84/3949 [00:27<20:20,  3.17it/s]  2%|▏         | 86/3949 [00:28<19:37,  3.28it/s]  2%|▏         | 86/3949 [00:28<20:00,  3.22it/s]  2%|▏         | 84/3949 [00:28<21:03,  3.06it/s]  2%|▏         | 87/3949 [00:28<23:53,  2.69it/s]  2%|▏         | 91/3949 [00:28<23:04,  2.79it/s]  2%|▏         | 87/3949 [00:28<22:56,  2.81it/s]  2%|▏         | 85/3949 [00:28<20:18,  3.17it/s]  2%|▏         | 87/3949 [00:28<19:14,  3.34it/s]  2%|▏         | 87/3949 [00:28<20:06,  3.20it/s]  2%|▏         | 85/3949 [00:28<21:13,  3.04it/s]  2%|▏         | 88/3949 [00:28<23:40,  2.72it/s]  2%|▏         | 92/3949 [00:28<23:07,  2.78it/s]  2%|▏         | 86/3949 [00:28<20:16,  3.17it/s]  2%|▏         | 88/3949 [00:28<22:49,  2.82it/s]  2%|▏         | 88/3949 [00:28<19:36,  3.28it/s]  2%|▏         | 88/3949 [00:28<20:14,  3.18it/s]  2%|▏         | 86/3949 [00:28<20:59,  3.07it/s]  2%|▏         | 89/3949 [00:28<22:42,  2.83it/s]  2%|▏         | 87/3949 [00:28<20:11,  3.19it/s]  2%|▏         | 89/3949 [00:28<22:54,  2.81it/s]  2%|▏         | 93/3949 [00:28<23:26,  2.74it/s]  2%|▏         | 89/3949 [00:28<19:20,  3.33it/s]  2%|▏         | 89/3949 [00:28<20:06,  3.20it/s]  2%|▏         | 87/3949 [00:29<21:06,  3.05it/s]  2%|▏         | 90/3949 [00:29<22:57,  2.80it/s]  2%|▏         | 88/3949 [00:29<20:25,  3.15it/s]  2%|▏         | 90/3949 [00:29<19:20,  3.32it/s]  2%|▏         | 94/3949 [00:29<22:58,  2.80it/s]  2%|▏         | 90/3949 [00:29<23:10,  2.77it/s]  2%|▏         | 90/3949 [00:29<19:42,  3.26it/s]  2%|▏         | 88/3949 [00:29<21:25,  3.00it/s]  2%|▏         | 89/3949 [00:29<20:21,  3.16it/s]  2%|▏         | 91/3949 [00:29<23:03,  2.79it/s]  2%|▏         | 91/3949 [00:29<19:41,  3.27it/s]  2%|▏         | 91/3949 [00:29<19:44,  3.26it/s]  2%|▏         | 91/3949 [00:29<22:49,  2.82it/s]  2%|▏         | 95/3949 [00:29<23:23,  2.75it/s]  2%|▏         | 89/3949 [00:29<20:51,  3.08it/s]  2%|▏         | 92/3949 [00:29<17:49,  3.61it/s]  2%|▏         | 90/3949 [00:29<20:33,  3.13it/s]  2%|▏         | 92/3949 [00:29<22:55,  2.80it/s]  2%|▏         | 92/3949 [00:29<19:55,  3.23it/s]  2%|▏         | 96/3949 [00:29<23:34,  2.72it/s]  2%|▏         | 92/3949 [00:29<23:34,  2.73it/s]  2%|▏         | 90/3949 [00:30<20:52,  3.08it/s]  2%|▏         | 93/3949 [00:30<18:14,  3.52it/s]  2%|▏         | 91/3949 [00:30<20:04,  3.20it/s]  2%|▏         | 93/3949 [00:30<20:07,  3.19it/s]  2%|▏         | 93/3949 [00:30<23:38,  2.72it/s]  2%|▏         | 94/3949 [00:30<17:29,  3.67it/s]  2%|▏         | 93/3949 [00:30<23:18,  2.76it/s]  2%|▏         | 97/3949 [00:30<23:44,  2.70it/s]  2%|▏         | 91/3949 [00:30<21:19,  3.02it/s]  2%|▏         | 92/3949 [00:30<21:16,  3.02it/s]  2%|▏         | 94/3949 [00:30<20:24,  3.15it/s]  2%|▏         | 94/3949 [00:30<23:00,  2.79it/s]  2%|▏         | 95/3949 [00:30<18:14,  3.52it/s]  2%|▏         | 94/3949 [00:30<23:07,  2.78it/s]  2%|▏         | 93/3949 [00:30<20:08,  3.19it/s]  2%|▏         | 98/3949 [00:30<23:58,  2.68it/s]  2%|▏         | 92/3949 [00:30<22:00,  2.92it/s]  2%|▏         | 95/3949 [00:30<20:23,  3.15it/s]  2%|▏         | 95/3949 [00:30<22:23,  2.87it/s]  2%|▏         | 96/3949 [00:30<19:05,  3.36it/s]  2%|▏         | 95/3949 [00:31<22:38,  2.84it/s]  2%|▏         | 94/3949 [00:31<20:49,  3.09it/s]  2%|▏         | 93/3949 [00:31<22:39,  2.84it/s]  3%|▎         | 99/3949 [00:31<24:03,  2.67it/s]  2%|▏         | 96/3949 [00:31<20:14,  3.17it/s]  2%|▏         | 97/3949 [00:31<18:18,  3.51it/s]  2%|▏         | 96/3949 [00:31<23:11,  2.77it/s]  2%|▏         | 96/3949 [00:31<22:39,  2.83it/s]  2%|▏         | 95/3949 [00:31<20:45,  3.09it/s]  2%|▏         | 94/3949 [00:31<21:32,  2.98it/s]  3%|▎         | 100/3949 [00:31<23:13,  2.76it/s]  2%|▏         | 97/3949 [00:31<20:17,  3.17it/s]  2%|▏         | 98/3949 [00:31<18:33,  3.46it/s]  2%|▏         | 97/3949 [00:31<23:27,  2.74it/s]  3%|▎         | 99/3949 [00:31<17:45,  3.61it/s]  2%|▏         | 96/3949 [00:31<21:01,  3.05it/s]  2%|▏         | 97/3949 [00:31<22:55,  2.80it/s]  2%|▏         | 98/3949 [00:31<20:28,  3.13it/s]  3%|▎         | 101/3949 [00:31<23:22,  2.74it/s]  2%|▏         | 95/3949 [00:31<23:06,  2.78it/s]  2%|▏         | 98/3949 [00:32<23:32,  2.73it/s]  2%|▏         | 97/3949 [00:32<20:31,  3.13it/s]  3%|▎         | 100/3949 [00:32<18:21,  3.49it/s]  2%|▏         | 96/3949 [00:32<21:33,  2.98it/s]  2%|▏         | 98/3949 [00:32<23:30,  2.73it/s]  3%|▎         | 99/3949 [00:32<20:43,  3.10it/s]  3%|▎         | 102/3949 [00:32<23:31,  2.73it/s]  2%|▏         | 98/3949 [00:32<19:45,  3.25it/s]  3%|▎         | 101/3949 [00:32<18:15,  3.51it/s]  3%|▎         | 99/3949 [00:32<24:03,  2.67it/s]  3%|▎         | 100/3949 [00:32<19:57,  3.21it/s]  2%|▏         | 97/3949 [00:32<22:09,  2.90it/s]  3%|▎         | 99/3949 [00:32<23:37,  2.72it/s]  3%|▎         | 103/3949 [00:32<22:56,  2.79it/s]  3%|▎         | 99/3949 [00:32<19:33,  3.28it/s]  3%|▎         | 102/3949 [00:32<18:37,  3.44it/s]  3%|▎         | 101/3949 [00:32<20:02,  3.20it/s]  2%|▏         | 98/3949 [00:32<21:01,  3.05it/s]  3%|▎         | 100/3949 [00:32<24:06,  2.66it/s]  3%|▎         | 103/3949 [00:32<18:18,  3.50it/s]  3%|▎         | 100/3949 [00:32<24:17,  2.64it/s]  3%|▎         | 104/3949 [00:32<23:33,  2.72it/s]  3%|▎         | 100/3949 [00:32<19:56,  3.22it/s]  3%|▎         | 102/3949 [00:33<20:51,  3.07it/s]  3%|▎         | 99/3949 [00:33<21:27,  2.99it/s]  3%|▎         | 101/3949 [00:33<23:55,  2.68it/s]  3%|▎         | 104/3949 [00:33<19:03,  3.36it/s]  3%|▎         | 105/3949 [00:33<23:05,  2.77it/s]  3%|▎         | 101/3949 [00:33<20:21,  3.15it/s]  3%|▎         | 101/3949 [00:33<24:10,  2.65it/s]  3%|▎         | 103/3949 [00:33<20:08,  3.18it/s]  3%|▎         | 100/3949 [00:33<21:18,  3.01it/s]  3%|▎         | 102/3949 [00:33<23:31,  2.73it/s]  3%|▎         | 105/3949 [00:33<18:58,  3.38it/s]  3%|▎         | 102/3949 [00:33<19:44,  3.25it/s]  3%|▎         | 106/3949 [00:33<23:42,  2.70it/s]  3%|▎         | 102/3949 [00:33<24:13,  2.65it/s]  3%|▎         | 104/3949 [00:33<19:44,  3.25it/s]  3%|▎         | 101/3949 [00:33<21:42,  2.96it/s]  3%|▎         | 106/3949 [00:33<19:09,  3.34it/s]  3%|▎         | 103/3949 [00:33<19:54,  3.22it/s]  3%|▎         | 103/3949 [00:33<23:27,  2.73it/s]  3%|▎         | 105/3949 [00:33<20:23,  3.14it/s]  3%|▎         | 103/3949 [00:34<24:06,  2.66it/s]  3%|▎         | 107/3949 [00:34<23:58,  2.67it/s]  3%|▎         | 102/3949 [00:34<20:39,  3.10it/s]  3%|▎         | 107/3949 [00:34<19:05,  3.35it/s]  3%|▎         | 104/3949 [00:34<19:12,  3.34it/s]  3%|▎         | 104/3949 [00:34<23:37,  2.71it/s]  3%|▎         | 106/3949 [00:34<20:50,  3.07it/s]  3%|▎         | 108/3949 [00:34<23:30,  2.72it/s]  3%|▎         | 104/3949 [00:34<23:46,  2.70it/s]  3%|▎         | 103/3949 [00:34<21:09,  3.03it/s]  3%|▎         | 105/3949 [00:34<19:45,  3.24it/s]  3%|▎         | 108/3949 [00:34<20:11,  3.17it/s]  3%|▎         | 105/3949 [00:34<22:45,  2.81it/s]  3%|▎         | 107/3949 [00:34<20:14,  3.16it/s]  3%|▎         | 109/3949 [00:34<23:19,  2.74it/s]  3%|▎         | 105/3949 [00:34<23:31,  2.72it/s]  3%|▎         | 106/3949 [00:34<19:11,  3.34it/s]  3%|▎         | 104/3949 [00:34<21:50,  2.93it/s]  3%|▎         | 109/3949 [00:34<20:26,  3.13it/s]  3%|▎         | 106/3949 [00:34<22:41,  2.82it/s]  3%|▎         | 108/3949 [00:34<20:46,  3.08it/s]  3%|▎         | 107/3949 [00:35<18:56,  3.38it/s]  3%|▎         | 106/3949 [00:35<22:51,  2.80it/s]  3%|▎         | 110/3949 [00:35<20:25,  3.13it/s]  3%|▎         | 110/3949 [00:35<23:50,  2.68it/s]  3%|▎         | 105/3949 [00:35<22:05,  2.90it/s]  3%|▎         | 107/3949 [00:35<23:07,  2.77it/s]  3%|▎         | 109/3949 [00:35<21:00,  3.05it/s]  3%|▎         | 108/3949 [00:35<19:23,  3.30it/s]  3%|▎         | 107/3949 [00:35<22:43,  2.82it/s]  3%|▎         | 106/3949 [00:35<21:20,  3.00it/s]  3%|▎         | 111/3949 [00:35<20:38,  3.10it/s]  3%|▎         | 111/3949 [00:35<23:04,  2.77it/s]  3%|▎         | 108/3949 [00:35<22:51,  2.80it/s]  3%|▎         | 110/3949 [00:35<20:47,  3.08it/s]  3%|▎         | 109/3949 [00:35<19:23,  3.30it/s]  3%|▎         | 112/3949 [00:35<19:39,  3.25it/s]  3%|▎         | 107/3949 [00:35<21:33,  2.97it/s]  3%|▎         | 108/3949 [00:35<23:14,  2.75it/s]  3%|▎         | 112/3949 [00:35<22:53,  2.79it/s]  3%|▎         | 110/3949 [00:35<19:00,  3.37it/s]  3%|▎         | 111/3949 [00:35<21:06,  3.03it/s]  3%|▎         | 109/3949 [00:36<23:26,  2.73it/s]  3%|▎         | 113/3949 [00:36<20:02,  3.19it/s]  3%|▎         | 108/3949 [00:36<21:22,  3.00it/s]  3%|▎         | 113/3949 [00:36<22:20,  2.86it/s]  3%|▎         | 109/3949 [00:36<23:03,  2.78it/s]  3%|▎         | 112/3949 [00:36<20:33,  3.11it/s]  3%|▎         | 111/3949 [00:36<20:28,  3.12it/s]  3%|▎         | 110/3949 [00:36<22:32,  2.84it/s]  3%|▎         | 114/3949 [00:36<20:32,  3.11it/s]  3%|▎         | 109/3949 [00:36<21:17,  3.01it/s]  3%|▎         | 110/3949 [00:36<22:14,  2.88it/s]  3%|▎         | 114/3949 [00:36<23:41,  2.70it/s]  3%|▎         | 112/3949 [00:36<19:30,  3.28it/s]  3%|▎         | 113/3949 [00:36<20:24,  3.13it/s]  3%|▎         | 115/3949 [00:36<20:41,  3.09it/s]  3%|▎         | 111/3949 [00:36<22:52,  2.80it/s]  3%|▎         | 110/3949 [00:36<20:52,  3.07it/s]  3%|▎         | 111/3949 [00:36<22:26,  2.85it/s]  3%|▎         | 115/3949 [00:36<22:37,  2.82it/s]  3%|▎         | 113/3949 [00:36<20:14,  3.16it/s]  3%|▎         | 114/3949 [00:36<20:51,  3.06it/s]  3%|▎         | 116/3949 [00:37<20:33,  3.11it/s]  3%|▎         | 112/3949 [00:37<22:44,  2.81it/s]  3%|▎         | 111/3949 [00:37<20:53,  3.06it/s]  3%|▎         | 112/3949 [00:37<22:16,  2.87it/s]  3%|▎         | 115/3949 [00:37<20:00,  3.19it/s]  3%|▎         | 116/3949 [00:37<22:42,  2.81it/s]  3%|▎         | 114/3949 [00:37<20:30,  3.12it/s]  3%|▎         | 117/3949 [00:37<20:28,  3.12it/s]  3%|▎         | 112/3949 [00:37<20:27,  3.13it/s]  3%|▎         | 113/3949 [00:37<22:30,  2.84it/s]  3%|▎         | 113/3949 [00:37<22:22,  2.86it/s]  3%|▎         | 116/3949 [00:37<19:56,  3.20it/s]  3%|▎         | 117/3949 [00:37<22:07,  2.89it/s]  3%|▎         | 115/3949 [00:37<21:30,  2.97it/s]  3%|▎         | 118/3949 [00:37<20:06,  3.17it/s]  3%|▎         | 113/3949 [00:37<20:14,  3.16it/s]  3%|▎         | 114/3949 [00:37<22:04,  2.90it/s]  3%|▎         | 117/3949 [00:37<19:47,  3.23it/s]  3%|▎         | 114/3949 [00:37<22:03,  2.90it/s]  3%|▎         | 116/3949 [00:37<20:40,  3.09it/s]  3%|▎         | 118/3949 [00:37<22:32,  2.83it/s]  3%|▎         | 119/3949 [00:37<20:15,  3.15it/s]  3%|▎         | 114/3949 [00:38<20:44,  3.08it/s]  3%|▎         | 115/3949 [00:38<22:41,  2.82it/s]  3%|▎         | 118/3949 [00:38<19:13,  3.32it/s]  3%|▎         | 117/3949 [00:38<20:36,  3.10it/s]  3%|▎         | 115/3949 [00:38<22:44,  2.81it/s]  3%|▎         | 119/3949 [00:38<22:11,  2.88it/s]  3%|▎         | 120/3949 [00:38<20:41,  3.08it/s]  3%|▎         | 115/3949 [00:38<20:25,  3.13it/s]  3%|▎         | 119/3949 [00:38<20:00,  3.19it/s]  3%|▎         | 116/3949 [00:38<22:47,  2.80it/s]  3%|▎         | 118/3949 [00:38<19:55,  3.21it/s]  3%|▎         | 116/3949 [00:38<21:56,  2.91it/s]  3%|▎         | 121/3949 [00:38<20:13,  3.15it/s]  3%|▎         | 120/3949 [00:38<22:30,  2.83it/s]  3%|▎         | 116/3949 [00:38<20:40,  3.09it/s]  3%|▎         | 120/3949 [00:38<19:37,  3.25it/s]  3%|▎         | 117/3949 [00:38<22:14,  2.87it/s]  3%|▎         | 119/3949 [00:38<20:41,  3.08it/s]  3%|▎         | 122/3949 [00:38<20:12,  3.16it/s]  3%|▎         | 117/3949 [00:38<22:36,  2.82it/s]  3%|▎         | 121/3949 [00:38<22:29,  2.84it/s]  3%|▎         | 117/3949 [00:39<21:02,  3.04it/s]  3%|▎         | 121/3949 [00:39<19:20,  3.30it/s]  3%|▎         | 120/3949 [00:39<20:23,  3.13it/s]  3%|▎         | 118/3949 [00:39<23:09,  2.76it/s]  3%|▎         | 123/3949 [00:39<19:30,  3.27it/s]  3%|▎         | 122/3949 [00:39<19:00,  3.35it/s]  3%|▎         | 122/3949 [00:39<22:52,  2.79it/s]  3%|▎         | 118/3949 [00:39<21:11,  3.01it/s]  3%|▎         | 118/3949 [00:39<23:58,  2.66it/s]  3%|▎         | 121/3949 [00:39<20:11,  3.16it/s]  3%|▎         | 124/3949 [00:39<19:52,  3.21it/s]  3%|▎         | 119/3949 [00:39<23:39,  2.70it/s]  3%|▎         | 123/3949 [00:39<19:05,  3.34it/s]  3%|▎         | 119/3949 [00:39<20:47,  3.07it/s]  3%|▎         | 119/3949 [00:39<23:18,  2.74it/s]  3%|▎         | 123/3949 [00:39<23:20,  2.73it/s]  3%|▎         | 122/3949 [00:39<19:41,  3.24it/s]  3%|▎         | 125/3949 [00:39<19:28,  3.27it/s]  3%|▎         | 120/3949 [00:39<23:16,  2.74it/s]  3%|▎         | 124/3949 [00:39<19:44,  3.23it/s]  3%|▎         | 120/3949 [00:40<21:40,  2.94it/s]  3%|▎         | 120/3949 [00:40<23:28,  2.72it/s]  3%|▎         | 124/3949 [00:40<23:14,  2.74it/s]  3%|▎         | 123/3949 [00:40<19:42,  3.24it/s]  3%|▎         | 126/3949 [00:40<20:01,  3.18it/s]  3%|▎         | 125/3949 [00:40<19:02,  3.35it/s]  3%|▎         | 121/3949 [00:40<22:30,  2.83it/s]  3%|▎         | 121/3949 [00:40<21:09,  3.02it/s]  3%|▎         | 125/3949 [00:40<22:29,  2.83it/s]  3%|▎         | 121/3949 [00:40<23:05,  2.76it/s]  3%|▎         | 127/3949 [00:40<19:18,  3.30it/s]  3%|▎         | 124/3949 [00:40<20:18,  3.14it/s]  3%|▎         | 126/3949 [00:40<19:49,  3.21it/s]  3%|▎         | 122/3949 [00:40<22:13,  2.87it/s]  3%|▎         | 122/3949 [00:40<21:18,  2.99it/s]  3%|▎         | 122/3949 [00:40<22:23,  2.85it/s]  3%|▎         | 128/3949 [00:40<19:41,  3.24it/s]  3%|▎         | 125/3949 [00:40<20:33,  3.10it/s]  3%|▎         | 126/3949 [00:40<23:33,  2.71it/s]  3%|▎         | 127/3949 [00:40<19:55,  3.20it/s]  3%|▎         | 123/3949 [00:40<22:59,  2.77it/s]  3%|▎         | 123/3949 [00:40<20:52,  3.06it/s]  3%|▎         | 126/3949 [00:41<20:23,  3.12it/s]  3%|▎         | 129/3949 [00:41<20:21,  3.13it/s]  3%|▎         | 123/3949 [00:41<22:35,  2.82it/s]  3%|▎         | 127/3949 [00:41<22:53,  2.78it/s]  3%|▎         | 128/3949 [00:41<19:27,  3.27it/s]  3%|▎         | 124/3949 [00:41<20:25,  3.12it/s]  3%|▎         | 124/3949 [00:41<22:42,  2.81it/s]  3%|▎         | 127/3949 [00:41<20:43,  3.07it/s]  3%|▎         | 130/3949 [00:41<20:38,  3.08it/s]  3%|▎         | 124/3949 [00:41<22:32,  2.83it/s]  3%|▎         | 129/3949 [00:41<19:16,  3.30it/s]  3%|▎         | 128/3949 [00:41<22:44,  2.80it/s]  3%|▎         | 125/3949 [00:41<20:32,  3.10it/s]  3%|▎         | 125/3949 [00:41<22:44,  2.80it/s]  3%|▎         | 131/3949 [00:41<20:17,  3.14it/s]  3%|▎         | 128/3949 [00:41<20:56,  3.04it/s]  3%|▎         | 125/3949 [00:41<22:31,  2.83it/s]  3%|▎         | 129/3949 [00:41<21:47,  2.92it/s]  3%|▎         | 130/3949 [00:41<20:18,  3.14it/s]  3%|▎         | 126/3949 [00:41<20:24,  3.12it/s]  3%|▎         | 126/3949 [00:41<21:21,  2.98it/s]  3%|▎         | 126/3949 [00:42<20:54,  3.05it/s]  3%|▎         | 132/3949 [00:42<21:00,  3.03it/s]  3%|▎         | 129/3949 [00:42<21:28,  2.96it/s]  3%|▎         | 130/3949 [00:42<21:10,  3.01it/s]  3%|▎         | 131/3949 [00:42<20:41,  3.08it/s]  3%|▎         | 127/3949 [00:42<20:30,  3.11it/s]  3%|▎         | 127/3949 [00:42<21:15,  3.00it/s]  3%|▎         | 127/3949 [00:42<20:21,  3.13it/s]  3%|▎         | 131/3949 [00:42<20:37,  3.09it/s]  3%|▎         | 133/3949 [00:42<21:22,  2.97it/s]  3%|▎         | 130/3949 [00:42<21:58,  2.90it/s]  3%|▎         | 132/3949 [00:42<21:28,  2.96it/s]  3%|▎         | 128/3949 [00:42<20:34,  3.09it/s]  3%|▎         | 128/3949 [00:42<21:46,  2.93it/s]  3%|▎         | 128/3949 [00:42<20:21,  3.13it/s]  3%|▎         | 132/3949 [00:42<19:43,  3.23it/s]  3%|▎         | 134/3949 [00:42<21:43,  2.93it/s]  3%|▎         | 131/3949 [00:42<21:48,  2.92it/s]  3%|▎         | 129/3949 [00:42<19:27,  3.27it/s]  3%|▎         | 133/3949 [00:42<21:48,  2.92it/s]  3%|▎         | 133/3949 [00:43<19:33,  3.25it/s]  3%|▎         | 129/3949 [00:43<21:49,  2.92it/s]  3%|▎         | 130/3949 [00:43<17:59,  3.54it/s]  3%|▎         | 129/3949 [00:43<21:50,  2.92it/s]  3%|▎         | 135/3949 [00:43<22:50,  2.78it/s]  3%|▎         | 132/3949 [00:43<22:52,  2.78it/s]  3%|▎         | 134/3949 [00:43<18:21,  3.46it/s]  3%|▎         | 134/3949 [00:43<22:39,  2.81it/s]  3%|▎         | 131/3949 [00:43<17:53,  3.56it/s]  3%|▎         | 130/3949 [00:43<22:35,  2.82it/s]  3%|▎         | 130/3949 [00:43<21:11,  3.00it/s]  3%|▎         | 135/3949 [00:43<17:54,  3.55it/s]  3%|▎         | 136/3949 [00:43<22:49,  2.79it/s]  3%|▎         | 133/3949 [00:43<23:39,  2.69it/s]  3%|▎         | 132/3949 [00:43<17:57,  3.54it/s]  3%|▎         | 135/3949 [00:43<23:17,  2.73it/s]  3%|▎         | 131/3949 [00:43<21:33,  2.95it/s]  3%|▎         | 131/3949 [00:43<22:49,  2.79it/s]  3%|▎         | 136/3949 [00:43<18:15,  3.48it/s]  3%|▎         | 133/3949 [00:43<17:15,  3.69it/s]  3%|▎         | 137/3949 [00:43<22:54,  2.77it/s]  3%|▎         | 134/3949 [00:44<24:13,  2.62it/s]  3%|▎         | 136/3949 [00:44<23:21,  2.72it/s]  3%|▎         | 132/3949 [00:44<21:02,  3.02it/s]  3%|▎         | 137/3949 [00:44<18:02,  3.52it/s]  3%|▎         | 132/3949 [00:44<23:17,  2.73it/s]  3%|▎         | 134/3949 [00:44<17:53,  3.55it/s]  3%|▎         | 138/3949 [00:44<22:45,  2.79it/s]  3%|▎         | 135/3949 [00:44<23:28,  2.71it/s]  3%|▎         | 133/3949 [00:44<20:50,  3.05it/s]  3%|▎         | 137/3949 [00:44<23:33,  2.70it/s]  3%|▎         | 138/3949 [00:44<18:55,  3.36it/s]  3%|▎         | 135/3949 [00:44<17:45,  3.58it/s]  3%|▎         | 133/3949 [00:44<23:29,  2.71it/s]  4%|▎         | 139/3949 [00:44<22:21,  2.84it/s]  3%|▎         | 134/3949 [00:44<21:00,  3.03it/s]  4%|▎         | 139/3949 [00:44<19:00,  3.34it/s]  3%|▎         | 136/3949 [00:44<23:45,  2.68it/s]  3%|▎         | 136/3949 [00:44<17:46,  3.58it/s]  3%|▎         | 138/3949 [00:44<23:33,  2.70it/s]  3%|▎         | 134/3949 [00:44<23:05,  2.75it/s]  4%|▎         | 140/3949 [00:44<18:12,  3.49it/s]  4%|▎         | 140/3949 [00:45<23:08,  2.74it/s]  3%|▎         | 135/3949 [00:45<20:37,  3.08it/s]  3%|▎         | 137/3949 [00:45<17:49,  3.56it/s]  3%|▎         | 137/3949 [00:45<23:37,  2.69it/s]  4%|▎         | 139/3949 [00:45<24:06,  2.63it/s]  3%|▎         | 138/3949 [00:45<16:20,  3.89it/s]  3%|▎         | 135/3949 [00:45<23:47,  2.67it/s]  4%|▎         | 141/3949 [00:45<18:24,  3.45it/s]  3%|▎         | 136/3949 [00:45<21:05,  3.01it/s]  4%|▎         | 141/3949 [00:45<23:41,  2.68it/s]  4%|▎         | 139/3949 [00:45<15:30,  4.09it/s]  3%|▎         | 138/3949 [00:45<24:50,  2.56it/s]  4%|▎         | 140/3949 [00:45<24:36,  2.58it/s]  4%|▎         | 142/3949 [00:45<19:26,  3.26it/s]  4%|▎         | 140/3949 [00:45<14:31,  4.37it/s]  3%|▎         | 136/3949 [00:45<23:58,  2.65it/s]  3%|▎         | 137/3949 [00:45<20:52,  3.04it/s]  4%|▎         | 142/3949 [00:45<23:30,  2.70it/s]  4%|▎         | 141/3949 [00:45<14:33,  4.36it/s]  4%|▎         | 139/3949 [00:45<24:49,  2.56it/s]  4%|▎         | 143/3949 [00:45<20:15,  3.13it/s]  4%|▎         | 141/3949 [00:45<24:33,  2.58it/s]  4%|▎         | 142/3949 [00:46<12:51,  4.93it/s]  3%|▎         | 138/3949 [00:46<21:14,  2.99it/s]  3%|▎         | 137/3949 [00:46<25:07,  2.53it/s]  4%|▎         | 143/3949 [00:46<12:28,  5.09it/s]  4%|▎         | 143/3949 [00:46<24:52,  2.55it/s]  4%|▎         | 144/3949 [00:46<19:13,  3.30it/s]  4%|▎         | 144/3949 [00:46<11:53,  5.33it/s]  4%|▎         | 140/3949 [00:46<26:01,  2.44it/s]  4%|▎         | 142/3949 [00:46<25:21,  2.50it/s]  4%|▎         | 139/3949 [00:46<21:42,  2.93it/s]  3%|▎         | 138/3949 [00:46<24:34,  2.58it/s]  4%|▎         | 145/3949 [00:46<19:23,  3.27it/s]  4%|▎         | 145/3949 [00:46<12:21,  5.13it/s]  4%|▎         | 144/3949 [00:46<24:36,  2.58it/s]  4%|▎         | 141/3949 [00:46<25:22,  2.50it/s]  4%|▎         | 140/3949 [00:46<22:11,  2.86it/s]  4%|▎         | 143/3949 [00:46<25:17,  2.51it/s]  4%|▎         | 146/3949 [00:46<13:18,  4.76it/s]  4%|▎         | 139/3949 [00:46<24:57,  2.55it/s]  4%|▎         | 146/3949 [00:46<19:46,  3.21it/s]  4%|▎         | 145/3949 [00:46<24:30,  2.59it/s]  4%|▎         | 141/3949 [00:47<21:36,  2.94it/s]  4%|▎         | 142/3949 [00:47<24:16,  2.61it/s]  4%|▎         | 147/3949 [00:47<15:22,  4.12it/s]  4%|▎         | 144/3949 [00:47<24:21,  2.60it/s]  4%|▎         | 147/3949 [00:47<19:28,  3.25it/s]  4%|▎         | 140/3949 [00:47<24:20,  2.61it/s]  4%|▎         | 146/3949 [00:47<24:02,  2.64it/s]  4%|▎         | 142/3949 [00:47<21:07,  3.00it/s]  4%|▎         | 148/3949 [00:47<16:22,  3.87it/s]  4%|▎         | 143/3949 [00:47<23:32,  2.69it/s]  4%|▎         | 148/3949 [00:47<19:34,  3.24it/s]  4%|▎         | 145/3949 [00:47<24:12,  2.62it/s]  4%|▎         | 141/3949 [00:47<23:54,  2.66it/s]  4%|▎         | 147/3949 [00:47<23:10,  2.73it/s]  4%|▎         | 143/3949 [00:47<20:24,  3.11it/s]  4%|▍         | 149/3949 [00:47<16:40,  3.80it/s]  4%|▍         | 149/3949 [00:47<18:56,  3.34it/s]  4%|▎         | 144/3949 [00:47<23:06,  2.74it/s]  4%|▎         | 146/3949 [00:47<23:43,  2.67it/s]  4%|▎         | 142/3949 [00:47<23:35,  2.69it/s]  4%|▍         | 150/3949 [00:48<17:53,  3.54it/s]  4%|▍         | 150/3949 [00:48<17:20,  3.65it/s]  4%|▎         | 148/3949 [00:48<23:05,  2.74it/s]  4%|▎         | 144/3949 [00:48<20:35,  3.08it/s]  4%|▎         | 145/3949 [00:48<22:59,  2.76it/s]  4%|▍         | 151/3949 [00:48<17:04,  3.71it/s]  4%|▎         | 147/3949 [00:48<23:34,  2.69it/s]  4%|▍         | 151/3949 [00:48<17:19,  3.65it/s]  4%|▎         | 145/3949 [00:48<19:47,  3.20it/s]  4%|▎         | 143/3949 [00:48<23:42,  2.68it/s]  4%|▍         | 149/3949 [00:48<24:28,  2.59it/s]  4%|▍         | 152/3949 [00:48<16:04,  3.94it/s]  4%|▎         | 146/3949 [00:48<23:09,  2.74it/s]  4%|▍         | 152/3949 [00:48<17:26,  3.63it/s]  4%|▍         | 153/3949 [00:48<14:15,  4.44it/s]  4%|▎         | 148/3949 [00:48<24:52,  2.55it/s]  4%|▎         | 144/3949 [00:48<23:59,  2.64it/s]  4%|▎         | 146/3949 [00:48<21:17,  2.98it/s]  4%|▍         | 154/3949 [00:48<12:55,  4.89it/s]  4%|▍         | 150/3949 [00:48<24:18,  2.60it/s]  4%|▍         | 155/3949 [00:48<11:40,  5.42it/s]  4%|▍         | 153/3949 [00:48<18:56,  3.34it/s]  4%|▎         | 147/3949 [00:48<24:31,  2.58it/s]  4%|▎         | 147/3949 [00:49<21:34,  2.94it/s]  4%|▍         | 149/3949 [00:49<25:08,  2.52it/s]  4%|▎         | 145/3949 [00:49<24:18,  2.61it/s]  4%|▍         | 156/3949 [00:49<12:01,  5.25it/s]  4%|▍         | 154/3949 [00:49<19:37,  3.22it/s]  4%|▍         | 151/3949 [00:49<24:41,  2.56it/s]  4%|▎         | 148/3949 [00:49<24:49,  2.55it/s]  4%|▍         | 157/3949 [00:49<13:42,  4.61it/s]  4%|▎         | 148/3949 [00:49<21:49,  2.90it/s]  4%|▍         | 150/3949 [00:49<24:12,  2.61it/s]  4%|▎         | 146/3949 [00:49<24:15,  2.61it/s]  4%|▍         | 155/3949 [00:49<19:31,  3.24it/s]  4%|▍         | 152/3949 [00:49<24:43,  2.56it/s]  4%|▍         | 158/3949 [00:49<14:54,  4.24it/s]  4%|▍         | 149/3949 [00:49<20:28,  3.09it/s]  4%|▍         | 149/3949 [00:49<24:16,  2.61it/s]  4%|▍         | 151/3949 [00:49<23:44,  2.67it/s]  4%|▍         | 156/3949 [00:49<19:00,  3.33it/s]  4%|▎         | 147/3949 [00:49<24:21,  2.60it/s]  4%|▍         | 159/3949 [00:49<15:27,  4.09it/s]  4%|▍         | 153/3949 [00:50<24:24,  2.59it/s]  4%|▍         | 150/3949 [00:50<20:29,  3.09it/s]  4%|▍         | 150/3949 [00:50<24:00,  2.64it/s]  4%|▍         | 157/3949 [00:50<18:49,  3.36it/s]  4%|▍         | 152/3949 [00:50<23:40,  2.67it/s]  4%|▍         | 160/3949 [00:50<15:23,  4.10it/s]  4%|▎         | 148/3949 [00:50<23:44,  2.67it/s]  4%|▍         | 154/3949 [00:50<24:16,  2.61it/s]  4%|▍         | 151/3949 [00:50<21:24,  2.96it/s]  4%|▍         | 161/3949 [00:50<15:09,  4.17it/s]  4%|▍         | 158/3949 [00:50<19:11,  3.29it/s]  4%|▍         | 151/3949 [00:50<23:32,  2.69it/s]  4%|▍         | 153/3949 [00:50<23:26,  2.70it/s]  4%|▍         | 162/3949 [00:50<15:08,  4.17it/s]  4%|▍         | 149/3949 [00:50<25:26,  2.49it/s]  4%|▍         | 152/3949 [00:50<21:30,  2.94it/s]  4%|▍         | 159/3949 [00:50<18:56,  3.33it/s]  4%|▍         | 163/3949 [00:50<13:09,  4.79it/s]  4%|▍         | 155/3949 [00:50<24:55,  2.54it/s]  4%|▍         | 152/3949 [00:50<24:00,  2.64it/s]  4%|▍         | 164/3949 [00:50<11:54,  5.29it/s]  4%|▍         | 154/3949 [00:50<24:10,  2.62it/s]  4%|▍         | 150/3949 [00:51<25:10,  2.52it/s]  4%|▍         | 165/3949 [00:51<11:46,  5.36it/s]  4%|▍         | 153/3949 [00:51<22:17,  2.84it/s]  4%|▍         | 160/3949 [00:51<20:24,  3.10it/s]  4%|▍         | 156/3949 [00:51<24:49,  2.55it/s]  4%|▍         | 153/3949 [00:51<24:03,  2.63it/s]  4%|▍         | 166/3949 [00:51<11:04,  5.69it/s]  4%|▍         | 155/3949 [00:51<24:55,  2.54it/s]  4%|▍         | 167/3949 [00:51<10:30,  6.00it/s]  4%|▍         | 154/3949 [00:51<22:18,  2.83it/s]  4%|▍         | 161/3949 [00:51<21:07,  2.99it/s]  4%|▍         | 151/3949 [00:51<25:19,  2.50it/s]  4%|▍         | 168/3949 [00:51<10:11,  6.19it/s]  4%|▍         | 157/3949 [00:51<25:20,  2.49it/s]  4%|▍         | 154/3949 [00:51<25:13,  2.51it/s]  4%|▍         | 156/3949 [00:51<24:45,  2.55it/s]  4%|▍         | 169/3949 [00:51<11:01,  5.71it/s]  4%|▍         | 155/3949 [00:51<22:04,  2.86it/s]  4%|▍         | 162/3949 [00:51<21:19,  2.96it/s]  4%|▍         | 152/3949 [00:51<24:50,  2.55it/s]  4%|▍         | 158/3949 [00:52<24:49,  2.54it/s]  4%|▍         | 170/3949 [00:52<12:23,  5.08it/s]  4%|▍         | 155/3949 [00:52<24:56,  2.54it/s]  4%|▍         | 163/3949 [00:52<20:25,  3.09it/s]  4%|▍         | 156/3949 [00:52<21:39,  2.92it/s]  4%|▍         | 157/3949 [00:52<24:39,  2.56it/s]  4%|▍         | 153/3949 [00:52<24:39,  2.57it/s]  4%|▍         | 171/3949 [00:52<13:36,  4.63it/s]  4%|▍         | 159/3949 [00:52<24:34,  2.57it/s]  4%|▍         | 164/3949 [00:52<19:09,  3.29it/s]  4%|▍         | 156/3949 [00:52<24:07,  2.62it/s]  4%|▍         | 157/3949 [00:52<20:46,  3.04it/s]  4%|▍         | 158/3949 [00:52<24:11,  2.61it/s]  4%|▍         | 172/3949 [00:52<14:58,  4.20it/s]  4%|▍         | 154/3949 [00:52<24:00,  2.63it/s]  4%|▍         | 165/3949 [00:52<19:14,  3.28it/s]  4%|▍         | 160/3949 [00:52<24:15,  2.60it/s]  4%|▍         | 158/3949 [00:52<20:33,  3.07it/s]  4%|▍         | 157/3949 [00:52<24:09,  2.62it/s]  4%|▍         | 173/3949 [00:52<15:37,  4.03it/s]  4%|▍         | 159/3949 [00:52<23:41,  2.67it/s]  4%|▍         | 166/3949 [00:52<18:42,  3.37it/s]  4%|▍         | 155/3949 [00:52<23:45,  2.66it/s]  4%|▍         | 159/3949 [00:53<19:59,  3.16it/s]  4%|▍         | 174/3949 [00:53<15:07,  4.16it/s]  4%|▍         | 161/3949 [00:53<23:31,  2.68it/s]  4%|▍         | 158/3949 [00:53<23:54,  2.64it/s]  4%|▍         | 175/3949 [00:53<14:15,  4.41it/s]  4%|▍         | 160/3949 [00:53<24:09,  2.61it/s]  4%|▍         | 167/3949 [00:53<19:55,  3.16it/s]  4%|▍         | 156/3949 [00:53<24:04,  2.63it/s]  4%|▍         | 160/3949 [00:53<20:29,  3.08it/s]  4%|▍         | 176/3949 [00:53<13:23,  4.70it/s]  4%|▍         | 162/3949 [00:53<24:01,  2.63it/s]  4%|▍         | 159/3949 [00:53<24:08,  2.62it/s]  4%|▍         | 177/3949 [00:53<12:42,  4.95it/s]  4%|▍         | 161/3949 [00:53<24:12,  2.61it/s]  4%|▍         | 168/3949 [00:53<20:41,  3.05it/s]  4%|▍         | 161/3949 [00:53<20:17,  3.11it/s]  5%|▍         | 178/3949 [00:53<12:09,  5.17it/s]  4%|▍         | 157/3949 [00:53<25:07,  2.52it/s]  4%|▍         | 163/3949 [00:53<24:09,  2.61it/s]  4%|▍         | 160/3949 [00:53<23:39,  2.67it/s]  5%|▍         | 179/3949 [00:53<12:19,  5.10it/s]  4%|▍         | 169/3949 [00:54<20:54,  3.01it/s]  4%|▍         | 162/3949 [00:54<24:13,  2.61it/s]  4%|▍         | 162/3949 [00:54<21:47,  2.90it/s]  4%|▍         | 158/3949 [00:54<24:05,  2.62it/s]  5%|▍         | 180/3949 [00:54<12:53,  4.87it/s]  4%|▍         | 164/3949 [00:54<24:23,  2.59it/s]  4%|▍         | 161/3949 [00:54<23:36,  2.67it/s]  4%|▍         | 170/3949 [00:54<19:47,  3.18it/s]  4%|▍         | 163/3949 [00:54<23:51,  2.64it/s]  4%|▍         | 163/3949 [00:54<21:06,  2.99it/s]  5%|▍         | 181/3949 [00:54<14:04,  4.46it/s]  4%|▍         | 159/3949 [00:54<24:20,  2.59it/s]  4%|▍         | 171/3949 [00:54<19:36,  3.21it/s]  4%|▍         | 162/3949 [00:54<22:50,  2.76it/s]  4%|▍         | 165/3949 [00:54<24:00,  2.63it/s]  4%|▍         | 164/3949 [00:54<20:19,  3.10it/s]  4%|▍         | 164/3949 [00:54<23:13,  2.72it/s]  5%|▍         | 182/3949 [00:54<15:05,  4.16it/s]  4%|▍         | 172/3949 [00:54<18:23,  3.42it/s]  4%|▍         | 160/3949 [00:54<23:42,  2.66it/s]  4%|▍         | 163/3949 [00:54<22:12,  2.84it/s]  4%|▍         | 166/3949 [00:55<23:49,  2.65it/s]  4%|▍         | 165/3949 [00:55<20:05,  3.14it/s]  5%|▍         | 183/3949 [00:55<16:13,  3.87it/s]  4%|▍         | 173/3949 [00:55<18:43,  3.36it/s]  4%|▍         | 165/3949 [00:55<23:58,  2.63it/s]  4%|▍         | 161/3949 [00:55<23:50,  2.65it/s]  4%|▍         | 164/3949 [00:55<22:13,  2.84it/s]  4%|▍         | 166/3949 [00:55<19:59,  3.15it/s]  4%|▍         | 167/3949 [00:55<23:14,  2.71it/s]  5%|▍         | 184/3949 [00:55<17:36,  3.56it/s]  4%|▍         | 174/3949 [00:55<18:27,  3.41it/s]  4%|▍         | 166/3949 [00:55<22:34,  2.79it/s]  4%|▍         | 162/3949 [00:55<23:49,  2.65it/s]  4%|▍         | 167/3949 [00:55<19:57,  3.16it/s]  4%|▍         | 165/3949 [00:55<22:36,  2.79it/s]  5%|▍         | 185/3949 [00:55<17:48,  3.52it/s]  4%|▍         | 175/3949 [00:55<18:12,  3.45it/s]  4%|▍         | 168/3949 [00:55<23:01,  2.74it/s]  4%|▍         | 167/3949 [00:55<23:13,  2.71it/s]  5%|▍         | 186/3949 [00:55<17:36,  3.56it/s]  4%|▍         | 168/3949 [00:55<20:03,  3.14it/s]  4%|▍         | 176/3949 [00:56<18:09,  3.46it/s]  4%|▍         | 163/3949 [00:56<23:37,  2.67it/s]  4%|▍         | 166/3949 [00:56<23:14,  2.71it/s]  4%|▍         | 169/3949 [00:56<23:01,  2.74it/s]  4%|▍         | 168/3949 [00:56<22:37,  2.78it/s]  5%|▍         | 187/3949 [00:56<17:05,  3.67it/s]  4%|▍         | 169/3949 [00:56<19:34,  3.22it/s]  4%|▍         | 177/3949 [00:56<18:12,  3.45it/s]  4%|▍         | 164/3949 [00:56<23:25,  2.69it/s]  4%|▍         | 167/3949 [00:56<23:21,  2.70it/s]  4%|▍         | 170/3949 [00:56<23:20,  2.70it/s]  5%|▍         | 188/3949 [00:56<17:03,  3.68it/s]  4%|▍         | 169/3949 [00:56<22:44,  2.77it/s]  4%|▍         | 170/3949 [00:56<19:41,  3.20it/s]  5%|▍         | 178/3949 [00:56<18:34,  3.38it/s]  5%|▍         | 189/3949 [00:56<16:49,  3.72it/s]  4%|▍         | 168/3949 [00:56<23:06,  2.73it/s]  4%|▍         | 165/3949 [00:56<24:24,  2.58it/s]  4%|▍         | 171/3949 [00:56<23:30,  2.68it/s]  5%|▍         | 179/3949 [00:56<18:18,  3.43it/s]  4%|▍         | 170/3949 [00:56<22:41,  2.78it/s]  4%|▍         | 171/3949 [00:56<20:30,  3.07it/s]  5%|▍         | 190/3949 [00:57<17:21,  3.61it/s]  4%|▍         | 166/3949 [00:57<22:49,  2.76it/s]  4%|▍         | 169/3949 [00:57<23:04,  2.73it/s]  5%|▍         | 180/3949 [00:57<18:36,  3.37it/s]  4%|▍         | 172/3949 [00:57<23:38,  2.66it/s]  4%|▍         | 172/3949 [00:57<20:01,  3.14it/s]  4%|▍         | 171/3949 [00:57<22:39,  2.78it/s]  5%|▍         | 191/3949 [00:57<17:09,  3.65it/s]  4%|▍         | 167/3949 [00:57<23:06,  2.73it/s]  5%|▍         | 181/3949 [00:57<18:34,  3.38it/s]  4%|▍         | 173/3949 [00:57<19:34,  3.21it/s]  4%|▍         | 170/3949 [00:57<23:16,  2.71it/s]  4%|▍         | 173/3949 [00:57<23:26,  2.69it/s]  5%|▍         | 192/3949 [00:57<17:21,  3.61it/s]  4%|▍         | 172/3949 [00:57<22:34,  2.79it/s]  5%|▍         | 182/3949 [00:57<19:17,  3.26it/s]  4%|▍         | 174/3949 [00:57<19:20,  3.25it/s]  4%|▍         | 168/3949 [00:57<23:41,  2.66it/s]  5%|▍         | 193/3949 [00:57<17:47,  3.52it/s]  4%|▍         | 171/3949 [00:57<23:07,  2.72it/s]  4%|▍         | 174/3949 [00:57<23:23,  2.69it/s]  4%|▍         | 173/3949 [00:58<22:53,  2.75it/s]  4%|▍         | 175/3949 [00:58<19:14,  3.27it/s]  5%|▍         | 183/3949 [00:58<19:29,  3.22it/s]  5%|▍         | 194/3949 [00:58<18:17,  3.42it/s]  4%|▍         | 169/3949 [00:58<23:12,  2.71it/s]  4%|▍         | 172/3949 [00:58<22:45,  2.77it/s]  4%|▍         | 175/3949 [00:58<22:47,  2.76it/s]  4%|▍         | 174/3949 [00:58<22:42,  2.77it/s]  4%|▍         | 176/3949 [00:58<18:34,  3.39it/s]  5%|▍         | 184/3949 [00:58<19:03,  3.29it/s]  5%|▍         | 195/3949 [00:58<18:13,  3.43it/s]  4%|▍         | 170/3949 [00:58<22:54,  2.75it/s]  4%|▍         | 173/3949 [00:58<22:33,  2.79it/s]  4%|▍         | 176/3949 [00:58<22:44,  2.76it/s]  5%|▍         | 185/3949 [00:58<18:23,  3.41it/s]  4%|▍         | 177/3949 [00:58<18:47,  3.34it/s]  4%|▍         | 175/3949 [00:58<23:02,  2.73it/s]  5%|▍         | 196/3949 [00:58<17:29,  3.58it/s]  4%|▍         | 171/3949 [00:58<23:21,  2.70it/s]  4%|▍         | 174/3949 [00:59<23:03,  2.73it/s]  5%|▍         | 178/3949 [00:59<18:48,  3.34it/s]  4%|▍         | 177/3949 [00:59<22:32,  2.79it/s]  5%|▍         | 186/3949 [00:59<18:49,  3.33it/s]  5%|▍         | 197/3949 [00:59<17:34,  3.56it/s]  4%|▍         | 176/3949 [00:59<23:12,  2.71it/s]  5%|▍         | 187/3949 [00:59<18:05,  3.47it/s]  5%|▌         | 198/3949 [00:59<17:49,  3.51it/s]  5%|▍         | 179/3949 [00:59<19:15,  3.26it/s]  4%|▍         | 172/3949 [00:59<23:18,  2.70it/s]  4%|▍         | 175/3949 [00:59<22:51,  2.75it/s]  5%|▍         | 178/3949 [00:59<23:09,  2.71it/s]  4%|▍         | 177/3949 [00:59<22:33,  2.79it/s]  5%|▍         | 188/3949 [00:59<17:40,  3.55it/s]  5%|▌         | 199/3949 [00:59<17:45,  3.52it/s]  5%|▍         | 180/3949 [00:59<19:37,  3.20it/s]  4%|▍         | 173/3949 [00:59<23:03,  2.73it/s]  4%|▍         | 176/3949 [00:59<22:56,  2.74it/s]  5%|▍         | 179/3949 [00:59<23:07,  2.72it/s]  5%|▍         | 189/3949 [00:59<16:44,  3.74it/s]  5%|▍         | 178/3949 [00:59<23:23,  2.69it/s]  5%|▌         | 200/3949 [00:59<18:32,  3.37it/s]  5%|▍         | 181/3949 [00:59<19:30,  3.22it/s]  5%|▍         | 190/3949 [01:00<17:16,  3.63it/s]  4%|▍         | 174/3949 [01:00<23:35,  2.67it/s]  4%|▍         | 177/3949 [01:00<23:19,  2.69it/s]  5%|▍         | 180/3949 [01:00<23:17,  2.70it/s]  5%|▍         | 179/3949 [01:00<22:44,  2.76it/s]  5%|▌         | 201/3949 [01:00<17:59,  3.47it/s]  5%|▍         | 182/3949 [01:00<19:14,  3.26it/s]  5%|▍         | 191/3949 [01:00<17:19,  3.62it/s]  4%|▍         | 175/3949 [01:00<22:47,  2.76it/s]  5%|▍         | 178/3949 [01:00<23:02,  2.73it/s]  5%|▌         | 202/3949 [01:00<17:44,  3.52it/s]  5%|▍         | 181/3949 [01:00<22:52,  2.74it/s]  5%|▍         | 180/3949 [01:00<22:38,  2.77it/s]  5%|▍         | 183/3949 [01:00<19:31,  3.21it/s]  5%|▍         | 192/3949 [01:00<17:45,  3.53it/s]  5%|▌         | 203/3949 [01:00<17:34,  3.55it/s]  4%|▍         | 176/3949 [01:00<22:21,  2.81it/s]  5%|▍         | 179/3949 [01:00<22:51,  2.75it/s]  5%|▍         | 182/3949 [01:00<23:14,  2.70it/s]  5%|▍         | 184/3949 [01:00<19:26,  3.23it/s]  5%|▍         | 181/3949 [01:00<22:39,  2.77it/s]  5%|▍         | 193/3949 [01:00<18:38,  3.36it/s]  5%|▌         | 204/3949 [01:00<16:44,  3.73it/s]  4%|▍         | 177/3949 [01:01<22:35,  2.78it/s]  5%|▍         | 180/3949 [01:01<23:03,  2.72it/s]  5%|▍         | 185/3949 [01:01<19:27,  3.22it/s]  5%|▍         | 182/3949 [01:01<22:21,  2.81it/s]  5%|▍         | 183/3949 [01:01<23:31,  2.67it/s]  5%|▍         | 194/3949 [01:01<18:34,  3.37it/s]  5%|▌         | 205/3949 [01:01<17:30,  3.56it/s]  5%|▍         | 178/3949 [01:01<22:35,  2.78it/s]  5%|▍         | 186/3949 [01:01<19:51,  3.16it/s]  5%|▍         | 195/3949 [01:01<18:22,  3.40it/s]  5%|▍         | 181/3949 [01:01<23:22,  2.69it/s]  5%|▌         | 206/3949 [01:01<17:35,  3.55it/s]  5%|▍         | 184/3949 [01:01<22:45,  2.76it/s]  5%|▍         | 183/3949 [01:01<22:45,  2.76it/s]  5%|▍         | 187/3949 [01:01<19:39,  3.19it/s]  5%|▍         | 179/3949 [01:01<22:24,  2.81it/s]  5%|▍         | 196/3949 [01:01<18:16,  3.42it/s]  5%|▌         | 207/3949 [01:01<17:24,  3.58it/s]  5%|▍         | 182/3949 [01:01<22:46,  2.76it/s]  5%|▍         | 185/3949 [01:01<23:11,  2.70it/s]  5%|▍         | 184/3949 [01:01<22:39,  2.77it/s]  5%|▌         | 208/3949 [01:02<16:55,  3.68it/s]  5%|▍         | 197/3949 [01:02<18:13,  3.43it/s]  5%|▍         | 188/3949 [01:02<19:19,  3.24it/s]  5%|▍         | 180/3949 [01:02<23:22,  2.69it/s]  5%|▍         | 183/3949 [01:02<22:38,  2.77it/s]  5%|▍         | 186/3949 [01:02<23:11,  2.70it/s]  5%|▌         | 209/3949 [01:02<16:49,  3.71it/s]  5%|▍         | 185/3949 [01:02<23:04,  2.72it/s]  5%|▍         | 189/3949 [01:02<19:07,  3.28it/s]  5%|▌         | 198/3949 [01:02<18:40,  3.35it/s]  5%|▍         | 181/3949 [01:02<23:34,  2.66it/s]  5%|▍         | 184/3949 [01:02<23:16,  2.70it/s]  5%|▌         | 210/3949 [01:02<17:23,  3.58it/s]  5%|▍         | 187/3949 [01:02<22:54,  2.74it/s]  5%|▌         | 199/3949 [01:02<18:25,  3.39it/s]  5%|▍         | 186/3949 [01:02<23:15,  2.70it/s]  5%|▍         | 190/3949 [01:02<19:41,  3.18it/s]  5%|▍         | 182/3949 [01:02<22:34,  2.78it/s]  5%|▌         | 211/3949 [01:02<18:00,  3.46it/s]  5%|▍         | 185/3949 [01:03<22:27,  2.79it/s]  5%|▌         | 200/3949 [01:03<18:13,  3.43it/s]  5%|▍         | 191/3949 [01:03<18:59,  3.30it/s]  5%|▍         | 188/3949 [01:03<23:16,  2.69it/s]  5%|▍         | 187/3949 [01:03<22:42,  2.76it/s]  5%|▌         | 212/3949 [01:03<17:32,  3.55it/s]  5%|▍         | 183/3949 [01:03<22:16,  2.82it/s]  5%|▌         | 201/3949 [01:03<18:40,  3.35it/s]  5%|▍         | 192/3949 [01:03<18:58,  3.30it/s]  5%|▍         | 186/3949 [01:03<22:31,  2.78it/s]  5%|▍         | 188/3949 [01:03<23:00,  2.73it/s]  5%|▍         | 189/3949 [01:03<23:38,  2.65it/s]  5%|▌         | 213/3949 [01:03<17:07,  3.64it/s]  5%|▍         | 193/3949 [01:03<18:19,  3.42it/s]  5%|▍         | 184/3949 [01:03<22:46,  2.75it/s]  5%|▌         | 202/3949 [01:03<19:39,  3.18it/s]  5%|▌         | 214/3949 [01:03<16:08,  3.86it/s]  5%|▍         | 187/3949 [01:03<23:08,  2.71it/s]  5%|▍         | 190/3949 [01:03<23:27,  2.67it/s]  5%|▍         | 189/3949 [01:03<23:16,  2.69it/s]  5%|▍         | 194/3949 [01:03<17:51,  3.50it/s]  5%|▌         | 215/3949 [01:04<16:30,  3.77it/s]  5%|▌         | 203/3949 [01:04<19:48,  3.15it/s]  5%|▍         | 185/3949 [01:04<23:20,  2.69it/s]  5%|▍         | 188/3949 [01:04<23:19,  2.69it/s]  5%|▍         | 195/3949 [01:04<17:52,  3.50it/s]  5%|▍         | 191/3949 [01:04<23:30,  2.66it/s]  5%|▍         | 190/3949 [01:04<23:46,  2.63it/s]  5%|▌         | 204/3949 [01:04<19:10,  3.25it/s]  5%|▌         | 216/3949 [01:04<16:53,  3.68it/s]  5%|▍         | 196/3949 [01:04<16:33,  3.78it/s]  5%|▍         | 186/3949 [01:04<23:03,  2.72it/s]  5%|▍         | 189/3949 [01:04<23:04,  2.72it/s]  5%|▌         | 205/3949 [01:04<19:15,  3.24it/s]  5%|▌         | 217/3949 [01:04<17:45,  3.50it/s]  5%|▍         | 192/3949 [01:04<23:42,  2.64it/s]  5%|▍         | 191/3949 [01:04<23:35,  2.66it/s]  5%|▍         | 197/3949 [01:04<17:26,  3.59it/s]  5%|▍         | 187/3949 [01:04<22:55,  2.73it/s]  5%|▍         | 190/3949 [01:04<23:12,  2.70it/s]  6%|▌         | 218/3949 [01:04<18:05,  3.44it/s]  5%|▌         | 206/3949 [01:04<19:16,  3.24it/s]  5%|▍         | 193/3949 [01:04<22:20,  2.80it/s]  5%|▍         | 192/3949 [01:04<22:45,  2.75it/s]  5%|▌         | 198/3949 [01:05<18:07,  3.45it/s]  5%|▍         | 188/3949 [01:05<22:31,  2.78it/s]  5%|▍         | 191/3949 [01:05<22:39,  2.77it/s]  6%|▌         | 219/3949 [01:05<18:42,  3.32it/s]  5%|▌         | 207/3949 [01:05<19:43,  3.16it/s]  5%|▌         | 199/3949 [01:05<17:09,  3.64it/s]  5%|▍         | 194/3949 [01:05<21:53,  2.86it/s]  5%|▍         | 193/3949 [01:05<23:11,  2.70it/s]  5%|▌         | 200/3949 [01:05<16:22,  3.82it/s]  5%|▌         | 208/3949 [01:05<19:07,  3.26it/s]  5%|▍         | 189/3949 [01:05<23:29,  2.67it/s]  6%|▌         | 220/3949 [01:05<19:07,  3.25it/s]  5%|▍         | 192/3949 [01:05<23:14,  2.69it/s]  5%|▍         | 195/3949 [01:05<22:43,  2.75it/s]  5%|▌         | 201/3949 [01:05<14:56,  4.18it/s]  5%|▍         | 194/3949 [01:05<23:29,  2.67it/s]  5%|▌         | 202/3949 [01:05<14:07,  4.42it/s]  6%|▌         | 221/3949 [01:05<19:20,  3.21it/s]  5%|▍         | 190/3949 [01:05<23:14,  2.69it/s]  5%|▌         | 209/3949 [01:05<20:33,  3.03it/s]  5%|▍         | 193/3949 [01:05<23:16,  2.69it/s]  5%|▍         | 196/3949 [01:06<22:50,  2.74it/s]  5%|▌         | 203/3949 [01:06<13:37,  4.58it/s]  5%|▍         | 195/3949 [01:06<24:04,  2.60it/s]  6%|▌         | 222/3949 [01:06<19:38,  3.16it/s]  5%|▌         | 204/3949 [01:06<12:50,  4.86it/s]  5%|▌         | 210/3949 [01:06<21:08,  2.95it/s]  5%|▍         | 191/3949 [01:06<24:11,  2.59it/s]  5%|▍         | 194/3949 [01:06<23:57,  2.61it/s]  5%|▌         | 205/3949 [01:06<11:58,  5.21it/s]  5%|▍         | 197/3949 [01:06<23:51,  2.62it/s]  5%|▍         | 196/3949 [01:06<24:17,  2.57it/s]  6%|▌         | 223/3949 [01:06<19:58,  3.11it/s]  5%|▌         | 206/3949 [01:06<11:56,  5.22it/s]  5%|▌         | 211/3949 [01:06<21:07,  2.95it/s]  5%|▍         | 192/3949 [01:06<24:23,  2.57it/s]  5%|▌         | 207/3949 [01:06<11:45,  5.30it/s]  5%|▍         | 195/3949 [01:06<24:43,  2.53it/s]  5%|▌         | 198/3949 [01:06<23:40,  2.64it/s]  6%|▌         | 224/3949 [01:06<20:31,  3.02it/s]  5%|▌         | 212/3949 [01:06<21:01,  2.96it/s]  5%|▍         | 197/3949 [01:06<25:06,  2.49it/s]  5%|▌         | 208/3949 [01:07<12:11,  5.11it/s]  5%|▍         | 193/3949 [01:07<23:44,  2.64it/s]  5%|▌         | 199/3949 [01:07<23:24,  2.67it/s]  5%|▍         | 196/3949 [01:07<24:56,  2.51it/s]  6%|▌         | 225/3949 [01:07<20:19,  3.05it/s]  5%|▌         | 209/3949 [01:07<13:18,  4.68it/s]  5%|▌         | 213/3949 [01:07<21:08,  2.94it/s]  5%|▌         | 198/3949 [01:07<24:27,  2.56it/s]  5%|▍         | 194/3949 [01:07<23:55,  2.62it/s]  5%|▌         | 210/3949 [01:07<13:52,  4.49it/s]  5%|▌         | 200/3949 [01:07<23:20,  2.68it/s]  6%|▌         | 226/3949 [01:07<20:25,  3.04it/s]  5%|▍         | 197/3949 [01:07<24:21,  2.57it/s]  5%|▌         | 214/3949 [01:07<19:56,  3.12it/s]  5%|▌         | 199/3949 [01:07<24:21,  2.57it/s]  5%|▌         | 211/3949 [01:07<15:08,  4.11it/s]  5%|▍         | 195/3949 [01:07<23:33,  2.66it/s]  6%|▌         | 227/3949 [01:07<19:57,  3.11it/s]  5%|▌         | 215/3949 [01:07<19:32,  3.19it/s]  5%|▌         | 201/3949 [01:07<23:24,  2.67it/s]  5%|▌         | 198/3949 [01:07<24:06,  2.59it/s]  5%|▌         | 212/3949 [01:08<15:53,  3.92it/s]  5%|▌         | 200/3949 [01:08<23:50,  2.62it/s]  5%|▌         | 216/3949 [01:08<18:34,  3.35it/s]  6%|▌         | 228/3949 [01:08<20:18,  3.05it/s]  5%|▍         | 196/3949 [01:08<23:29,  2.66it/s]  5%|▌         | 199/3949 [01:08<22:42,  2.75it/s]  5%|▌         | 202/3949 [01:08<22:55,  2.72it/s]  5%|▌         | 213/3949 [01:08<16:42,  3.73it/s]  5%|▌         | 217/3949 [01:08<18:23,  3.38it/s]  5%|▌         | 201/3949 [01:08<23:40,  2.64it/s]  6%|▌         | 229/3949 [01:08<19:41,  3.15it/s]  5%|▍         | 197/3949 [01:08<23:04,  2.71it/s]  5%|▌         | 200/3949 [01:08<21:52,  2.86it/s]  5%|▌         | 203/3949 [01:08<23:06,  2.70it/s]  5%|▌         | 214/3949 [01:08<17:45,  3.51it/s]  6%|▌         | 218/3949 [01:08<17:54,  3.47it/s]  6%|▌         | 230/3949 [01:08<19:18,  3.21it/s]  5%|▌         | 202/3949 [01:08<23:51,  2.62it/s]  5%|▌         | 198/3949 [01:08<23:09,  2.70it/s]  6%|▌         | 219/3949 [01:08<17:12,  3.61it/s]  5%|▌         | 201/3949 [01:08<22:49,  2.74it/s]  5%|▌         | 215/3949 [01:09<18:38,  3.34it/s]  5%|▌         | 204/3949 [01:09<23:26,  2.66it/s]  6%|▌         | 231/3949 [01:09<18:59,  3.26it/s]  6%|▌         | 220/3949 [01:09<16:59,  3.66it/s]  5%|▌         | 203/3949 [01:09<23:34,  2.65it/s]  5%|▌         | 216/3949 [01:09<17:58,  3.46it/s]  5%|▌         | 199/3949 [01:09<23:03,  2.71it/s]  5%|▌         | 202/3949 [01:09<22:53,  2.73it/s]  5%|▌         | 205/3949 [01:09<23:01,  2.71it/s]  6%|▌         | 232/3949 [01:09<19:06,  3.24it/s]  6%|▌         | 221/3949 [01:09<16:27,  3.77it/s]  5%|▌         | 204/3949 [01:09<22:52,  2.73it/s]  5%|▌         | 217/3949 [01:09<17:53,  3.48it/s]  6%|▌         | 233/3949 [01:09<19:05,  3.24it/s]  5%|▌         | 200/3949 [01:09<23:45,  2.63it/s]  6%|▌         | 222/3949 [01:09<16:27,  3.77it/s]  5%|▌         | 203/3949 [01:09<23:22,  2.67it/s]  5%|▌         | 206/3949 [01:09<22:58,  2.71it/s]  6%|▌         | 218/3949 [01:09<17:52,  3.48it/s]  5%|▌         | 205/3949 [01:09<23:27,  2.66it/s]  6%|▌         | 223/3949 [01:09<16:26,  3.78it/s]  6%|▌         | 234/3949 [01:10<18:45,  3.30it/s]  5%|▌         | 201/3949 [01:10<23:30,  2.66it/s]  5%|▌         | 207/3949 [01:10<22:32,  2.77it/s]  6%|▌         | 219/3949 [01:10<17:32,  3.54it/s]  5%|▌         | 204/3949 [01:10<23:39,  2.64it/s]  6%|▌         | 224/3949 [01:10<17:12,  3.61it/s]  5%|▌         | 206/3949 [01:10<22:47,  2.74it/s]  6%|▌         | 235/3949 [01:10<18:59,  3.26it/s]  5%|▌         | 202/3949 [01:10<22:58,  2.72it/s]  6%|▌         | 220/3949 [01:10<18:08,  3.43it/s]  5%|▌         | 208/3949 [01:10<22:32,  2.77it/s]  5%|▌         | 205/3949 [01:10<23:03,  2.71it/s]  6%|▌         | 225/3949 [01:10<17:17,  3.59it/s]  6%|▌         | 236/3949 [01:10<19:08,  3.23it/s]  5%|▌         | 207/3949 [01:10<23:15,  2.68it/s]  6%|▌         | 221/3949 [01:10<18:19,  3.39it/s]  5%|▌         | 203/3949 [01:10<22:39,  2.76it/s]  5%|▌         | 206/3949 [01:10<22:49,  2.73it/s]  5%|▌         | 209/3949 [01:10<22:50,  2.73it/s]  6%|▌         | 226/3949 [01:10<17:18,  3.59it/s]  6%|▌         | 237/3949 [01:10<18:06,  3.42it/s]  5%|▌         | 208/3949 [01:11<22:53,  2.72it/s]  6%|▌         | 222/3949 [01:11<19:16,  3.22it/s]  5%|▌         | 204/3949 [01:11<22:39,  2.75it/s]  6%|▌         | 227/3949 [01:11<17:30,  3.54it/s]  5%|▌         | 210/3949 [01:11<22:23,  2.78it/s]  6%|▌         | 238/3949 [01:11<18:29,  3.34it/s]  5%|▌         | 207/3949 [01:11<23:20,  2.67it/s]  5%|▌         | 209/3949 [01:11<22:18,  2.80it/s]  6%|▌         | 223/3949 [01:11<18:50,  3.29it/s]  6%|▌         | 228/3949 [01:11<17:26,  3.56it/s]  5%|▌         | 205/3949 [01:11<22:48,  2.74it/s]  6%|▌         | 239/3949 [01:11<18:57,  3.26it/s]  5%|▌         | 208/3949 [01:11<22:17,  2.80it/s]  5%|▌         | 211/3949 [01:11<22:41,  2.74it/s]  6%|▌         | 229/3949 [01:11<16:46,  3.69it/s]  6%|▌         | 224/3949 [01:11<19:11,  3.24it/s]  5%|▌         | 210/3949 [01:11<22:35,  2.76it/s]  6%|▌         | 240/3949 [01:11<18:11,  3.40it/s]  5%|▌         | 206/3949 [01:11<22:40,  2.75it/s]  5%|▌         | 209/3949 [01:11<22:10,  2.81it/s]  5%|▌         | 212/3949 [01:11<22:51,  2.73it/s]  6%|▌         | 230/3949 [01:11<17:17,  3.58it/s]  6%|▌         | 225/3949 [01:12<19:42,  3.15it/s]  6%|▌         | 241/3949 [01:12<17:33,  3.52it/s]  5%|▌         | 211/3949 [01:12<22:57,  2.71it/s]  5%|▌         | 207/3949 [01:12<22:39,  2.75it/s]  6%|▌         | 231/3949 [01:12<17:50,  3.47it/s]  6%|▌         | 242/3949 [01:12<16:51,  3.66it/s]  5%|▌         | 210/3949 [01:12<22:55,  2.72it/s]  6%|▌         | 226/3949 [01:12<19:03,  3.26it/s]  5%|▌         | 213/3949 [01:12<23:12,  2.68it/s]  5%|▌         | 212/3949 [01:12<22:58,  2.71it/s]  6%|▌         | 232/3949 [01:12<17:03,  3.63it/s]  6%|▌         | 243/3949 [01:12<16:33,  3.73it/s]  5%|▌         | 208/3949 [01:12<22:23,  2.79it/s]  5%|▌         | 211/3949 [01:12<22:21,  2.79it/s]  6%|▌         | 227/3949 [01:12<19:26,  3.19it/s]  5%|▌         | 214/3949 [01:12<23:21,  2.67it/s]  6%|▌         | 244/3949 [01:12<16:38,  3.71it/s]  6%|▌         | 233/3949 [01:12<17:19,  3.57it/s]  5%|▌         | 213/3949 [01:12<22:58,  2.71it/s]  6%|▌         | 228/3949 [01:12<19:26,  3.19it/s]  5%|▌         | 209/3949 [01:12<23:09,  2.69it/s]  5%|▌         | 212/3949 [01:13<22:17,  2.79it/s]  5%|▌         | 215/3949 [01:13<23:22,  2.66it/s]  6%|▌         | 234/3949 [01:13<17:24,  3.56it/s]  6%|▌         | 245/3949 [01:13<16:46,  3.68it/s]  5%|▌         | 214/3949 [01:13<22:39,  2.75it/s]  6%|▌         | 229/3949 [01:13<19:40,  3.15it/s]  5%|▌         | 210/3949 [01:13<22:32,  2.76it/s]  6%|▌         | 246/3949 [01:13<16:23,  3.76it/s]  5%|▌         | 213/3949 [01:13<22:33,  2.76it/s]  6%|▌         | 235/3949 [01:13<17:13,  3.59it/s]  5%|▌         | 216/3949 [01:13<22:55,  2.71it/s]  6%|▋         | 247/3949 [01:13<16:02,  3.85it/s]  6%|▌         | 236/3949 [01:13<16:24,  3.77it/s]  6%|▌         | 230/3949 [01:13<19:41,  3.15it/s]  5%|▌         | 215/3949 [01:13<23:23,  2.66it/s]  5%|▌         | 211/3949 [01:13<22:50,  2.73it/s]  5%|▌         | 214/3949 [01:13<23:06,  2.69it/s]  6%|▋         | 248/3949 [01:13<14:48,  4.17it/s]  5%|▌         | 217/3949 [01:13<23:37,  2.63it/s]  6%|▌         | 237/3949 [01:13<16:00,  3.87it/s]  6%|▌         | 231/3949 [01:13<20:24,  3.04it/s]  5%|▌         | 216/3949 [01:14<23:45,  2.62it/s]  6%|▌         | 238/3949 [01:14<15:33,  3.98it/s]  6%|▋         | 249/3949 [01:14<16:00,  3.85it/s]  5%|▌         | 212/3949 [01:14<23:39,  2.63it/s]  5%|▌         | 215/3949 [01:14<23:06,  2.69it/s]  6%|▌         | 218/3949 [01:14<24:16,  2.56it/s]  6%|▌         | 232/3949 [01:14<20:58,  2.95it/s]  6%|▌         | 239/3949 [01:14<15:33,  3.97it/s]  6%|▋         | 250/3949 [01:14<15:52,  3.88it/s]  5%|▌         | 217/3949 [01:14<23:46,  2.62it/s]  5%|▌         | 213/3949 [01:14<23:16,  2.68it/s]  5%|▌         | 216/3949 [01:14<24:01,  2.59it/s]  6%|▌         | 240/3949 [01:14<14:57,  4.13it/s]  6%|▌         | 219/3949 [01:14<24:15,  2.56it/s]  6%|▌         | 233/3949 [01:14<20:46,  2.98it/s]  6%|▋         | 251/3949 [01:14<17:00,  3.63it/s]  6%|▌         | 218/3949 [01:14<23:08,  2.69it/s]  6%|▌         | 241/3949 [01:14<15:47,  3.91it/s]  5%|▌         | 214/3949 [01:14<23:18,  2.67it/s]  5%|▌         | 217/3949 [01:14<23:37,  2.63it/s]  6%|▋         | 252/3949 [01:14<16:43,  3.68it/s]  6%|▌         | 234/3949 [01:14<20:50,  2.97it/s]  6%|▌         | 220/3949 [01:15<23:55,  2.60it/s]  6%|▌         | 219/3949 [01:15<22:22,  2.78it/s]  6%|▌         | 242/3949 [01:15<16:34,  3.73it/s]  5%|▌         | 215/3949 [01:15<22:33,  2.76it/s]  6%|▋         | 253/3949 [01:15<16:59,  3.62it/s]  6%|▌         | 218/3949 [01:15<23:23,  2.66it/s]  6%|▌         | 235/3949 [01:15<21:05,  2.94it/s]  6%|▌         | 221/3949 [01:15<23:17,  2.67it/s]  6%|▌         | 243/3949 [01:15<16:35,  3.72it/s]  6%|▋         | 254/3949 [01:15<15:49,  3.89it/s]  6%|▌         | 220/3949 [01:15<23:01,  2.70it/s]  5%|▌         | 216/3949 [01:15<22:15,  2.80it/s]  6%|▋         | 255/3949 [01:15<14:36,  4.21it/s]  6%|▌         | 236/3949 [01:15<20:51,  2.97it/s]  6%|▌         | 219/3949 [01:15<23:39,  2.63it/s]  6%|▌         | 244/3949 [01:15<17:24,  3.55it/s]  6%|▌         | 222/3949 [01:15<23:47,  2.61it/s]  6%|▋         | 256/3949 [01:15<13:39,  4.51it/s]  6%|▌         | 221/3949 [01:15<22:52,  2.72it/s]  5%|▌         | 217/3949 [01:15<23:17,  2.67it/s]  7%|▋         | 257/3949 [01:15<12:27,  4.94it/s]  6%|▌         | 237/3949 [01:16<20:50,  2.97it/s]  6%|▌         | 220/3949 [01:16<24:03,  2.58it/s]  6%|▌         | 245/3949 [01:16<19:27,  3.17it/s]  7%|▋         | 258/3949 [01:16<12:08,  5.07it/s]  6%|▌         | 222/3949 [01:16<22:29,  2.76it/s]  6%|▌         | 223/3949 [01:16<24:26,  2.54it/s]  7%|▋         | 259/3949 [01:16<11:16,  5.46it/s]  6%|▌         | 218/3949 [01:16<23:54,  2.60it/s]  6%|▌         | 238/3949 [01:16<21:10,  2.92it/s]  6%|▌         | 246/3949 [01:16<19:49,  3.11it/s]  7%|▋         | 260/3949 [01:16<10:51,  5.66it/s]  6%|▌         | 221/3949 [01:16<24:26,  2.54it/s]  6%|▌         | 224/3949 [01:16<24:13,  2.56it/s]  6%|▌         | 223/3949 [01:16<23:27,  2.65it/s]  7%|▋         | 261/3949 [01:16<11:17,  5.44it/s]  6%|▌         | 239/3949 [01:16<21:04,  2.93it/s]  6%|▌         | 219/3949 [01:16<23:59,  2.59it/s]  6%|▋         | 247/3949 [01:16<19:21,  3.19it/s]  7%|▋         | 262/3949 [01:16<10:51,  5.66it/s]  6%|▌         | 222/3949 [01:16<24:43,  2.51it/s]  6%|▌         | 225/3949 [01:17<24:46,  2.51it/s]  7%|▋         | 263/3949 [01:17<11:08,  5.51it/s]  6%|▋         | 248/3949 [01:17<18:44,  3.29it/s]  6%|▌         | 224/3949 [01:17<24:20,  2.55it/s]  6%|▌         | 240/3949 [01:17<22:24,  2.76it/s]  6%|▌         | 220/3949 [01:17<23:58,  2.59it/s]  7%|▋         | 264/3949 [01:17<11:13,  5.47it/s]  6%|▌         | 223/3949 [01:17<24:58,  2.49it/s]  6%|▋         | 249/3949 [01:17<19:03,  3.24it/s]  6%|▌         | 226/3949 [01:17<24:22,  2.55it/s]  6%|▌         | 225/3949 [01:17<24:19,  2.55it/s]  6%|▌         | 241/3949 [01:17<21:30,  2.87it/s]  6%|▌         | 221/3949 [01:17<23:09,  2.68it/s]  7%|▋         | 265/3949 [01:17<12:55,  4.75it/s]  6%|▋         | 250/3949 [01:17<18:46,  3.28it/s]  6%|▌         | 224/3949 [01:17<24:15,  2.56it/s]  6%|▌         | 227/3949 [01:17<23:15,  2.67it/s]  7%|▋         | 266/3949 [01:17<13:26,  4.57it/s]  6%|▌         | 242/3949 [01:17<21:03,  2.93it/s]  6%|▌         | 226/3949 [01:17<24:25,  2.54it/s]  6%|▌         | 222/3949 [01:17<23:16,  2.67it/s]  6%|▋         | 251/3949 [01:17<17:35,  3.50it/s]  7%|▋         | 267/3949 [01:18<14:43,  4.17it/s]  6%|▌         | 225/3949 [01:18<24:01,  2.58it/s]  6%|▌         | 243/3949 [01:18<20:15,  3.05it/s]  6%|▌         | 228/3949 [01:18<23:31,  2.64it/s]  6%|▋         | 252/3949 [01:18<17:37,  3.50it/s]  6%|▌         | 227/3949 [01:18<24:16,  2.56it/s]  6%|▌         | 223/3949 [01:18<22:58,  2.70it/s]  7%|▋         | 268/3949 [01:18<15:19,  4.00it/s]  6%|▌         | 244/3949 [01:18<19:21,  3.19it/s]  6%|▋         | 253/3949 [01:18<17:05,  3.61it/s]  6%|▌         | 226/3949 [01:18<23:57,  2.59it/s]  6%|▌         | 229/3949 [01:18<24:03,  2.58it/s]  6%|▌         | 228/3949 [01:18<23:15,  2.67it/s]  7%|▋         | 269/3949 [01:18<16:06,  3.81it/s]  6%|▌         | 224/3949 [01:18<23:35,  2.63it/s]  6%|▌         | 245/3949 [01:18<19:33,  3.16it/s]  6%|▋         | 254/3949 [01:18<17:11,  3.58it/s]  6%|▌         | 227/3949 [01:18<22:45,  2.73it/s]  7%|▋         | 270/3949 [01:18<15:40,  3.91it/s]  6%|▌         | 230/3949 [01:18<23:37,  2.62it/s]  6%|▌         | 229/3949 [01:18<23:03,  2.69it/s]  6%|▌         | 246/3949 [01:18<19:00,  3.25it/s]  6%|▌         | 225/3949 [01:18<23:17,  2.67it/s]  6%|▋         | 255/3949 [01:19<17:39,  3.49it/s]  7%|▋         | 271/3949 [01:19<15:59,  3.83it/s]  6%|▌         | 228/3949 [01:19<23:38,  2.62it/s]  6%|▌         | 231/3949 [01:19<23:24,  2.65it/s]  6%|▌         | 230/3949 [01:19<22:30,  2.75it/s]  6%|▋         | 247/3949 [01:19<19:41,  3.13it/s]  6%|▋         | 256/3949 [01:19<17:31,  3.51it/s]  6%|▌         | 226/3949 [01:19<22:44,  2.73it/s]  7%|▋         | 272/3949 [01:19<15:58,  3.84it/s]  6%|▌         | 229/3949 [01:19<23:15,  2.67it/s]  6%|▋         | 248/3949 [01:19<19:24,  3.18it/s]  6%|▌         | 232/3949 [01:19<22:53,  2.71it/s]  7%|▋         | 257/3949 [01:19<17:53,  3.44it/s]  7%|▋         | 273/3949 [01:19<16:02,  3.82it/s]  6%|▌         | 231/3949 [01:19<23:21,  2.65it/s]  6%|▌         | 227/3949 [01:19<22:46,  2.72it/s]  7%|▋         | 258/3949 [01:19<17:38,  3.49it/s]  6%|▌         | 230/3949 [01:19<22:48,  2.72it/s]  7%|▋         | 274/3949 [01:19<16:20,  3.75it/s]  6%|▋         | 249/3949 [01:19<19:09,  3.22it/s]  6%|▌         | 233/3949 [01:19<23:12,  2.67it/s]  6%|▌         | 232/3949 [01:20<22:40,  2.73it/s]  6%|▌         | 228/3949 [01:20<22:18,  2.78it/s]  7%|▋         | 259/3949 [01:20<17:59,  3.42it/s]  6%|▋         | 250/3949 [01:20<19:14,  3.21it/s]  7%|▋         | 275/3949 [01:20<17:26,  3.51it/s]  6%|▌         | 231/3949 [01:20<22:15,  2.78it/s]  6%|▌         | 233/3949 [01:20<21:58,  2.82it/s]  6%|▌         | 234/3949 [01:20<22:51,  2.71it/s]  6%|▌         | 229/3949 [01:20<22:01,  2.81it/s]  7%|▋         | 260/3949 [01:20<17:59,  3.42it/s]  7%|▋         | 276/3949 [01:20<17:58,  3.41it/s]  6%|▋         | 251/3949 [01:20<19:22,  3.18it/s]  6%|▌         | 232/3949 [01:20<22:35,  2.74it/s]  6%|▌         | 234/3949 [01:20<21:12,  2.92it/s]  6%|▌         | 235/3949 [01:20<22:05,  2.80it/s]  6%|▌         | 230/3949 [01:20<22:03,  2.81it/s]  7%|▋         | 261/3949 [01:20<17:52,  3.44it/s]  7%|▋         | 277/3949 [01:20<17:22,  3.52it/s]  6%|▋         | 252/3949 [01:20<19:09,  3.22it/s]  6%|▌         | 233/3949 [01:20<22:23,  2.77it/s]  6%|▌         | 236/3949 [01:21<22:20,  2.77it/s]  7%|▋         | 262/3949 [01:21<17:40,  3.48it/s]  7%|▋         | 278/3949 [01:21<16:56,  3.61it/s]  6%|▌         | 235/3949 [01:21<22:24,  2.76it/s]  6%|▌         | 231/3949 [01:21<22:31,  2.75it/s]  6%|▋         | 253/3949 [01:21<19:40,  3.13it/s]  7%|▋         | 263/3949 [01:21<17:13,  3.57it/s]  6%|▌         | 234/3949 [01:21<22:33,  2.75it/s]  7%|▋         | 279/3949 [01:21<17:02,  3.59it/s]  6%|▌         | 237/3949 [01:21<21:57,  2.82it/s]  6%|▌         | 236/3949 [01:21<22:11,  2.79it/s]  6%|▋         | 254/3949 [01:21<19:15,  3.20it/s]  6%|▌         | 232/3949 [01:21<22:51,  2.71it/s]  7%|▋         | 264/3949 [01:21<16:32,  3.71it/s]  7%|▋         | 280/3949 [01:21<17:30,  3.49it/s]  6%|▌         | 235/3949 [01:21<22:07,  2.80it/s]  6%|▌         | 237/3949 [01:21<21:57,  2.82it/s]  6%|▌         | 238/3949 [01:21<22:24,  2.76it/s]  6%|▋         | 255/3949 [01:21<19:30,  3.16it/s]  7%|▋         | 265/3949 [01:21<16:20,  3.76it/s]  6%|▌         | 233/3949 [01:21<22:17,  2.78it/s]  7%|▋         | 281/3949 [01:21<17:15,  3.54it/s]  6%|▌         | 236/3949 [01:22<22:22,  2.77it/s]  6%|▌         | 238/3949 [01:22<21:34,  2.87it/s]  6%|▌         | 239/3949 [01:22<21:57,  2.82it/s]  6%|▋         | 256/3949 [01:22<19:21,  3.18it/s]  7%|▋         | 266/3949 [01:22<16:51,  3.64it/s]  6%|▌         | 234/3949 [01:22<21:48,  2.84it/s]  7%|▋         | 282/3949 [01:22<17:32,  3.48it/s]  6%|▌         | 237/3949 [01:22<21:41,  2.85it/s]  7%|▋         | 267/3949 [01:22<17:28,  3.51it/s]  7%|▋         | 257/3949 [01:22<19:22,  3.18it/s]  6%|▌         | 239/3949 [01:22<21:53,  2.82it/s]  6%|▌         | 240/3949 [01:22<22:45,  2.72it/s]  7%|▋         | 283/3949 [01:22<17:30,  3.49it/s]  6%|▌         | 235/3949 [01:22<22:03,  2.81it/s]  6%|▌         | 238/3949 [01:22<21:23,  2.89it/s]  7%|▋         | 268/3949 [01:22<17:44,  3.46it/s]  7%|▋         | 258/3949 [01:22<19:30,  3.15it/s]  7%|▋         | 284/3949 [01:22<17:56,  3.40it/s]  6%|▌         | 241/3949 [01:22<21:56,  2.82it/s]  6%|▌         | 240/3949 [01:22<22:11,  2.78it/s]  6%|▌         | 236/3949 [01:22<22:14,  2.78it/s]  7%|▋         | 269/3949 [01:22<17:28,  3.51it/s]  7%|▋         | 259/3949 [01:23<18:36,  3.30it/s]  6%|▌         | 239/3949 [01:23<21:53,  2.82it/s]  7%|▋         | 285/3949 [01:23<17:45,  3.44it/s]  6%|▌         | 241/3949 [01:23<21:51,  2.83it/s]  6%|▌         | 242/3949 [01:23<21:54,  2.82it/s]  6%|▌         | 237/3949 [01:23<21:56,  2.82it/s]  7%|▋         | 260/3949 [01:23<18:13,  3.37it/s]  7%|▋         | 270/3949 [01:23<18:03,  3.40it/s]  7%|▋         | 286/3949 [01:23<18:07,  3.37it/s]  6%|▌         | 240/3949 [01:23<21:59,  2.81it/s]  6%|▌         | 242/3949 [01:23<21:26,  2.88it/s]  6%|▌         | 243/3949 [01:23<22:02,  2.80it/s]  7%|▋         | 261/3949 [01:23<18:14,  3.37it/s]  6%|▌         | 238/3949 [01:23<21:54,  2.82it/s]  7%|▋         | 271/3949 [01:23<18:10,  3.37it/s]  7%|▋         | 287/3949 [01:23<18:05,  3.37it/s]  6%|▌         | 241/3949 [01:23<21:05,  2.93it/s]  6%|▌         | 243/3949 [01:23<21:33,  2.86it/s]  6%|▌         | 244/3949 [01:23<21:19,  2.89it/s]  7%|▋         | 262/3949 [01:23<18:12,  3.38it/s]  7%|▋         | 272/3949 [01:23<18:34,  3.30it/s]  6%|▌         | 239/3949 [01:23<21:34,  2.87it/s]  7%|▋         | 288/3949 [01:23<18:00,  3.39it/s]  6%|▌         | 242/3949 [01:24<21:16,  2.90it/s]  6%|▌         | 244/3949 [01:24<21:16,  2.90it/s]  7%|▋         | 263/3949 [01:24<18:41,  3.29it/s]  7%|▋         | 273/3949 [01:24<18:23,  3.33it/s]  7%|▋         | 289/3949 [01:24<16:55,  3.61it/s]  6%|▌         | 240/3949 [01:24<20:40,  2.99it/s]  6%|▌         | 245/3949 [01:24<22:18,  2.77it/s]  6%|▌         | 243/3949 [01:24<21:51,  2.83it/s]  7%|▋         | 274/3949 [01:24<17:29,  3.50it/s]  7%|▋         | 290/3949 [01:24<16:21,  3.73it/s]  7%|▋         | 264/3949 [01:24<19:21,  3.17it/s]  6%|▌         | 245/3949 [01:24<22:30,  2.74it/s]  6%|▌         | 241/3949 [01:24<21:54,  2.82it/s]  6%|▌         | 246/3949 [01:24<23:10,  2.66it/s]  7%|▋         | 291/3949 [01:24<15:20,  3.97it/s]  7%|▋         | 275/3949 [01:24<16:44,  3.66it/s]  6%|▌         | 244/3949 [01:24<22:19,  2.77it/s]  7%|▋         | 265/3949 [01:24<19:27,  3.16it/s]  7%|▋         | 292/3949 [01:24<15:33,  3.92it/s]  6%|▌         | 246/3949 [01:24<22:22,  2.76it/s]  7%|▋         | 276/3949 [01:24<16:35,  3.69it/s]  6%|▌         | 242/3949 [01:25<22:41,  2.72it/s]  6%|▋         | 247/3949 [01:25<23:27,  2.63it/s]  7%|▋         | 266/3949 [01:25<19:46,  3.10it/s]  7%|▋         | 293/3949 [01:25<15:38,  3.90it/s]  7%|▋         | 277/3949 [01:25<16:19,  3.75it/s]  6%|▌         | 245/3949 [01:25<23:00,  2.68it/s]  6%|▋         | 247/3949 [01:25<22:50,  2.70it/s]  6%|▋         | 248/3949 [01:25<22:50,  2.70it/s]  6%|▌         | 243/3949 [01:25<22:48,  2.71it/s]  7%|▋         | 267/3949 [01:25<19:19,  3.18it/s]  7%|▋         | 278/3949 [01:25<16:27,  3.72it/s]  7%|▋         | 294/3949 [01:25<16:48,  3.62it/s]  6%|▌         | 246/3949 [01:25<22:06,  2.79it/s]  6%|▋         | 248/3949 [01:25<21:48,  2.83it/s]  6%|▋         | 249/3949 [01:25<23:01,  2.68it/s]  6%|▌         | 244/3949 [01:25<22:53,  2.70it/s]  7%|▋         | 279/3949 [01:25<16:54,  3.62it/s]  7%|▋         | 268/3949 [01:25<19:07,  3.21it/s]  7%|▋         | 295/3949 [01:25<17:05,  3.56it/s]  6%|▋         | 247/3949 [01:25<22:14,  2.77it/s]  6%|▋         | 249/3949 [01:25<21:38,  2.85it/s]  7%|▋         | 280/3949 [01:26<16:49,  3.64it/s]  7%|▋         | 269/3949 [01:26<18:43,  3.28it/s]  7%|▋         | 296/3949 [01:26<17:25,  3.49it/s]  6%|▌         | 245/3949 [01:26<22:14,  2.78it/s]  6%|▋         | 250/3949 [01:26<22:58,  2.68it/s]  6%|▋         | 248/3949 [01:26<22:47,  2.71it/s]  6%|▋         | 250/3949 [01:26<21:50,  2.82it/s]  7%|▋         | 281/3949 [01:26<17:14,  3.54it/s]  7%|▋         | 270/3949 [01:26<18:32,  3.31it/s]  8%|▊         | 297/3949 [01:26<17:25,  3.49it/s]  6%|▌         | 246/3949 [01:26<21:38,  2.85it/s]  6%|▋         | 251/3949 [01:26<22:41,  2.72it/s]  7%|▋         | 282/3949 [01:26<17:22,  3.52it/s]  8%|▊         | 298/3949 [01:26<17:17,  3.52it/s]  7%|▋         | 271/3949 [01:26<18:21,  3.34it/s]  6%|▋         | 249/3949 [01:26<23:06,  2.67it/s]  6%|▋         | 251/3949 [01:26<22:17,  2.76it/s]  6%|▋         | 247/3949 [01:26<22:00,  2.80it/s]  6%|▋         | 252/3949 [01:26<22:53,  2.69it/s]  7%|▋         | 283/3949 [01:26<17:06,  3.57it/s]  8%|▊         | 299/3949 [01:26<17:02,  3.57it/s]  7%|▋         | 272/3949 [01:26<18:13,  3.36it/s]  6%|▋         | 252/3949 [01:27<21:23,  2.88it/s]  6%|▋         | 250/3949 [01:27<23:04,  2.67it/s]  7%|▋         | 284/3949 [01:27<16:59,  3.60it/s]  6%|▋         | 248/3949 [01:27<22:42,  2.72it/s]  6%|▋         | 253/3949 [01:27<22:25,  2.75it/s]  8%|▊         | 300/3949 [01:27<17:09,  3.55it/s]  7%|▋         | 273/3949 [01:27<17:42,  3.46it/s]  6%|▋         | 253/3949 [01:27<22:02,  2.79it/s]  6%|▋         | 251/3949 [01:27<22:52,  2.69it/s]  7%|▋         | 285/3949 [01:27<17:31,  3.48it/s]  7%|▋         | 274/3949 [01:27<17:26,  3.51it/s]  8%|▊         | 301/3949 [01:27<17:11,  3.54it/s]  6%|▋         | 249/3949 [01:27<22:00,  2.80it/s]  6%|▋         | 254/3949 [01:27<22:45,  2.71it/s]  7%|▋         | 286/3949 [01:27<17:16,  3.54it/s]  6%|▋         | 254/3949 [01:27<22:09,  2.78it/s]  6%|▋         | 252/3949 [01:27<22:55,  2.69it/s]  7%|▋         | 275/3949 [01:27<17:55,  3.42it/s]  8%|▊         | 302/3949 [01:27<17:43,  3.43it/s]  6%|▋         | 250/3949 [01:27<22:22,  2.75it/s]  6%|▋         | 255/3949 [01:27<22:39,  2.72it/s]  7%|▋         | 287/3949 [01:28<16:43,  3.65it/s]  8%|▊         | 303/3949 [01:28<16:57,  3.58it/s]  6%|▋         | 255/3949 [01:28<22:08,  2.78it/s]  7%|▋         | 276/3949 [01:28<18:49,  3.25it/s]  6%|▋         | 253/3949 [01:28<23:20,  2.64it/s]  6%|▋         | 251/3949 [01:28<22:23,  2.75it/s]  7%|▋         | 288/3949 [01:28<16:11,  3.77it/s]  6%|▋         | 256/3949 [01:28<22:29,  2.74it/s]  8%|▊         | 304/3949 [01:28<16:52,  3.60it/s]  6%|▋         | 256/3949 [01:28<22:05,  2.79it/s]  7%|▋         | 277/3949 [01:28<19:28,  3.14it/s]  7%|▋         | 289/3949 [01:28<16:17,  3.74it/s]  6%|▋         | 254/3949 [01:28<22:57,  2.68it/s]  6%|▋         | 252/3949 [01:28<21:58,  2.80it/s]  8%|▊         | 305/3949 [01:28<16:48,  3.61it/s]  7%|▋         | 257/3949 [01:28<22:46,  2.70it/s]  7%|▋         | 290/3949 [01:28<15:22,  3.97it/s]  7%|▋         | 278/3949 [01:28<19:39,  3.11it/s]  8%|▊         | 306/3949 [01:28<16:36,  3.65it/s]  7%|▋         | 257/3949 [01:28<22:25,  2.74it/s]  7%|▋         | 291/3949 [01:28<14:10,  4.30it/s]  6%|▋         | 255/3949 [01:28<23:21,  2.64it/s]  6%|▋         | 253/3949 [01:29<22:12,  2.77it/s]  7%|▋         | 258/3949 [01:29<22:39,  2.71it/s]  7%|▋         | 279/3949 [01:29<19:48,  3.09it/s]  8%|▊         | 307/3949 [01:29<16:47,  3.62it/s]  7%|▋         | 292/3949 [01:29<14:57,  4.07it/s]  7%|▋         | 258/3949 [01:29<23:03,  2.67it/s]  6%|▋         | 256/3949 [01:29<23:09,  2.66it/s]  7%|▋         | 259/3949 [01:29<22:19,  2.75it/s]  8%|▊         | 308/3949 [01:29<16:50,  3.60it/s]  7%|▋         | 280/3949 [01:29<19:29,  3.14it/s]  7%|▋         | 293/3949 [01:29<14:58,  4.07it/s]  7%|▋         | 259/3949 [01:29<21:50,  2.82it/s]  7%|▋         | 257/3949 [01:29<22:00,  2.80it/s]  7%|▋         | 260/3949 [01:29<21:11,  2.90it/s]  8%|▊         | 309/3949 [01:29<16:41,  3.63it/s]  7%|▋         | 281/3949 [01:29<18:23,  3.32it/s]  7%|▋         | 294/3949 [01:29<15:21,  3.97it/s]  7%|▋         | 260/3949 [01:29<20:48,  2.96it/s]  7%|▋         | 258/3949 [01:29<20:21,  3.02it/s]  8%|▊         | 310/3949 [01:29<16:25,  3.69it/s]  7%|▋         | 282/3949 [01:30<17:38,  3.46it/s]  7%|▋         | 261/3949 [01:30<20:00,  3.07it/s]  7%|▋         | 295/3949 [01:30<15:55,  3.82it/s]  7%|▋         | 261/3949 [01:30<20:00,  3.07it/s]  7%|▋         | 259/3949 [01:30<19:30,  3.15it/s]  8%|▊         | 311/3949 [01:30<16:12,  3.74it/s]  7%|▋         | 283/3949 [01:30<17:04,  3.58it/s]  7%|▋         | 296/3949 [01:30<16:06,  3.78it/s]  7%|▋         | 262/3949 [01:30<19:30,  3.15it/s]  7%|▋         | 262/3949 [01:30<19:21,  3.17it/s]  8%|▊         | 312/3949 [01:30<15:51,  3.82it/s]  7%|▋         | 260/3949 [01:30<19:14,  3.19it/s]  7%|▋         | 284/3949 [01:30<16:42,  3.65it/s]  8%|▊         | 297/3949 [01:30<15:56,  3.82it/s]  7%|▋         | 263/3949 [01:30<20:19,  3.02it/s]  8%|▊         | 313/3949 [01:30<15:38,  3.88it/s]  7%|▋         | 285/3949 [01:30<16:15,  3.76it/s]  7%|▋         | 263/3949 [01:30<18:55,  3.25it/s]  7%|▋         | 261/3949 [01:30<19:20,  3.18it/s]  8%|▊         | 298/3949 [01:30<16:10,  3.76it/s]  7%|▋         | 264/3949 [01:30<19:40,  3.12it/s]  8%|▊         | 314/3949 [01:31<15:41,  3.86it/s]  7%|▋         | 286/3949 [01:31<16:01,  3.81it/s]  8%|▊         | 299/3949 [01:31<16:01,  3.80it/s]  7%|▋         | 264/3949 [01:31<19:16,  3.19it/s]  7%|▋         | 262/3949 [01:31<19:03,  3.22it/s]  7%|▋         | 265/3949 [01:31<19:00,  3.23it/s]  8%|▊         | 315/3949 [01:31<15:48,  3.83it/s]  7%|▋         | 287/3949 [01:31<16:01,  3.81it/s]  8%|▊         | 300/3949 [01:31<16:24,  3.71it/s]  7%|▋         | 265/3949 [01:31<19:19,  3.18it/s]  7%|▋         | 263/3949 [01:31<19:10,  3.20it/s]  8%|▊         | 316/3949 [01:31<15:38,  3.87it/s]  7%|▋         | 288/3949 [01:31<16:10,  3.77it/s]  7%|▋         | 266/3949 [01:31<19:01,  3.23it/s]  8%|▊         | 301/3949 [01:31<16:00,  3.80it/s]  7%|▋         | 266/3949 [01:31<18:47,  3.27it/s]  7%|▋         | 264/3949 [01:31<18:52,  3.25it/s]  8%|▊         | 317/3949 [01:31<15:38,  3.87it/s]  7%|▋         | 289/3949 [01:31<15:48,  3.86it/s]  7%|▋         | 267/3949 [01:31<18:59,  3.23it/s]  8%|▊         | 302/3949 [01:31<15:56,  3.81it/s]  7%|▋         | 267/3949 [01:32<18:33,  3.31it/s]  8%|▊         | 318/3949 [01:32<15:36,  3.88it/s]  7%|▋         | 265/3949 [01:32<18:53,  3.25it/s]  7%|▋         | 290/3949 [01:32<15:50,  3.85it/s]  8%|▊         | 303/3949 [01:32<16:09,  3.76it/s]  7%|▋         | 268/3949 [01:32<18:51,  3.25it/s]  8%|▊         | 319/3949 [01:32<14:58,  4.04it/s]  7%|▋         | 268/3949 [01:32<18:36,  3.30it/s]  7%|▋         | 291/3949 [01:32<15:51,  3.84it/s]  7%|▋         | 266/3949 [01:32<18:28,  3.32it/s]  8%|▊         | 304/3949 [01:32<15:48,  3.84it/s]  7%|▋         | 269/3949 [01:32<18:51,  3.25it/s]  8%|▊         | 320/3949 [01:32<15:23,  3.93it/s]  7%|▋         | 269/3949 [01:32<17:47,  3.45it/s]  7%|▋         | 292/3949 [01:32<16:16,  3.74it/s]  7%|▋         | 267/3949 [01:32<18:01,  3.40it/s]  8%|▊         | 305/3949 [01:32<16:12,  3.75it/s]  7%|▋         | 270/3949 [01:32<18:07,  3.38it/s]  8%|▊         | 321/3949 [01:32<15:41,  3.86it/s]  7%|▋         | 270/3949 [01:32<17:27,  3.51it/s]  7%|▋         | 293/3949 [01:32<16:21,  3.73it/s]  7%|▋         | 268/3949 [01:32<17:43,  3.46it/s]  8%|▊         | 306/3949 [01:32<16:22,  3.71it/s]  7%|▋         | 271/3949 [01:33<17:30,  3.50it/s]  8%|▊         | 322/3949 [01:33<15:47,  3.83it/s]  7%|▋         | 271/3949 [01:33<17:26,  3.52it/s]  7%|▋         | 294/3949 [01:33<16:09,  3.77it/s]  7%|▋         | 269/3949 [01:33<17:40,  3.47it/s]  8%|▊         | 307/3949 [01:33<16:06,  3.77it/s]  7%|▋         | 272/3949 [01:33<17:43,  3.46it/s]  8%|▊         | 323/3949 [01:33<15:38,  3.86it/s]  7%|▋         | 272/3949 [01:33<17:31,  3.50it/s]  7%|▋         | 295/3949 [01:33<16:20,  3.73it/s]  7%|▋         | 270/3949 [01:33<17:21,  3.53it/s]  8%|▊         | 308/3949 [01:33<15:47,  3.84it/s]  7%|▋         | 273/3949 [01:33<17:12,  3.56it/s]  8%|▊         | 324/3949 [01:33<15:30,  3.89it/s]  7%|▋         | 273/3949 [01:33<17:24,  3.52it/s]  7%|▋         | 296/3949 [01:33<16:32,  3.68it/s]  7%|▋         | 271/3949 [01:33<16:55,  3.62it/s]  8%|▊         | 309/3949 [01:33<16:24,  3.70it/s]  8%|▊         | 325/3949 [01:33<15:51,  3.81it/s]  7%|▋         | 274/3949 [01:33<17:15,  3.55it/s]  7%|▋         | 274/3949 [01:33<17:11,  3.56it/s]  7%|▋         | 272/3949 [01:33<16:40,  3.67it/s]  8%|▊         | 297/3949 [01:33<16:42,  3.64it/s]  8%|▊         | 310/3949 [01:34<16:23,  3.70it/s]  8%|▊         | 326/3949 [01:34<16:08,  3.74it/s]  7%|▋         | 275/3949 [01:34<17:11,  3.56it/s]  7%|▋         | 275/3949 [01:34<16:56,  3.62it/s]  7%|▋         | 273/3949 [01:34<16:41,  3.67it/s]  8%|▊         | 298/3949 [01:34<16:44,  3.64it/s]  8%|▊         | 311/3949 [01:34<16:26,  3.69it/s]  7%|▋         | 276/3949 [01:34<16:38,  3.68it/s]  8%|▊         | 327/3949 [01:34<16:21,  3.69it/s]  7%|▋         | 276/3949 [01:34<15:52,  3.86it/s]  8%|▊         | 299/3949 [01:34<16:05,  3.78it/s]  8%|▊         | 312/3949 [01:34<16:22,  3.70it/s]  7%|▋         | 277/3949 [01:34<15:15,  4.01it/s]  7%|▋         | 277/3949 [01:34<14:40,  4.17it/s]  8%|▊         | 328/3949 [01:34<15:43,  3.84it/s]  8%|▊         | 300/3949 [01:34<16:03,  3.79it/s]  7%|▋         | 278/3949 [01:34<14:15,  4.29it/s]  8%|▊         | 313/3949 [01:34<15:34,  3.89it/s]  7%|▋         | 278/3949 [01:34<13:57,  4.38it/s]  8%|▊         | 329/3949 [01:34<15:11,  3.97it/s]  7%|▋         | 279/3949 [01:34<13:21,  4.58it/s]  8%|▊         | 301/3949 [01:34<15:20,  3.96it/s]  7%|▋         | 279/3949 [01:35<13:12,  4.63it/s]  8%|▊         | 314/3949 [01:35<15:47,  3.84it/s]  7%|▋         | 280/3949 [01:35<12:26,  4.92it/s]  8%|▊         | 330/3949 [01:35<15:51,  3.80it/s]  8%|▊         | 302/3949 [01:35<15:36,  3.90it/s]  7%|▋         | 280/3949 [01:35<13:11,  4.63it/s]  7%|▋         | 281/3949 [01:35<11:36,  5.27it/s]  8%|▊         | 315/3949 [01:35<15:50,  3.82it/s]  7%|▋         | 281/3949 [01:35<12:34,  4.86it/s]  8%|▊         | 331/3949 [01:35<15:50,  3.81it/s]  7%|▋         | 282/3949 [01:35<11:27,  5.34it/s]  8%|▊         | 303/3949 [01:35<15:48,  3.85it/s]  8%|▊         | 316/3949 [01:35<16:12,  3.74it/s]  7%|▋         | 282/3949 [01:35<12:16,  4.98it/s]  8%|▊         | 332/3949 [01:35<15:25,  3.91it/s]  7%|▋         | 283/3949 [01:35<11:50,  5.16it/s]  8%|▊         | 304/3949 [01:35<15:43,  3.86it/s]  7%|▋         | 283/3949 [01:35<11:42,  5.22it/s]  7%|▋         | 284/3949 [01:35<11:54,  5.13it/s]  8%|▊         | 317/3949 [01:35<16:08,  3.75it/s]  8%|▊         | 333/3949 [01:35<15:20,  3.93it/s]  7%|▋         | 284/3949 [01:35<11:42,  5.22it/s]  8%|▊         | 305/3949 [01:36<15:32,  3.91it/s]  7%|▋         | 285/3949 [01:36<12:08,  5.03it/s]  8%|▊         | 318/3949 [01:36<15:26,  3.92it/s]  8%|▊         | 334/3949 [01:36<15:26,  3.90it/s]  7%|▋         | 285/3949 [01:36<12:23,  4.93it/s]  7%|▋         | 286/3949 [01:36<11:53,  5.13it/s]  8%|▊         | 306/3949 [01:36<15:48,  3.84it/s]  8%|▊         | 319/3949 [01:36<15:16,  3.96it/s]  7%|▋         | 286/3949 [01:36<11:48,  5.17it/s]  8%|▊         | 335/3949 [01:36<15:07,  3.98it/s]  7%|▋         | 287/3949 [01:36<11:57,  5.10it/s]  8%|▊         | 307/3949 [01:36<15:00,  4.04it/s]  7%|▋         | 287/3949 [01:36<12:10,  5.01it/s]  8%|▊         | 320/3949 [01:36<15:03,  4.02it/s]  9%|▊         | 336/3949 [01:36<14:57,  4.03it/s]  7%|▋         | 288/3949 [01:36<12:08,  5.03it/s]  8%|▊         | 308/3949 [01:36<14:48,  4.10it/s]  7%|▋         | 288/3949 [01:36<12:14,  4.99it/s]  8%|▊         | 321/3949 [01:36<14:39,  4.12it/s]  7%|▋         | 289/3949 [01:36<12:06,  5.04it/s]  9%|▊         | 337/3949 [01:36<14:51,  4.05it/s]  7%|▋         | 289/3949 [01:36<12:04,  5.05it/s]  8%|▊         | 309/3949 [01:37<15:11,  3.99it/s]  8%|▊         | 322/3949 [01:37<14:45,  4.10it/s]  7%|▋         | 290/3949 [01:37<12:12,  5.00it/s]  9%|▊         | 338/3949 [01:37<14:53,  4.04it/s]  7%|▋         | 290/3949 [01:37<11:55,  5.11it/s]  8%|▊         | 310/3949 [01:37<15:14,  3.98it/s]  7%|▋         | 291/3949 [01:37<12:17,  4.96it/s]  8%|▊         | 323/3949 [01:37<14:59,  4.03it/s]  7%|▋         | 291/3949 [01:37<11:59,  5.08it/s]  9%|▊         | 339/3949 [01:37<15:08,  3.97it/s]  7%|▋         | 292/3949 [01:37<12:12,  4.99it/s]  8%|▊         | 311/3949 [01:37<15:11,  3.99it/s]  7%|▋         | 292/3949 [01:37<11:29,  5.31it/s]  8%|▊         | 324/3949 [01:37<15:25,  3.92it/s]  9%|▊         | 340/3949 [01:37<15:00,  4.01it/s]  7%|▋         | 293/3949 [01:37<12:20,  4.94it/s]  7%|▋         | 293/3949 [01:37<11:33,  5.27it/s]  8%|▊         | 312/3949 [01:37<15:19,  3.95it/s]  8%|▊         | 325/3949 [01:37<14:57,  4.04it/s]  7%|▋         | 294/3949 [01:37<12:26,  4.89it/s]  9%|▊         | 341/3949 [01:37<14:45,  4.07it/s]  7%|▋         | 294/3949 [01:37<11:44,  5.19it/s]  8%|▊         | 313/3949 [01:38<15:14,  3.98it/s]  8%|▊         | 326/3949 [01:38<14:50,  4.07it/s]  7%|▋         | 295/3949 [01:38<12:29,  4.88it/s]  7%|▋         | 295/3949 [01:38<11:56,  5.10it/s]  9%|▊         | 342/3949 [01:38<15:06,  3.98it/s]  8%|▊         | 314/3949 [01:38<14:37,  4.14it/s]  7%|▋         | 296/3949 [01:38<12:32,  4.86it/s]  8%|▊         | 327/3949 [01:38<15:06,  4.00it/s]  7%|▋         | 296/3949 [01:38<12:03,  5.05it/s]  9%|▊         | 343/3949 [01:38<14:45,  4.07it/s]  8%|▊         | 315/3949 [01:38<14:36,  4.15it/s]  8%|▊         | 297/3949 [01:38<12:39,  4.81it/s]  8%|▊         | 297/3949 [01:38<11:45,  5.18it/s]  8%|▊         | 328/3949 [01:38<14:41,  4.11it/s]  9%|▊         | 344/3949 [01:38<15:11,  3.95it/s]  8%|▊         | 298/3949 [01:38<12:05,  5.03it/s]  8%|▊         | 316/3949 [01:38<14:35,  4.15it/s]  8%|▊         | 298/3949 [01:38<12:02,  5.05it/s]  8%|▊         | 329/3949 [01:38<14:46,  4.08it/s]  8%|▊         | 299/3949 [01:38<12:13,  4.98it/s]  9%|▊         | 345/3949 [01:38<15:20,  3.91it/s]  8%|▊         | 299/3949 [01:38<12:07,  5.02it/s]  8%|▊         | 317/3949 [01:38<14:49,  4.09it/s]  8%|▊         | 330/3949 [01:39<15:13,  3.96it/s]  8%|▊         | 300/3949 [01:39<11:48,  5.15it/s]  8%|▊         | 300/3949 [01:39<11:43,  5.19it/s]  9%|▉         | 346/3949 [01:39<15:33,  3.86it/s]  8%|▊         | 301/3949 [01:39<11:46,  5.16it/s]  8%|▊         | 318/3949 [01:39<16:05,  3.76it/s]  8%|▊         | 331/3949 [01:39<14:39,  4.11it/s]  8%|▊         | 301/3949 [01:39<11:45,  5.17it/s]  9%|▉         | 347/3949 [01:39<15:14,  3.94it/s]  8%|▊         | 302/3949 [01:39<11:56,  5.09it/s]  8%|▊         | 302/3949 [01:39<11:56,  5.09it/s]  8%|▊         | 319/3949 [01:39<15:48,  3.83it/s]  8%|▊         | 332/3949 [01:39<14:39,  4.11it/s]  8%|▊         | 303/3949 [01:39<11:39,  5.21it/s]  8%|▊         | 303/3949 [01:39<11:29,  5.29it/s]  9%|▉         | 348/3949 [01:39<15:29,  3.87it/s]  8%|▊         | 333/3949 [01:39<14:49,  4.06it/s]  8%|▊         | 320/3949 [01:39<16:03,  3.77it/s]  8%|▊         | 304/3949 [01:39<10:57,  5.54it/s]  8%|▊         | 304/3949 [01:39<11:38,  5.22it/s]  9%|▉         | 349/3949 [01:39<15:32,  3.86it/s]  8%|▊         | 305/3949 [01:40<11:08,  5.45it/s]  8%|▊         | 334/3949 [01:40<15:11,  3.97it/s]  8%|▊         | 305/3949 [01:40<12:10,  4.99it/s]  8%|▊         | 321/3949 [01:40<16:06,  3.75it/s]  8%|▊         | 306/3949 [01:40<11:13,  5.41it/s]  9%|▉         | 350/3949 [01:40<15:39,  3.83it/s]  8%|▊         | 335/3949 [01:40<15:07,  3.98it/s]  8%|▊         | 306/3949 [01:40<12:54,  4.70it/s]  8%|▊         | 322/3949 [01:40<15:57,  3.79it/s]  8%|▊         | 307/3949 [01:40<11:22,  5.34it/s]  8%|▊         | 307/3949 [01:40<12:08,  5.00it/s]  9%|▉         | 351/3949 [01:40<15:45,  3.80it/s]  9%|▊         | 336/3949 [01:40<15:17,  3.94it/s]  8%|▊         | 323/3949 [01:40<15:44,  3.84it/s]  8%|▊         | 308/3949 [01:40<11:02,  5.50it/s]  8%|▊         | 308/3949 [01:40<12:29,  4.86it/s]  8%|▊         | 309/3949 [01:40<10:56,  5.55it/s]  9%|▉         | 352/3949 [01:40<15:54,  3.77it/s]  9%|▊         | 337/3949 [01:40<15:12,  3.96it/s]  8%|▊         | 324/3949 [01:40<15:44,  3.84it/s]  8%|▊         | 309/3949 [01:40<12:51,  4.72it/s]  8%|▊         | 310/3949 [01:40<11:15,  5.39it/s]  9%|▉         | 353/3949 [01:41<15:37,  3.84it/s]  9%|▊         | 338/3949 [01:41<15:39,  3.85it/s]  8%|▊         | 310/3949 [01:41<12:31,  4.84it/s]  8%|▊         | 325/3949 [01:41<16:33,  3.65it/s]  8%|▊         | 311/3949 [01:41<11:36,  5.23it/s]  9%|▉         | 354/3949 [01:41<15:32,  3.85it/s]  8%|▊         | 311/3949 [01:41<12:25,  4.88it/s]  9%|▊         | 339/3949 [01:41<15:41,  3.83it/s]  8%|▊         | 312/3949 [01:41<11:49,  5.13it/s]  8%|▊         | 326/3949 [01:41<15:50,  3.81it/s]  9%|▉         | 355/3949 [01:41<15:35,  3.84it/s]  8%|▊         | 312/3949 [01:41<13:03,  4.64it/s]  8%|▊         | 313/3949 [01:41<11:58,  5.06it/s]  9%|▊         | 340/3949 [01:41<15:21,  3.91it/s]  8%|▊         | 327/3949 [01:41<15:23,  3.92it/s]  8%|▊         | 313/3949 [01:41<12:37,  4.80it/s]  8%|▊         | 314/3949 [01:41<12:04,  5.02it/s]  9%|▉         | 356/3949 [01:41<15:15,  3.93it/s]  9%|▊         | 341/3949 [01:41<15:08,  3.97it/s]  8%|▊         | 328/3949 [01:41<14:57,  4.03it/s]  8%|▊         | 314/3949 [01:41<12:35,  4.81it/s]  8%|▊         | 315/3949 [01:41<12:11,  4.96it/s]  9%|▉         | 357/3949 [01:42<15:06,  3.96it/s]  9%|▊         | 342/3949 [01:42<14:59,  4.01it/s]  8%|▊         | 329/3949 [01:42<15:03,  4.01it/s]  8%|▊         | 315/3949 [01:42<12:10,  4.97it/s]  8%|▊         | 316/3949 [01:42<12:17,  4.92it/s]  9%|▉         | 358/3949 [01:42<15:28,  3.87it/s]  9%|▊         | 343/3949 [01:42<14:39,  4.10it/s]  8%|▊         | 316/3949 [01:42<12:05,  5.01it/s]  8%|▊         | 330/3949 [01:42<14:58,  4.03it/s]  8%|▊         | 317/3949 [01:42<12:21,  4.90it/s]  8%|▊         | 317/3949 [01:42<11:41,  5.18it/s]  9%|▉         | 359/3949 [01:42<15:10,  3.94it/s]  8%|▊         | 318/3949 [01:42<12:10,  4.97it/s]  9%|▊         | 344/3949 [01:42<15:08,  3.97it/s]  8%|▊         | 331/3949 [01:42<14:58,  4.03it/s]  8%|▊         | 318/3949 [01:42<11:45,  5.14it/s]  8%|▊         | 319/3949 [01:42<11:45,  5.15it/s]  9%|▉         | 360/3949 [01:42<15:13,  3.93it/s]  8%|▊         | 332/3949 [01:42<15:19,  3.93it/s]  9%|▊         | 345/3949 [01:42<15:46,  3.81it/s]  8%|▊         | 319/3949 [01:42<11:17,  5.35it/s]  8%|▊         | 320/3949 [01:42<11:04,  5.46it/s]  9%|▉         | 361/3949 [01:43<15:15,  3.92it/s]  8%|▊         | 321/3949 [01:43<10:35,  5.71it/s]  8%|▊         | 320/3949 [01:43<11:40,  5.18it/s]  8%|▊         | 333/3949 [01:43<15:36,  3.86it/s]  9%|▉         | 346/3949 [01:43<16:06,  3.73it/s]  8%|▊         | 322/3949 [01:43<10:11,  5.93it/s]  8%|▊         | 321/3949 [01:43<11:28,  5.27it/s]  9%|▉         | 362/3949 [01:43<15:51,  3.77it/s]  8%|▊         | 334/3949 [01:43<15:12,  3.96it/s]  8%|▊         | 323/3949 [01:43<09:58,  6.06it/s]  9%|▉         | 347/3949 [01:43<16:13,  3.70it/s]  8%|▊         | 322/3949 [01:43<12:06,  4.99it/s]  8%|▊         | 324/3949 [01:43<09:50,  6.14it/s]  9%|▉         | 363/3949 [01:43<16:15,  3.67it/s]  8%|▊         | 335/3949 [01:43<15:24,  3.91it/s]  8%|▊         | 323/3949 [01:43<11:53,  5.08it/s]  9%|▉         | 348/3949 [01:43<16:04,  3.73it/s]  8%|▊         | 325/3949 [01:43<10:08,  5.96it/s]  8%|▊         | 324/3949 [01:43<11:46,  5.13it/s]  9%|▉         | 364/3949 [01:43<16:04,  3.72it/s]  8%|▊         | 326/3949 [01:43<10:06,  5.97it/s]  9%|▊         | 336/3949 [01:43<15:53,  3.79it/s]  9%|▉         | 349/3949 [01:43<15:56,  3.76it/s]  8%|▊         | 327/3949 [01:44<09:58,  6.06it/s]  8%|▊         | 325/3949 [01:44<12:04,  5.01it/s]  9%|▉         | 365/3949 [01:44<16:04,  3.72it/s]  9%|▊         | 337/3949 [01:44<15:57,  3.77it/s]  8%|▊         | 328/3949 [01:44<10:02,  6.01it/s]  9%|▉         | 350/3949 [01:44<16:03,  3.73it/s]  8%|▊         | 326/3949 [01:44<12:01,  5.02it/s]  9%|▉         | 366/3949 [01:44<15:44,  3.79it/s]  9%|▊         | 338/3949 [01:44<15:11,  3.96it/s]  8%|▊         | 329/3949 [01:44<10:50,  5.56it/s]  9%|▉         | 351/3949 [01:44<15:41,  3.82it/s]  8%|▊         | 327/3949 [01:44<12:47,  4.72it/s]  9%|▊         | 339/3949 [01:44<14:51,  4.05it/s]  8%|▊         | 330/3949 [01:44<11:28,  5.26it/s]  9%|▉         | 367/3949 [01:44<15:15,  3.91it/s]  9%|▉         | 352/3949 [01:44<15:05,  3.97it/s]  8%|▊         | 328/3949 [01:44<12:32,  4.81it/s]  8%|▊         | 331/3949 [01:44<12:02,  5.01it/s]  9%|▊         | 340/3949 [01:44<14:31,  4.14it/s]  9%|▉         | 368/3949 [01:44<14:48,  4.03it/s]  8%|▊         | 329/3949 [01:44<12:31,  4.81it/s]  9%|▉         | 353/3949 [01:44<14:48,  4.05it/s]  8%|▊         | 332/3949 [01:45<12:14,  4.93it/s]  9%|▊         | 341/3949 [01:45<14:18,  4.20it/s]  9%|▉         | 369/3949 [01:45<14:27,  4.13it/s]  8%|▊         | 330/3949 [01:45<12:13,  4.93it/s]  9%|▉         | 354/3949 [01:45<14:45,  4.06it/s]  8%|▊         | 333/3949 [01:45<12:08,  4.96it/s]  8%|▊         | 331/3949 [01:45<11:54,  5.07it/s]  9%|▉         | 370/3949 [01:45<14:32,  4.10it/s]  9%|▊         | 342/3949 [01:45<14:58,  4.02it/s]  9%|▉         | 355/3949 [01:45<14:57,  4.00it/s]  8%|▊         | 334/3949 [01:45<12:06,  4.98it/s]  8%|▊         | 332/3949 [01:45<12:04,  4.99it/s]  9%|▉         | 371/3949 [01:45<14:41,  4.06it/s]  9%|▊         | 343/3949 [01:45<15:11,  3.96it/s]  8%|▊         | 335/3949 [01:45<11:39,  5.17it/s]  9%|▉         | 356/3949 [01:45<14:33,  4.11it/s]  8%|▊         | 333/3949 [01:45<12:04,  4.99it/s]  9%|▊         | 336/3949 [01:45<11:01,  5.46it/s]  9%|▊         | 344/3949 [01:45<14:37,  4.11it/s]  9%|▉         | 372/3949 [01:45<15:25,  3.87it/s]  8%|▊         | 334/3949 [01:45<12:15,  4.92it/s]  9%|▉         | 357/3949 [01:45<15:24,  3.88it/s]  9%|▊         | 337/3949 [01:46<11:14,  5.36it/s]  9%|▊         | 345/3949 [01:46<14:38,  4.10it/s]  8%|▊         | 335/3949 [01:46<11:54,  5.06it/s]  9%|▉         | 373/3949 [01:46<14:49,  4.02it/s]  9%|▊         | 338/3949 [01:46<11:05,  5.43it/s]  9%|▉         | 358/3949 [01:46<15:26,  3.88it/s]  9%|▊         | 336/3949 [01:46<12:01,  5.01it/s]  9%|▉         | 346/3949 [01:46<14:42,  4.08it/s]  9%|▉         | 374/3949 [01:46<14:51,  4.01it/s]  9%|▊         | 339/3949 [01:46<11:17,  5.33it/s]  9%|▉         | 359/3949 [01:46<15:34,  3.84it/s]  9%|▊         | 337/3949 [01:46<11:54,  5.06it/s]  9%|▊         | 340/3949 [01:46<11:14,  5.35it/s]  9%|▉         | 375/3949 [01:46<14:31,  4.10it/s]  9%|▉         | 347/3949 [01:46<15:02,  3.99it/s]  9%|▊         | 338/3949 [01:46<12:00,  5.02it/s]  9%|▉         | 360/3949 [01:46<15:30,  3.86it/s]  9%|▊         | 341/3949 [01:46<11:05,  5.42it/s] 10%|▉         | 376/3949 [01:46<14:32,  4.09it/s]  9%|▉         | 348/3949 [01:46<15:19,  3.92it/s]  9%|▊         | 342/3949 [01:46<10:59,  5.47it/s]  9%|▊         | 339/3949 [01:46<12:13,  4.92it/s]  9%|▉         | 361/3949 [01:47<15:22,  3.89it/s]  9%|▊         | 343/3949 [01:47<10:06,  5.94it/s] 10%|▉         | 377/3949 [01:47<15:11,  3.92it/s]  9%|▊         | 340/3949 [01:47<12:16,  4.90it/s]  9%|▉         | 349/3949 [01:47<15:56,  3.76it/s]  9%|▊         | 344/3949 [01:47<09:35,  6.27it/s]  9%|▉         | 362/3949 [01:47<15:27,  3.87it/s]  9%|▊         | 341/3949 [01:47<12:23,  4.85it/s]  9%|▊         | 345/3949 [01:47<09:49,  6.12it/s] 10%|▉         | 378/3949 [01:47<15:23,  3.87it/s]  9%|▉         | 350/3949 [01:47<15:46,  3.80it/s]  9%|▉         | 363/3949 [01:47<15:17,  3.91it/s]  9%|▉         | 346/3949 [01:47<09:41,  6.20it/s]  9%|▊         | 342/3949 [01:47<12:30,  4.81it/s] 10%|▉         | 379/3949 [01:47<15:30,  3.84it/s]  9%|▉         | 351/3949 [01:47<15:42,  3.82it/s]  9%|▉         | 347/3949 [01:47<10:00,  6.00it/s]  9%|▊         | 343/3949 [01:47<12:25,  4.84it/s]  9%|▉         | 364/3949 [01:47<15:52,  3.77it/s]  9%|▉         | 348/3949 [01:47<10:28,  5.73it/s] 10%|▉         | 380/3949 [01:47<15:12,  3.91it/s]  9%|▉         | 352/3949 [01:47<15:29,  3.87it/s]  9%|▊         | 344/3949 [01:47<12:43,  4.72it/s]  9%|▉         | 365/3949 [01:48<15:23,  3.88it/s]  9%|▉         | 349/3949 [01:48<10:38,  5.63it/s] 10%|▉         | 381/3949 [01:48<15:00,  3.96it/s]  9%|▊         | 345/3949 [01:48<12:33,  4.78it/s]  9%|▉         | 353/3949 [01:48<15:19,  3.91it/s]  9%|▉         | 350/3949 [01:48<10:44,  5.58it/s]  9%|▉         | 366/3949 [01:48<15:21,  3.89it/s] 10%|▉         | 382/3949 [01:48<14:55,  3.98it/s]  9%|▉         | 346/3949 [01:48<12:49,  4.68it/s]  9%|▉         | 354/3949 [01:48<15:00,  3.99it/s]  9%|▉         | 351/3949 [01:48<11:07,  5.39it/s]  9%|▉         | 367/3949 [01:48<14:59,  3.98it/s]  9%|▉         | 347/3949 [01:48<12:16,  4.89it/s] 10%|▉         | 383/3949 [01:48<15:12,  3.91it/s]  9%|▉         | 355/3949 [01:48<14:51,  4.03it/s]  9%|▉         | 352/3949 [01:48<11:26,  5.24it/s]  9%|▉         | 368/3949 [01:48<14:26,  4.13it/s]  9%|▉         | 348/3949 [01:48<12:33,  4.78it/s]  9%|▉         | 353/3949 [01:48<11:34,  5.18it/s] 10%|▉         | 384/3949 [01:48<14:49,  4.01it/s]  9%|▉         | 356/3949 [01:48<14:38,  4.09it/s]  9%|▉         | 369/3949 [01:49<14:28,  4.12it/s]  9%|▉         | 349/3949 [01:49<12:13,  4.91it/s]  9%|▉         | 354/3949 [01:49<12:19,  4.86it/s]  9%|▉         | 357/3949 [01:49<14:03,  4.26it/s] 10%|▉         | 385/3949 [01:49<14:48,  4.01it/s]  9%|▉         | 350/3949 [01:49<12:11,  4.92it/s]  9%|▉         | 370/3949 [01:49<14:51,  4.01it/s]  9%|▉         | 355/3949 [01:49<12:25,  4.82it/s]  9%|▉         | 358/3949 [01:49<14:08,  4.23it/s] 10%|▉         | 386/3949 [01:49<14:44,  4.03it/s]  9%|▉         | 351/3949 [01:49<12:10,  4.92it/s]  9%|▉         | 371/3949 [01:49<14:48,  4.03it/s]  9%|▉         | 356/3949 [01:49<12:00,  4.99it/s]  9%|▉         | 352/3949 [01:49<12:00,  5.00it/s]  9%|▉         | 359/3949 [01:49<14:20,  4.17it/s] 10%|▉         | 387/3949 [01:49<14:45,  4.02it/s]  9%|▉         | 357/3949 [01:49<12:13,  4.89it/s]  9%|▉         | 372/3949 [01:49<14:31,  4.10it/s]  9%|▉         | 353/3949 [01:49<11:38,  5.15it/s]  9%|▉         | 360/3949 [01:49<14:41,  4.07it/s]  9%|▉         | 358/3949 [01:49<11:57,  5.01it/s] 10%|▉         | 388/3949 [01:49<15:06,  3.93it/s]  9%|▉         | 354/3949 [01:49<11:42,  5.12it/s]  9%|▉         | 373/3949 [01:50<15:14,  3.91it/s]  9%|▉         | 359/3949 [01:50<11:31,  5.19it/s]  9%|▉         | 361/3949 [01:50<14:54,  4.01it/s]  9%|▉         | 355/3949 [01:50<11:32,  5.19it/s] 10%|▉         | 389/3949 [01:50<15:29,  3.83it/s]  9%|▉         | 374/3949 [01:50<14:43,  4.04it/s]  9%|▉         | 360/3949 [01:50<11:54,  5.02it/s]  9%|▉         | 356/3949 [01:50<11:26,  5.23it/s]  9%|▉         | 362/3949 [01:50<14:40,  4.07it/s] 10%|▉         | 390/3949 [01:50<15:01,  3.95it/s]  9%|▉         | 361/3949 [01:50<11:50,  5.05it/s]  9%|▉         | 375/3949 [01:50<14:45,  4.04it/s]  9%|▉         | 357/3949 [01:50<11:55,  5.02it/s]  9%|▉         | 363/3949 [01:50<14:40,  4.07it/s] 10%|▉         | 391/3949 [01:50<14:36,  4.06it/s]  9%|▉         | 362/3949 [01:50<12:15,  4.88it/s] 10%|▉         | 376/3949 [01:50<14:40,  4.06it/s]  9%|▉         | 358/3949 [01:50<12:02,  4.97it/s]  9%|▉         | 364/3949 [01:50<14:12,  4.21it/s] 10%|▉         | 392/3949 [01:50<14:18,  4.14it/s]  9%|▉         | 363/3949 [01:50<12:25,  4.81it/s] 10%|▉         | 377/3949 [01:50<14:18,  4.16it/s]  9%|▉         | 359/3949 [01:50<12:10,  4.91it/s]  9%|▉         | 365/3949 [01:51<14:03,  4.25it/s] 10%|▉         | 393/3949 [01:51<14:06,  4.20it/s]  9%|▉         | 364/3949 [01:51<12:32,  4.76it/s]  9%|▉         | 360/3949 [01:51<12:13,  4.89it/s] 10%|▉         | 378/3949 [01:51<14:11,  4.20it/s]  9%|▉         | 366/3949 [01:51<13:56,  4.28it/s]  9%|▉         | 365/3949 [01:51<12:29,  4.78it/s] 10%|▉         | 394/3949 [01:51<14:05,  4.20it/s]  9%|▉         | 361/3949 [01:51<12:31,  4.78it/s] 10%|▉         | 379/3949 [01:51<14:16,  4.17it/s]  9%|▉         | 367/3949 [01:51<14:03,  4.25it/s]  9%|▉         | 366/3949 [01:51<12:55,  4.62it/s] 10%|█         | 395/3949 [01:51<14:00,  4.23it/s]  9%|▉         | 362/3949 [01:51<12:00,  4.98it/s] 10%|▉         | 380/3949 [01:51<14:14,  4.18it/s]  9%|▉         | 368/3949 [01:51<14:09,  4.22it/s]  9%|▉         | 367/3949 [01:51<12:31,  4.76it/s]  9%|▉         | 363/3949 [01:51<12:07,  4.93it/s] 10%|█         | 396/3949 [01:51<13:48,  4.29it/s] 10%|▉         | 381/3949 [01:51<14:13,  4.18it/s]  9%|▉         | 368/3949 [01:51<12:28,  4.78it/s]  9%|▉         | 364/3949 [01:52<12:06,  4.93it/s]  9%|▉         | 369/3949 [01:52<14:28,  4.12it/s] 10%|█         | 397/3949 [01:52<13:56,  4.25it/s]  9%|▉         | 369/3949 [01:52<12:12,  4.89it/s]  9%|▉         | 365/3949 [01:52<11:49,  5.05it/s] 10%|▉         | 382/3949 [01:52<14:51,  4.00it/s]  9%|▉         | 370/3949 [01:52<14:32,  4.10it/s] 10%|█         | 398/3949 [01:52<14:42,  4.02it/s]  9%|▉         | 370/3949 [01:52<11:51,  5.03it/s]  9%|▉         | 366/3949 [01:52<11:34,  5.16it/s] 10%|▉         | 383/3949 [01:52<14:33,  4.08it/s]  9%|▉         | 371/3949 [01:52<14:51,  4.01it/s]  9%|▉         | 367/3949 [01:52<11:00,  5.43it/s]  9%|▉         | 371/3949 [01:52<11:37,  5.13it/s] 10%|█         | 399/3949 [01:52<14:51,  3.98it/s] 10%|▉         | 384/3949 [01:52<14:50,  4.00it/s]  9%|▉         | 368/3949 [01:52<10:56,  5.46it/s]  9%|▉         | 372/3949 [01:52<11:49,  5.04it/s]  9%|▉         | 372/3949 [01:52<15:29,  3.85it/s] 10%|█         | 400/3949 [01:52<15:08,  3.91it/s]  9%|▉         | 369/3949 [01:52<10:49,  5.51it/s] 10%|▉         | 385/3949 [01:52<14:59,  3.96it/s]  9%|▉         | 373/3949 [01:52<11:54,  5.01it/s]  9%|▉         | 370/3949 [01:53<10:44,  5.56it/s]  9%|▉         | 373/3949 [01:53<15:34,  3.83it/s] 10%|█         | 401/3949 [01:53<15:29,  3.82it/s]  9%|▉         | 374/3949 [01:53<11:32,  5.16it/s] 10%|▉         | 386/3949 [01:53<15:44,  3.77it/s]  9%|▉         | 371/3949 [01:53<10:39,  5.59it/s]  9%|▉         | 375/3949 [01:53<11:41,  5.09it/s]  9%|▉         | 374/3949 [01:53<15:48,  3.77it/s] 10%|█         | 402/3949 [01:53<15:24,  3.84it/s]  9%|▉         | 372/3949 [01:53<10:55,  5.46it/s] 10%|▉         | 387/3949 [01:53<15:46,  3.76it/s] 10%|▉         | 376/3949 [01:53<11:24,  5.22it/s]  9%|▉         | 373/3949 [01:53<10:18,  5.78it/s]  9%|▉         | 375/3949 [01:53<15:28,  3.85it/s] 10%|█         | 403/3949 [01:53<15:36,  3.79it/s] 10%|▉         | 377/3949 [01:53<11:56,  4.99it/s] 10%|▉         | 388/3949 [01:53<15:12,  3.90it/s]  9%|▉         | 374/3949 [01:53<10:45,  5.53it/s] 10%|▉         | 376/3949 [01:53<15:12,  3.92it/s] 10%|█         | 404/3949 [01:53<15:01,  3.93it/s] 10%|▉         | 378/3949 [01:53<11:58,  4.97it/s]  9%|▉         | 375/3949 [01:53<10:52,  5.48it/s] 10%|▉         | 389/3949 [01:54<15:16,  3.88it/s] 10%|▉         | 377/3949 [01:54<15:16,  3.90it/s] 10%|▉         | 376/3949 [01:54<10:31,  5.66it/s] 10%|█         | 405/3949 [01:54<14:58,  3.95it/s] 10%|▉         | 379/3949 [01:54<12:10,  4.89it/s] 10%|▉         | 390/3949 [01:54<15:27,  3.84it/s] 10%|▉         | 377/3949 [01:54<10:35,  5.62it/s] 10%|▉         | 380/3949 [01:54<12:27,  4.78it/s] 10%|▉         | 378/3949 [01:54<15:32,  3.83it/s] 10%|█         | 406/3949 [01:54<15:04,  3.92it/s] 10%|▉         | 378/3949 [01:54<10:33,  5.63it/s] 10%|▉         | 391/3949 [01:54<15:01,  3.95it/s] 10%|▉         | 381/3949 [01:54<12:39,  4.70it/s] 10%|▉         | 379/3949 [01:54<15:06,  3.94it/s] 10%|█         | 407/3949 [01:54<15:08,  3.90it/s] 10%|▉         | 379/3949 [01:54<10:34,  5.63it/s] 10%|▉         | 392/3949 [01:54<14:51,  3.99it/s] 10%|▉         | 382/3949 [01:54<12:17,  4.84it/s] 10%|▉         | 380/3949 [01:54<14:50,  4.01it/s] 10%|▉         | 380/3949 [01:54<10:38,  5.59it/s] 10%|█         | 408/3949 [01:54<15:20,  3.85it/s] 10%|▉         | 393/3949 [01:55<14:50,  4.00it/s] 10%|▉         | 383/3949 [01:55<12:27,  4.77it/s] 10%|▉         | 381/3949 [01:55<10:52,  5.47it/s] 10%|▉         | 381/3949 [01:55<14:40,  4.05it/s] 10%|█         | 409/3949 [01:55<15:04,  3.91it/s] 10%|▉         | 384/3949 [01:55<12:20,  4.81it/s] 10%|▉         | 382/3949 [01:55<10:45,  5.53it/s] 10%|▉         | 394/3949 [01:55<14:55,  3.97it/s] 10%|▉         | 382/3949 [01:55<14:57,  3.98it/s] 10%|▉         | 383/3949 [01:55<10:32,  5.64it/s] 10%|▉         | 385/3949 [01:55<11:57,  4.97it/s] 10%|█         | 410/3949 [01:55<15:28,  3.81it/s] 10%|█         | 395/3949 [01:55<14:49,  3.99it/s] 10%|▉         | 384/3949 [01:55<10:39,  5.58it/s] 10%|▉         | 386/3949 [01:55<11:40,  5.09it/s] 10%|▉         | 383/3949 [01:55<15:09,  3.92it/s] 10%|█         | 411/3949 [01:55<15:24,  3.83it/s] 10%|▉         | 385/3949 [01:55<10:08,  5.86it/s] 10%|█         | 396/3949 [01:55<15:00,  3.95it/s] 10%|▉         | 387/3949 [01:55<12:13,  4.86it/s] 10%|▉         | 386/3949 [01:55<09:40,  6.14it/s] 10%|▉         | 384/3949 [01:55<15:08,  3.92it/s] 10%|█         | 412/3949 [01:55<15:27,  3.81it/s] 10%|█         | 397/3949 [01:56<14:57,  3.96it/s] 10%|▉         | 387/3949 [01:56<09:18,  6.37it/s] 10%|▉         | 388/3949 [01:56<12:18,  4.82it/s] 10%|▉         | 385/3949 [01:56<15:00,  3.96it/s] 10%|▉         | 388/3949 [01:56<08:52,  6.69it/s] 10%|▉         | 389/3949 [01:56<12:44,  4.65it/s] 10%|█         | 413/3949 [01:56<15:49,  3.73it/s] 10%|█         | 398/3949 [01:56<15:15,  3.88it/s] 10%|▉         | 389/3949 [01:56<08:44,  6.79it/s] 10%|▉         | 386/3949 [01:56<15:26,  3.85it/s] 10%|▉         | 390/3949 [01:56<08:47,  6.74it/s] 10%|▉         | 390/3949 [01:56<12:20,  4.80it/s] 10%|█         | 414/3949 [01:56<15:46,  3.73it/s] 10%|█         | 399/3949 [01:56<15:12,  3.89it/s] 10%|▉         | 391/3949 [01:56<09:41,  6.12it/s] 10%|▉         | 387/3949 [01:56<15:19,  3.87it/s] 10%|▉         | 391/3949 [01:56<12:25,  4.77it/s] 11%|█         | 415/3949 [01:56<15:25,  3.82it/s] 10%|█         | 400/3949 [01:56<15:08,  3.91it/s] 10%|▉         | 392/3949 [01:56<10:07,  5.86it/s] 10%|▉         | 388/3949 [01:56<14:57,  3.97it/s] 10%|▉         | 392/3949 [01:56<12:56,  4.58it/s] 10%|▉         | 393/3949 [01:57<10:07,  5.86it/s] 11%|█         | 416/3949 [01:57<15:17,  3.85it/s] 10%|█         | 401/3949 [01:57<14:46,  4.00it/s] 10%|▉         | 393/3949 [01:57<12:38,  4.69it/s] 10%|▉         | 389/3949 [01:57<15:02,  3.95it/s] 10%|▉         | 394/3949 [01:57<09:54,  5.98it/s] 11%|█         | 417/3949 [01:57<15:24,  3.82it/s] 10%|▉         | 394/3949 [01:57<12:36,  4.70it/s] 10%|█         | 402/3949 [01:57<15:30,  3.81it/s] 10%|█         | 395/3949 [01:57<09:51,  6.01it/s] 10%|▉         | 390/3949 [01:57<15:04,  3.93it/s] 10%|█         | 395/3949 [01:57<12:13,  4.84it/s] 11%|█         | 418/3949 [01:57<15:00,  3.92it/s] 10%|█         | 403/3949 [01:57<14:50,  3.98it/s] 10%|█         | 396/3949 [01:57<10:52,  5.44it/s] 10%|▉         | 391/3949 [01:57<14:39,  4.05it/s] 10%|█         | 396/3949 [01:57<12:26,  4.76it/s] 10%|█         | 397/3949 [01:57<11:07,  5.32it/s] 11%|█         | 419/3949 [01:57<14:46,  3.98it/s] 10%|█         | 404/3949 [01:57<14:39,  4.03it/s] 10%|▉         | 392/3949 [01:57<14:37,  4.06it/s] 10%|█         | 397/3949 [01:57<12:23,  4.78it/s] 10%|█         | 398/3949 [01:57<11:17,  5.24it/s] 11%|█         | 420/3949 [01:58<14:29,  4.06it/s] 10%|█         | 405/3949 [01:58<14:23,  4.10it/s] 10%|█         | 398/3949 [01:58<12:11,  4.85it/s] 10%|▉         | 393/3949 [01:58<14:37,  4.05it/s] 10%|█         | 399/3949 [01:58<11:15,  5.25it/s] 10%|█         | 406/3949 [01:58<14:28,  4.08it/s] 11%|█         | 421/3949 [01:58<14:41,  4.00it/s] 10%|█         | 400/3949 [01:58<10:48,  5.47it/s] 10%|█         | 399/3949 [01:58<12:20,  4.80it/s] 10%|▉         | 394/3949 [01:58<14:39,  4.04it/s] 10%|█         | 407/3949 [01:58<13:56,  4.24it/s] 10%|█         | 401/3949 [01:58<10:59,  5.38it/s] 11%|█         | 422/3949 [01:58<14:56,  3.93it/s] 10%|█         | 400/3949 [01:58<12:23,  4.77it/s] 10%|█         | 395/3949 [01:58<14:21,  4.13it/s] 10%|█         | 402/3949 [01:58<11:26,  5.17it/s] 10%|█         | 408/3949 [01:58<14:07,  4.18it/s] 11%|█         | 423/3949 [01:58<14:13,  4.13it/s] 10%|█         | 401/3949 [01:58<12:41,  4.66it/s] 10%|█         | 396/3949 [01:58<13:48,  4.29it/s] 10%|█         | 403/3949 [01:58<11:38,  5.08it/s] 10%|█         | 409/3949 [01:58<13:55,  4.24it/s] 11%|█         | 424/3949 [01:58<13:51,  4.24it/s] 10%|█         | 402/3949 [01:59<12:44,  4.64it/s] 10%|█         | 397/3949 [01:59<13:43,  4.31it/s] 10%|█         | 404/3949 [01:59<12:02,  4.91it/s] 10%|█         | 410/3949 [01:59<13:48,  4.27it/s] 11%|█         | 425/3949 [01:59<13:35,  4.32it/s] 10%|█         | 403/3949 [01:59<12:32,  4.71it/s] 10%|█         | 398/3949 [01:59<13:44,  4.31it/s] 10%|█         | 405/3949 [01:59<12:39,  4.66it/s] 10%|█         | 411/3949 [01:59<13:33,  4.35it/s] 11%|█         | 426/3949 [01:59<13:24,  4.38it/s] 10%|█         | 404/3949 [01:59<12:47,  4.62it/s] 10%|█         | 399/3949 [01:59<13:47,  4.29it/s] 10%|█         | 406/3949 [01:59<12:41,  4.65it/s] 10%|█         | 412/3949 [01:59<13:02,  4.52it/s] 11%|█         | 427/3949 [01:59<13:27,  4.36it/s] 10%|█         | 405/3949 [01:59<12:54,  4.57it/s] 10%|█         | 400/3949 [01:59<13:39,  4.33it/s] 10%|█         | 407/3949 [01:59<13:10,  4.48it/s] 10%|█         | 413/3949 [01:59<13:00,  4.53it/s] 11%|█         | 428/3949 [01:59<12:58,  4.52it/s] 10%|█         | 406/3949 [01:59<13:11,  4.48it/s] 10%|█         | 401/3949 [01:59<13:46,  4.29it/s] 10%|█         | 414/3949 [02:00<12:52,  4.58it/s] 10%|█         | 408/3949 [02:00<13:22,  4.41it/s] 11%|█         | 429/3949 [02:00<12:52,  4.56it/s] 10%|█         | 407/3949 [02:00<13:28,  4.38it/s] 10%|█         | 402/3949 [02:00<13:45,  4.30it/s] 11%|█         | 415/3949 [02:00<12:37,  4.67it/s] 11%|█         | 430/3949 [02:00<12:40,  4.63it/s] 10%|█         | 409/3949 [02:00<13:56,  4.23it/s]import nvidia_smi failed
True
8
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/madehua/code/FoodHealthMMLLM/tools/img_filter_food.py", line 224, in <module>
    main(device_id, threshold_lower, threshold_upper, interval_step)
  File "/ML-A800/home/guoshuyue/madehua/code/FoodHealthMMLLM/tools/img_filter_food.py", line 175, in main
    all_directories.sort()
NameError: name 'all_directories' is not defined
