nohup: 忽略输入
0,1,2,3,7
[2024-04-22 16:54:53,644] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 16:54:56,158] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-22 16:54:56,158] [INFO] [runner.py:555:main] cmd = /home/data_llm/anaconda3/envs/moellava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgN119 --master_addr=127.0.0.1 --master_port=2225 --enable_each_rank_log=None /home/data_llm/FoodHealthMMLLM/moellava/train/train_xformers.py --moe_enable True --num_experts 4 --top_k_experts 2 --capacity_factor 1.5 --moe_mode sparse --use_residual False --router_aux_loss_coef 0.01 --train_modules fc1 fc2 wg --deepspeed ../../zero2_offload.json --model_name_or_path /mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-nutv2 --version phi --data_path /mnt/data_llm/json_file/101_train_prompt1.json /mnt/data_llm/json_file/172_train_prompt1.json /mnt/data_llm/json_file/2k_train_prompt1.json --image_folder /media/LLM_data/food_recognition_dataset --image_tower openai/clip-vit-large-patch14-336 --image_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --check_point_file_name /mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-nutv2-moe-v2.json --output_dir /mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-nutv2-moe-v2 --num_train_epochs 1 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 5000 --save_total_limit 30 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 10 --tf32 False --model_max_length 512 --gradient_checkpointing True --dataloader_num_workers 16 --lazy_preprocess True --report_to tensorboard --cache_dir /media/fast_data/huggingface/hub/
[2024-04-22 16:54:57,518] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 16:55:01,825] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2024-04-22 16:55:01,825] [INFO] [launch.py:138:main] 0 NCCL_IB_TIMEOUT=22
[2024-04-22 16:55:01,825] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 7]}
[2024-04-22 16:55:01,825] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=5, node_rank=0
[2024-04-22 16:55:01,825] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4]})
[2024-04-22 16:55:01,825] [INFO] [launch.py:163:main] dist_world_size=5
[2024-04-22 16:55:01,825] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,7
[2024-04-22 16:55:05,348] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 16:55:05,353] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 16:55:05,384] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 16:55:05,384] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 16:55:05,478] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 16:55:06,368] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-22 16:55:06,369] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-04-22 16:55:06,431] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-22 16:55:06,432] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-04-22 16:55:06,440] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-22 16:55:06,441] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-04-22 16:55:06,441] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-22 16:55:06,475] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-22 16:55:06,475] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-04-22 16:55:06,550] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-22 16:55:06,550] [INFO] [comm.py:594:init_distributed] cdb=None
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.79s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]
LLM init. firstly
 MoELLaVAPhiForCausalLM(
  (model): MoELLaVAPhiModel(
    (embed_tokens): Embedding(51200, 2560, padding_idx=50295)
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-31): 32 x PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower()
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2560, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2560, out_features=2560, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2560, out_features=51200, bias=False)
)
[2024-04-22 16:55:13,525] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.96s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.88s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.85s/it]
[2024-04-22 16:55:21,645] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 16:55:29,166] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 16:55:36,329] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 16:55:45,552] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 16:55:52,777] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 16:56:01,098] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 16:56:09,594] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 16:56:17,054] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 16:56:26,151] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 16:56:33,417] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 16:56:40,860] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 16:56:50,033] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 16:56:57,263] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 16:57:06,669] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 16:57:13,941] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Vision encoder and proj init.
 MoELLaVAPhiForCausalLM(
  (model): MoELLaVAPhiModel(
    (embed_tokens): Embedding(51200, 2560, padding_idx=50295)
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (1): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (2): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (3): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (4): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (5): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (6): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (7): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (8): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (9): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (10): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (11): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (12): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (13): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (14): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (15): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (16): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (17): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (18): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (19): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (20): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (21): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (22): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (23): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (24): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (25): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (26): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (27): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (28): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (29): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (30): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (31): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2560, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2560, out_features=2560, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2560, out_features=51200, bias=False)
)
model.layers.0.mlp.deepspeed_moe.gate.wg.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.1.mlp.fc1.weight
model.layers.1.mlp.fc1.bias
model.layers.1.mlp.fc2.weight
model.layers.1.mlp.fc2.bias
model.layers.2.mlp.deepspeed_moe.gate.wg.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.3.mlp.fc1.weight
model.layers.3.mlp.fc1.bias
model.layers.3.mlp.fc2.weight
model.layers.3.mlp.fc2.bias
model.layers.4.mlp.deepspeed_moe.gate.wg.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.5.mlp.fc1.weight
model.layers.5.mlp.fc1.bias
model.layers.5.mlp.fc2.weight
model.layers.5.mlp.fc2.bias
model.layers.6.mlp.deepspeed_moe.gate.wg.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.7.mlp.fc1.weight
model.layers.7.mlp.fc1.bias
model.layers.7.mlp.fc2.weight
model.layers.7.mlp.fc2.bias
model.layers.8.mlp.deepspeed_moe.gate.wg.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.9.mlp.fc1.weight
model.layers.9.mlp.fc1.bias
model.layers.9.mlp.fc2.weight
model.layers.9.mlp.fc2.bias
model.layers.10.mlp.deepspeed_moe.gate.wg.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.11.mlp.fc1.weight
model.layers.11.mlp.fc1.bias
model.layers.11.mlp.fc2.weight
model.layers.11.mlp.fc2.bias
model.layers.12.mlp.deepspeed_moe.gate.wg.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.13.mlp.fc1.weight
model.layers.13.mlp.fc1.bias
model.layers.13.mlp.fc2.weight
model.layers.13.mlp.fc2.bias
model.layers.14.mlp.deepspeed_moe.gate.wg.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.15.mlp.fc1.weight
model.layers.15.mlp.fc1.bias
model.layers.15.mlp.fc2.weight
model.layers.15.mlp.fc2.bias
model.layers.16.mlp.deepspeed_moe.gate.wg.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.17.mlp.fc1.weight
model.layers.17.mlp.fc1.bias
model.layers.17.mlp.fc2.weight
model.layers.17.mlp.fc2.bias
model.layers.18.mlp.deepspeed_moe.gate.wg.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.19.mlp.fc1.weight
model.layers.19.mlp.fc1.bias
model.layers.19.mlp.fc2.weight
model.layers.19.mlp.fc2.bias
model.layers.20.mlp.deepspeed_moe.gate.wg.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.21.mlp.fc1.weight
model.layers.21.mlp.fc1.bias
model.layers.21.mlp.fc2.weight
model.layers.21.mlp.fc2.bias
model.layers.22.mlp.deepspeed_moe.gate.wg.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.23.mlp.fc1.weight
model.layers.23.mlp.fc1.bias
model.layers.23.mlp.fc2.weight
model.layers.23.mlp.fc2.bias
model.layers.24.mlp.deepspeed_moe.gate.wg.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.25.mlp.fc1.weight
model.layers.25.mlp.fc1.bias
model.layers.25.mlp.fc2.weight
model.layers.25.mlp.fc2.bias
model.layers.26.mlp.deepspeed_moe.gate.wg.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.27.mlp.fc1.weight
model.layers.27.mlp.fc1.bias
model.layers.27.mlp.fc2.weight
model.layers.27.mlp.fc2.bias
model.layers.28.mlp.deepspeed_moe.gate.wg.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.29.mlp.fc1.weight
model.layers.29.mlp.fc1.bias
model.layers.29.mlp.fc2.weight
model.layers.29.mlp.fc2.bias
model.layers.30.mlp.deepspeed_moe.gate.wg.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.31.mlp.fc1.weight
model.layers.31.mlp.fc1.bias
model.layers.31.mlp.fc2.weight
model.layers.31.mlp.fc2.bias
model.mm_projector.image_spatial_proj.0.weight
model.mm_projector.image_spatial_proj.0.bias
model.mm_projector.image_spatial_proj.2.weight
model.mm_projector.image_spatial_proj.2.bias
MoELLaVAPhiForCausalLM(
  (model): MoELLaVAPhiModel(
    (embed_tokens): Embedding(51200, 2560, padding_idx=50295)
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (1): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (2): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (3): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (4): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (5): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (6): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (7): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (8): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (9): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (10): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (11): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (12): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (13): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (14): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (15): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (16): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (17): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (18): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (19): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (20): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (21): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (22): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (23): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (24): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (25): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (26): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (27): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (28): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (29): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (30): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (31): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2560, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2560, out_features=2560, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2560, out_features=51200, bias=False)
)
Formatting inputs...Skip in lazy mode
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.448791027069092 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.475257635116577 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4589099884033203 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.560015916824341 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.559446334838867 seconds
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
Rank: 2 partition count [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] and sizes[(169639936, False), (41984, False), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (10485760, True), (163840, True)] 
Rank: 0 partition count [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] and sizes[(169639936, False), (41984, False), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (10485760, True), (163840, True)] 
Rank: 3 partition count [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] and sizes[(169639936, False), (41984, False), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (10485760, True), (163840, True)] 
Rank: 4 partition count [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] and sizes[(169639936, False), (41984, False), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (10485760, True), (163840, True)] 
Rank: 1 partition count [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] and sizes[(169639936, False), (41984, False), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (10485760, True), (163840, True)] 
  0%|          | 0/13423 [00:00<?, ?it/s]/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/13423 [00:55<206:36:25, 55.42s/it]  0%|          | 2/13423 [01:28<157:10:38, 42.16s/it]  0%|          | 3/13423 [02:00<140:14:36, 37.62s/it]  0%|          | 4/13423 [02:33<132:51:24, 35.64s/it]  0%|          | 5/13423 [03:05<127:50:17, 34.30s/it]  0%|          | 6/13423 [03:37<125:39:13, 33.71s/it]  0%|          | 7/13423 [04:09<123:40:19, 33.19s/it]  0%|          | 8/13423 [04:42<122:54:33, 32.98s/it]  0%|          | 9/13423 [05:14<121:39:59, 32.65s/it]  0%|          | 10/13423 [05:46<121:27:37, 32.60s/it]                                                      {'loss': 4.1457, 'learning_rate': 4.962779156327544e-07, 'epoch': 0.0}
  0%|          | 10/13423 [05:46<121:27:37, 32.60s/it]  0%|          | 11/13423 [06:18<120:26:28, 32.33s/it]  0%|          | 12/13423 [06:51<121:16:06, 32.55s/it]  0%|          | 13/13423 [07:23<120:41:44, 32.40s/it]  0%|          | 14/13423 [07:56<121:35:26, 32.64s/it]  0%|          | 15/13423 [08:28<120:56:40, 32.47s/it]  0%|          | 16/13423 [09:02<121:46:15, 32.70s/it]  0%|          | 17/13423 [09:34<121:02:19, 32.50s/it]  0%|          | 18/13423 [10:07<121:48:29, 32.71s/it]  0%|          | 19/13423 [10:39<120:57:30, 32.49s/it]  0%|          | 20/13423 [11:12<121:48:13, 32.72s/it]                                                      {'loss': 4.0084, 'learning_rate': 9.925558312655088e-07, 'epoch': 0.0}
  0%|          | 20/13423 [11:12<121:48:13, 32.72s/it]  0%|          | 21/13423 [11:44<120:57:28, 32.49s/it]  0%|          | 22/13423 [12:17<121:30:03, 32.64s/it]  0%|          | 23/13423 [12:49<120:56:55, 32.49s/it]  0%|          | 24/13423 [13:22<121:27:18, 32.63s/it]  0%|          | 25/13423 [13:54<120:40:29, 32.42s/it]  0%|          | 26/13423 [14:27<120:56:37, 32.50s/it]  0%|          | 27/13423 [14:58<120:05:54, 32.27s/it]  0%|          | 28/13423 [15:31<120:03:10, 32.27s/it]  0%|          | 29/13423 [16:02<119:33:24, 32.13s/it]  0%|          | 30/13423 [16:34<119:25:45, 32.10s/it]                                                      {'loss': 3.7716, 'learning_rate': 1.488833746898263e-06, 'epoch': 0.0}
  0%|          | 30/13423 [16:35<119:25:45, 32.10s/it]  0%|          | 31/13423 [17:07<119:23:39, 32.10s/it]  0%|          | 32/13423 [17:39<119:13:34, 32.05s/it]  0%|          | 33/13423 [18:11<119:16:16, 32.07s/it]  0%|          | 34/13423 [18:42<118:37:37, 31.90s/it]  0%|          | 35/13423 [19:14<118:51:29, 31.96s/it]  0%|          | 36/13423 [19:46<118:34:20, 31.89s/it]  0%|          | 37/13423 [20:18<118:43:43, 31.93s/it]  0%|          | 38/13423 [20:50<118:21:42, 31.83s/it]  0%|          | 39/13423 [21:22<119:23:59, 32.12s/it]  0%|          | 40/13423 [21:54<119:08:39, 32.05s/it]                                                      {'loss': 3.4903, 'learning_rate': 1.9851116625310177e-06, 'epoch': 0.0}
  0%|          | 40/13423 [21:54<119:08:39, 32.05s/it]  0%|          | 41/13423 [22:27<120:23:34, 32.39s/it]  0%|          | 42/13423 [22:59<119:44:12, 32.21s/it]  0%|          | 43/13423 [23:32<120:48:14, 32.50s/it]  0%|          | 44/13423 [24:04<120:04:15, 32.31s/it]  0%|          | 45/13423 [24:37<120:45:06, 32.49s/it]  0%|          | 46/13423 [25:09<120:06:22, 32.32s/it]  0%|          | 47/13423 [25:42<120:35:09, 32.45s/it]  0%|          | 48/13423 [26:14<119:57:05, 32.29s/it]  0%|          | 49/13423 [26:46<120:03:25, 32.32s/it]  0%|          | 50/13423 [27:18<119:27:32, 32.16s/it]                                                      {'loss': 3.1675, 'learning_rate': 2.481389578163772e-06, 'epoch': 0.0}
  0%|          | 50/13423 [27:18<119:27:32, 32.16s/it]  0%|          | 51/13423 [27:50<119:14:46, 32.10s/it]  0%|          | 52/13423 [28:22<119:09:29, 32.08s/it]  0%|          | 53/13423 [28:54<118:52:48, 32.01s/it]  0%|          | 54/13423 [29:26<118:56:44, 32.03s/it]  0%|          | 55/13423 [29:57<118:25:17, 31.89s/it]  0%|          | 56/13423 [30:30<118:42:39, 31.97s/it]  0%|          | 57/13423 [31:01<118:26:30, 31.90s/it]  0%|          | 58/13423 [31:34<118:44:54, 31.99s/it]  0%|          | 59/13423 [32:05<118:22:31, 31.89s/it]  0%|          | 60/13423 [32:38<119:23:47, 32.17s/it]                                                      {'loss': 3.1377, 'learning_rate': 2.977667493796526e-06, 'epoch': 0.0}
  0%|          | 60/13423 [32:38<119:23:47, 32.17s/it]  0%|          | 61/13423 [33:10<119:06:12, 32.09s/it]  0%|          | 62/13423 [33:43<120:09:02, 32.37s/it]  0%|          | 63/13423 [34:15<119:45:51, 32.27s/it]  0%|          | 64/13423 [34:48<120:11:08, 32.39s/it]  0%|          | 65/13423 [35:20<119:38:28, 32.24s/it]  0%|          | 66/13423 [35:52<119:35:40, 32.23s/it]  0%|          | 67/13423 [36:24<119:09:37, 32.12s/it]  1%|          | 68/13423 [36:55<118:47:57, 32.02s/it]  1%|          | 69/13423 [37:27<118:35:13, 31.97s/it]  1%|          | 70/13423 [37:59<118:21:40, 31.91s/it]                                                      {'loss': 3.0417, 'learning_rate': 3.4739454094292807e-06, 'epoch': 0.01}
  1%|          | 70/13423 [37:59<118:21:40, 31.91s/it]  1%|          | 71/13423 [38:31<118:22:01, 31.91s/it]  1%|          | 72/13423 [39:02<117:52:50, 31.79s/it]  1%|          | 73/13423 [39:34<118:04:02, 31.84s/it]  1%|          | 74/13423 [40:06<117:35:52, 31.71s/it]  1%|          | 75/13423 [40:38<117:56:43, 31.81s/it]  1%|          | 76/13423 [41:10<117:47:58, 31.77s/it]  1%|          | 77/13423 [41:43<119:10:16, 32.15s/it]  1%|          | 78/13423 [42:14<118:52:12, 32.07s/it]  1%|          | 79/13423 [42:47<119:41:04, 32.29s/it]  1%|          | 80/13423 [43:19<119:07:36, 32.14s/it]                                                      {'loss': 2.8587, 'learning_rate': 3.970223325062035e-06, 'epoch': 0.01}
  1%|          | 80/13423 [43:19<119:07:36, 32.14s/it]  1%|          | 81/13423 [43:51<119:26:32, 32.23s/it]  1%|          | 82/13423 [44:23<118:54:25, 32.09s/it]  1%|          | 83/13423 [44:55<118:43:43, 32.04s/it]  1%|          | 84/13423 [45:27<118:31:19, 31.99s/it]  1%|          | 85/13423 [45:59<118:06:35, 31.88s/it]  1%|          | 86/13423 [46:31<118:10:39, 31.90s/it]  1%|          | 87/13423 [47:02<117:32:47, 31.73s/it]  1%|          | 88/13423 [47:34<117:57:43, 31.85s/it]  1%|          | 89/13423 [48:05<117:27:08, 31.71s/it]  1%|          | 90/13423 [48:37<117:36:26, 31.75s/it]                                                      {'loss': 2.821, 'learning_rate': 4.4665012406947896e-06, 'epoch': 0.01}
  1%|          | 90/13423 [48:37<117:36:26, 31.75s/it]  1%|          | 91/13423 [49:09<117:11:32, 31.65s/it]  1%|          | 92/13423 [49:41<118:25:49, 31.98s/it]  1%|          | 93/13423 [50:13<118:17:52, 31.95s/it]  1%|          | 94/13423 [50:46<119:15:37, 32.21s/it]  1%|          | 95/13423 [51:18<118:47:34, 32.09s/it]  1%|          | 96/13423 [51:50<119:15:43, 32.22s/it]  1%|          | 97/13423 [52:22<118:51:08, 32.11s/it]  1%|          | 98/13423 [52:54<118:42:38, 32.07s/it]  1%|          | 99/13423 [53:26<118:18:34, 31.97s/it]  1%|          | 100/13423 [53:58<117:58:08, 31.88s/it]                                                       {'loss': 2.7515, 'learning_rate': 4.962779156327544e-06, 'epoch': 0.01}
  1%|          | 100/13423 [53:58<117:58:08, 31.88s/it]  1%|          | 101/13423 [54:29<117:47:19, 31.83s/it]  1%|          | 102/13423 [55:01<117:12:51, 31.68s/it]  1%|          | 103/13423 [55:33<117:18:38, 31.71s/it]  1%|          | 104/13423 [56:04<116:59:38, 31.62s/it]  1%|          | 105/13423 [56:36<117:17:15, 31.70s/it]  1%|          | 106/13423 [57:07<117:04:16, 31.65s/it]  1%|          | 107/13423 [57:40<118:23:05, 32.01s/it]  1%|          | 108/13423 [58:12<118:05:33, 31.93s/it]  1%|          | 109/13423 [58:45<119:04:57, 32.20s/it]  1%|          | 110/13423 [59:16<118:30:59, 32.05s/it]                                                       {'loss': 2.6814, 'learning_rate': 5.459057071960299e-06, 'epoch': 0.01}
  1%|          | 110/13423 [59:16<118:30:59, 32.05s/it]  1%|          | 111/13423 [59:49<118:46:20, 32.12s/it]  1%|          | 112/13423 [1:00:20<118:12:12, 31.97s/it]  1%|          | 113/13423 [1:00:52<117:39:42, 31.82s/it]  1%|          | 114/13423 [1:01:24<117:38:52, 31.82s/it]  1%|          | 115/13423 [1:01:55<117:12:41, 31.71s/it]  1%|          | 116/13423 [1:02:27<117:32:11, 31.80s/it]  1%|          | 117/13423 [1:02:58<117:00:53, 31.66s/it]  1%|          | 118/13423 [1:03:30<117:20:55, 31.75s/it]  1%|          | 119/13423 [1:04:02<117:02:42, 31.67s/it]  1%|          | 120/13423 [1:04:34<117:53:35, 31.90s/it]                                                         {'loss': 2.5142, 'learning_rate': 5.955334987593052e-06, 'epoch': 0.01}
  1%|          | 120/13423 [1:04:34<117:53:35, 31.90s/it]  1%|          | 121/13423 [1:05:06<117:50:53, 31.89s/it]  1%|          | 122/13423 [1:05:39<119:11:06, 32.26s/it]  1%|          | 123/13423 [1:06:11<118:38:20, 32.11s/it]  1%|          | 124/13423 [1:06:44<119:09:56, 32.26s/it]  1%|          | 125/13423 [1:07:15<118:24:52, 32.06s/it]  1%|          | 126/13423 [1:07:47<118:27:30, 32.07s/it]  1%|          | 127/13423 [1:08:19<117:45:05, 31.88s/it]  1%|          | 128/13423 [1:08:50<117:21:36, 31.78s/it]  1%|          | 129/13423 [1:09:22<117:05:50, 31.71s/it]  1%|          | 130/13423 [1:09:53<116:46:06, 31.62s/it]                                                         {'loss': 2.4592, 'learning_rate': 6.451612903225806e-06, 'epoch': 0.01}
  1%|          | 130/13423 [1:09:53<116:46:06, 31.62s/it]  1%|          | 131/13423 [1:10:25<117:09:57, 31.73s/it]  1%|          | 132/13423 [1:10:57<116:47:30, 31.63s/it]  1%|          | 133/13423 [1:11:29<117:01:51, 31.70s/it]  1%|          | 134/13423 [1:12:00<116:53:58, 31.67s/it]  1%|          | 135/13423 [1:12:33<118:12:52, 32.03s/it]  1%|          | 136/13423 [1:13:05<117:51:38, 31.93s/it]  1%|          | 137/13423 [1:13:37<118:43:11, 32.17s/it]  1%|          | 138/13423 [1:14:09<118:06:43, 32.01s/it]  1%|          | 139/13423 [1:14:42<118:43:23, 32.17s/it]  1%|          | 140/13423 [1:15:13<117:59:10, 31.98s/it]                                                         {'loss': 2.3971, 'learning_rate': 6.9478908188585614e-06, 'epoch': 0.01}
  1%|          | 140/13423 [1:15:13<117:59:10, 31.98s/it]  1%|          | 141/13423 [1:15:45<118:12:00, 32.04s/it]  1%|          | 142/13423 [1:16:17<117:50:42, 31.94s/it]  1%|          | 143/13423 [1:16:49<117:22:16, 31.82s/it]  1%|          | 144/13423 [1:17:20<117:17:37, 31.80s/it]  1%|          | 145/13423 [1:17:52<116:47:31, 31.67s/it]  1%|          | 146/13423 [1:18:24<116:56:37, 31.71s/it]  1%|          | 147/13423 [1:18:55<116:37:53, 31.63s/it]  1%|          | 148/13423 [1:19:27<117:05:37, 31.75s/it]  1%|          | 149/13423 [1:19:58<116:41:14, 31.65s/it]  1%|          | 150/13423 [1:20:31<118:09:58, 32.05s/it]                                                         {'loss': 2.3025, 'learning_rate': 7.444168734491316e-06, 'epoch': 0.01}
  1%|          | 150/13423 [1:20:31<118:09:58, 32.05s/it]  1%|          | 151/13423 [1:21:03<117:43:01, 31.93s/it]  1%|          | 152/13423 [1:21:36<118:25:46, 32.13s/it]  1%|          | 153/13423 [1:22:07<117:52:24, 31.98s/it]  1%|          | 154/13423 [1:22:39<117:58:15, 32.01s/it]  1%|          | 155/13423 [1:23:11<117:24:08, 31.85s/it]  1%|          | 156/13423 [1:23:43<117:10:23, 31.79s/it]  1%|          | 157/13423 [1:24:14<116:56:33, 31.73s/it]  1%|          | 158/13423 [1:24:46<116:35:35, 31.64s/it]  1%|          | 159/13423 [1:25:18<117:00:38, 31.76s/it]  1%|          | 160/13423 [1:25:49<116:32:52, 31.63s/it]                                                         {'loss': 2.2642, 'learning_rate': 7.94044665012407e-06, 'epoch': 0.01}
  1%|          | 160/13423 [1:25:49<116:32:52, 31.63s/it]  1%|          | 161/13423 [1:26:21<116:49:44, 31.71s/it]  1%|          | 162/13423 [1:26:52<116:25:29, 31.61s/it]  1%|          | 163/13423 [1:27:25<117:36:25, 31.93s/it]  1%|          | 164/13423 [1:27:57<117:17:50, 31.85s/it]  1%|          | 165/13423 [1:28:29<118:18:29, 32.12s/it]  1%|          | 166/13423 [1:29:01<117:50:54, 32.00s/it]  1%|          | 167/13423 [1:29:33<118:00:46, 32.05s/it]  1%|▏         | 168/13423 [1:30:05<117:20:03, 31.87s/it]  1%|▏         | 169/13423 [1:30:36<117:08:17, 31.82s/it]  1%|▏         | 170/13423 [1:31:08<116:50:15, 31.74s/it]                                                         {'loss': 2.1975, 'learning_rate': 8.436724565756825e-06, 'epoch': 0.01}
  1%|▏         | 170/13423 [1:31:08<116:50:15, 31.74s/it]  1%|▏         | 171/13423 [1:31:39<116:32:59, 31.66s/it]  1%|▏         | 172/13423 [1:32:11<116:52:44, 31.75s/it]  1%|▏         | 173/13423 [1:32:43<116:19:21, 31.60s/it]  1%|▏         | 174/13423 [1:33:14<116:29:59, 31.66s/it]  1%|▏         | 175/13423 [1:33:46<116:19:02, 31.61s/it]  1%|▏         | 176/13423 [1:34:18<116:55:54, 31.78s/it]  1%|▏         | 177/13423 [1:34:49<116:27:23, 31.65s/it]  1%|▏         | 178/13423 [1:35:22<117:57:47, 32.06s/it]  1%|▏         | 179/13423 [1:35:54<117:28:51, 31.93s/it]  1%|▏         | 180/13423 [1:36:27<118:22:04, 32.18s/it]                                                         {'loss': 2.1221, 'learning_rate': 8.933002481389579e-06, 'epoch': 0.01}
  1%|▏         | 180/13423 [1:36:27<118:22:04, 32.18s/it]  1%|▏         | 181/13423 [1:36:58<117:42:28, 32.00s/it]  1%|▏         | 182/13423 [1:37:31<118:22:35, 32.18s/it]  1%|▏         | 183/13423 [1:38:03<117:42:47, 32.01s/it]  1%|▏         | 184/13423 [1:38:34<117:21:30, 31.91s/it]  1%|▏         | 185/13423 [1:39:06<117:07:09, 31.85s/it]  1%|▏         | 186/13423 [1:39:37<116:42:50, 31.74s/it]  1%|▏         | 187/13423 [1:40:09<116:44:16, 31.75s/it]  1%|▏         | 188/13423 [1:40:41<116:16:20, 31.63s/it]  1%|▏         | 189/13423 [1:41:12<116:27:25, 31.68s/it]  1%|▏         | 190/13423 [1:41:44<116:18:00, 31.64s/it]                                                         {'loss': 2.1443, 'learning_rate': 9.429280397022333e-06, 'epoch': 0.01}
  1%|▏         | 190/13423 [1:41:44<116:18:00, 31.64s/it]  1%|▏         | 191/13423 [1:42:16<116:45:33, 31.77s/it]  1%|▏         | 192/13423 [1:42:48<116:31:14, 31.70s/it]  1%|▏         | 193/13423 [1:43:20<117:30:08, 31.97s/it]  1%|▏         | 194/13423 [1:43:52<117:15:37, 31.91s/it]  1%|▏         | 195/13423 [1:44:25<118:14:55, 32.18s/it]  1%|▏         | 196/13423 [1:44:56<117:36:25, 32.01s/it]  1%|▏         | 197/13423 [1:45:29<117:57:40, 32.11s/it]  1%|▏         | 198/13423 [1:46:00<117:22:42, 31.95s/it]  1%|▏         | 199/13423 [1:46:32<116:55:11, 31.83s/it]  1%|▏         | 200/13423 [1:47:04<116:54:32, 31.83s/it]                                                         {'loss': 1.9238, 'learning_rate': 9.925558312655088e-06, 'epoch': 0.01}
  1%|▏         | 200/13423 [1:47:04<116:54:32, 31.83s/it]  1%|▏         | 201/13423 [1:47:35<116:35:10, 31.74s/it]  2%|▏         | 202/13423 [1:48:07<116:47:32, 31.80s/it]  2%|▏         | 203/13423 [1:48:38<116:14:07, 31.65s/it]  2%|▏         | 204/13423 [1:49:10<116:17:29, 31.67s/it]  2%|▏         | 205/13423 [1:49:42<116:10:31, 31.64s/it]  2%|▏         | 206/13423 [1:50:14<117:09:52, 31.91s/it]  2%|▏         | 207/13423 [1:50:46<117:14:02, 31.93s/it]  2%|▏         | 208/13423 [1:51:19<118:37:00, 32.31s/it]  2%|▏         | 209/13423 [1:51:51<118:05:53, 32.17s/it]  2%|▏         | 210/13423 [1:52:24<118:41:46, 32.34s/it]                                                         {'loss': 1.8881, 'learning_rate': 1.0421836228287843e-05, 'epoch': 0.02}
  2%|▏         | 210/13423 [1:52:24<118:41:46, 32.34s/it]  2%|▏         | 211/13423 [1:52:56<117:53:04, 32.12s/it]  2%|▏         | 212/13423 [1:53:28<118:09:30, 32.20s/it]  2%|▏         | 213/13423 [1:53:59<117:22:21, 31.99s/it]  2%|▏         | 214/13423 [1:54:31<117:14:18, 31.95s/it]  2%|▏         | 215/13423 [1:55:03<116:57:13, 31.88s/it]  2%|▏         | 216/13423 [1:55:35<116:36:21, 31.78s/it]  2%|▏         | 217/13423 [1:56:06<116:34:13, 31.78s/it]  2%|▏         | 218/13423 [1:56:38<116:06:47, 31.66s/it]  2%|▏         | 219/13423 [1:57:10<116:20:11, 31.72s/it]  2%|▏         | 220/13423 [1:57:41<116:06:01, 31.66s/it]                                                         {'loss': 1.8586, 'learning_rate': 1.0918114143920598e-05, 'epoch': 0.02}
  2%|▏         | 220/13423 [1:57:41<116:06:01, 31.66s/it]  2%|▏         | 221/13423 [1:58:13<116:22:46, 31.74s/it]  2%|▏         | 222/13423 [1:58:44<115:54:12, 31.61s/it]  2%|▏         | 223/13423 [1:59:17<117:28:35, 32.04s/it]  2%|▏         | 224/13423 [1:59:49<117:10:28, 31.96s/it]  2%|▏         | 225/13423 [2:00:22<118:02:45, 32.20s/it]  2%|▏         | 226/13423 [2:00:54<117:30:37, 32.06s/it]  2%|▏         | 227/13423 [2:01:26<117:21:26, 32.02s/it]  2%|▏         | 228/13423 [2:01:57<116:56:53, 31.91s/it]  2%|▏         | 229/13423 [2:02:29<116:30:19, 31.79s/it]  2%|▏         | 230/13423 [2:03:00<116:14:06, 31.72s/it]                                                         {'loss': 1.7491, 'learning_rate': 1.141439205955335e-05, 'epoch': 0.02}
  2%|▏         | 230/13423 [2:03:00<116:14:06, 31.72s/it]  2%|▏         | 231/13423 [2:03:32<115:52:53, 31.62s/it]  2%|▏         | 232/13423 [2:04:04<116:11:42, 31.71s/it]  2%|▏         | 233/13423 [2:04:35<115:46:33, 31.60s/it]  2%|▏         | 234/13423 [2:05:07<116:17:46, 31.74s/it]  2%|▏         | 235/13423 [2:05:38<115:59:41, 31.66s/it]  2%|▏         | 236/13423 [2:06:11<117:24:01, 32.05s/it]  2%|▏         | 237/13423 [2:06:43<116:54:06, 31.92s/it]  2%|▏         | 238/13423 [2:07:16<117:37:21, 32.12s/it]  2%|▏         | 239/13423 [2:07:47<117:01:02, 31.95s/it]  2%|▏         | 240/13423 [2:08:19<116:37:35, 31.85s/it]                                                         {'loss': 1.7396, 'learning_rate': 1.1910669975186104e-05, 'epoch': 0.02}
  2%|▏         | 240/13423 [2:08:19<116:37:35, 31.85s/it]  2%|▏         | 241/13423 [2:08:50<116:23:25, 31.79s/it]  2%|▏         | 242/13423 [2:09:22<116:13:21, 31.74s/it]  2%|▏         | 243/13423 [2:09:54<116:09:09, 31.73s/it]  2%|▏         | 244/13423 [2:10:25<115:38:20, 31.59s/it]  2%|▏         | 245/13423 [2:10:57<115:46:24, 31.63s/it]  2%|▏         | 246/13423 [2:11:28<115:36:27, 31.58s/it]  2%|▏         | 247/13423 [2:12:01<116:31:23, 31.84s/it]  2%|▏         | 248/13423 [2:12:32<116:25:33, 31.81s/it]  2%|▏         | 249/13423 [2:13:06<117:47:48, 32.19s/it]  2%|▏         | 250/13423 [2:13:37<117:12:20, 32.03s/it]                                                         {'loss': 1.683, 'learning_rate': 1.2406947890818859e-05, 'epoch': 0.02}
  2%|▏         | 250/13423 [2:13:37<117:12:20, 32.03s/it]  2%|▏         | 251/13423 [2:14:10<117:57:28, 32.24s/it]  2%|▏         | 252/13423 [2:14:42<117:22:40, 32.08s/it]  2%|▏         | 253/13423 [2:15:14<117:46:06, 32.19s/it]  2%|▏         | 254/13423 [2:15:46<117:02:33, 32.00s/it]  2%|▏         | 255/13423 [2:16:17<116:43:08, 31.91s/it]  2%|▏         | 256/13423 [2:16:49<116:16:12, 31.79s/it]  2%|▏         | 257/13423 [2:17:20<116:02:58, 31.73s/it]  2%|▏         | 258/13423 [2:17:52<116:09:03, 31.76s/it]  2%|▏         | 259/13423 [2:18:23<115:34:51, 31.61s/it]  2%|▏         | 260/13423 [2:18:55<115:38:32, 31.63s/it]                                                         {'loss': 1.6374, 'learning_rate': 1.2903225806451613e-05, 'epoch': 0.02}
  2%|▏         | 260/13423 [2:18:55<115:38:32, 31.63s/it]  2%|▏         | 261/13423 [2:19:27<115:31:05, 31.60s/it]  2%|▏         | 262/13423 [2:19:59<116:40:47, 31.92s/it]  2%|▏         | 263/13423 [2:20:31<116:28:35, 31.86s/it]  2%|▏         | 264/13423 [2:21:04<117:38:13, 32.18s/it]  2%|▏         | 265/13423 [2:21:36<117:11:07, 32.06s/it]  2%|▏         | 266/13423 [2:22:08<117:51:55, 32.25s/it]  2%|▏         | 267/13423 [2:22:40<117:07:28, 32.05s/it]  2%|▏         | 268/13423 [2:23:12<116:43:19, 31.94s/it]  2%|▏         | 269/13423 [2:23:43<116:18:45, 31.83s/it]  2%|▏         | 270/13423 [2:24:15<115:59:40, 31.75s/it]                                                         {'loss': 1.6296, 'learning_rate': 1.3399503722084369e-05, 'epoch': 0.02}
  2%|▏         | 270/13423 [2:24:15<115:59:40, 31.75s/it]  2%|▏         | 271/13423 [2:24:47<116:15:56, 31.82s/it]  2%|▏         | 272/13423 [2:25:18<115:34:50, 31.64s/it]  2%|▏         | 273/13423 [2:25:50<115:49:46, 31.71s/it]  2%|▏         | 274/13423 [2:26:21<115:33:30, 31.64s/it]  2%|▏         | 275/13423 [2:26:54<116:40:40, 31.95s/it]  2%|▏         | 276/13423 [2:27:26<116:21:39, 31.86s/it]  2%|▏         | 277/13423 [2:27:59<117:31:16, 32.18s/it]  2%|▏         | 278/13423 [2:28:30<116:56:37, 32.03s/it]  2%|▏         | 279/13423 [2:29:03<117:23:23, 32.15s/it]  2%|▏         | 280/13423 [2:29:34<116:38:46, 31.95s/it]                                                         {'loss': 1.5388, 'learning_rate': 1.3895781637717123e-05, 'epoch': 0.02}
  2%|▏         | 280/13423 [2:29:34<116:38:46, 31.95s/it]  2%|▏         | 281/13423 [2:30:06<116:31:01, 31.92s/it]  2%|▏         | 282/13423 [2:30:38<116:12:45, 31.84s/it]  2%|▏         | 283/13423 [2:31:09<115:47:48, 31.73s/it]  2%|▏         | 284/13423 [2:31:41<116:00:18, 31.78s/it]  2%|▏         | 285/13423 [2:32:12<115:29:33, 31.65s/it]  2%|▏         | 286/13423 [2:32:44<115:45:58, 31.72s/it]  2%|▏         | 287/13423 [2:33:16<115:29:01, 31.65s/it]  2%|▏         | 288/13423 [2:33:49<116:37:27, 31.96s/it]  2%|▏         | 289/13423 [2:34:20<116:17:27, 31.88s/it]  2%|▏         | 290/13423 [2:34:53<117:15:32, 32.14s/it]                                                         {'loss': 1.5087, 'learning_rate': 1.4392059553349877e-05, 'epoch': 0.02}
  2%|▏         | 290/13423 [2:34:53<117:15:32, 32.14s/it]  2%|▏         | 291/13423 [2:35:25<116:37:43, 31.97s/it]  2%|▏         | 292/13423 [2:35:56<116:33:36, 31.96s/it]  2%|▏         | 293/13423 [2:36:28<116:10:59, 31.86s/it]  2%|▏         | 294/13423 [2:37:00<116:00:36, 31.81s/it]  2%|▏         | 295/13423 [2:37:31<115:44:33, 31.74s/it]  2%|▏         | 296/13423 [2:38:03<115:27:27, 31.66s/it]  2%|▏         | 297/13423 [2:38:35<115:41:14, 31.73s/it]  2%|▏         | 298/13423 [2:39:06<115:17:08, 31.62s/it]  2%|▏         | 299/13423 [2:39:38<115:47:39, 31.76s/it]  2%|▏         | 300/13423 [2:40:10<115:30:27, 31.69s/it]                                                         {'loss': 1.4655, 'learning_rate': 1.4888337468982631e-05, 'epoch': 0.02}
  2%|▏         | 300/13423 [2:40:10<115:30:27, 31.69s/it]  2%|▏         | 301/13423 [2:40:43<116:46:21, 32.04s/it]  2%|▏         | 302/13423 [2:41:14<116:26:18, 31.95s/it]  2%|▏         | 303/13423 [2:41:47<117:32:55, 32.25s/it]  2%|▏         | 304/13423 [2:42:19<116:46:19, 32.04s/it]  2%|▏         | 305/13423 [2:42:51<117:20:19, 32.20s/it]  2%|▏         | 306/13423 [2:43:23<116:33:06, 31.99s/it]  2%|▏         | 307/13423 [2:43:55<116:07:00, 31.87s/it]  2%|▏         | 308/13423 [2:44:26<115:53:30, 31.81s/it]  2%|▏         | 309/13423 [2:44:58<115:33:03, 31.72s/it]  2%|▏         | 310/13423 [2:45:30<115:59:07, 31.84s/it]                                                         {'loss': 1.4291, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.02}
  2%|▏         | 310/13423 [2:45:30<115:59:07, 31.84s/it]  2%|▏         | 311/13423 [2:46:01<115:23:26, 31.68s/it]  2%|▏         | 312/13423 [2:46:33<115:45:49, 31.79s/it]  2%|▏         | 313/13423 [2:47:04<115:16:21, 31.65s/it]  2%|▏         | 314/13423 [2:47:37<116:03:22, 31.87s/it]  2%|▏         | 315/13423 [2:48:09<115:48:38, 31.81s/it]  2%|▏         | 316/13423 [2:48:42<117:05:56, 32.16s/it]  2%|▏         | 317/13423 [2:49:13<116:26:22, 31.98s/it]  2%|▏         | 318/13423 [2:49:46<117:12:23, 32.20s/it]  2%|▏         | 319/13423 [2:50:17<116:33:11, 32.02s/it]  2%|▏         | 320/13423 [2:50:49<116:24:49, 31.98s/it]                                                         {'loss': 1.3562, 'learning_rate': 1.588089330024814e-05, 'epoch': 0.02}
  2%|▏         | 320/13423 [2:50:49<116:24:49, 31.98s/it]  2%|▏         | 321/13423 [2:51:21<116:01:05, 31.88s/it]  2%|▏         | 322/13423 [2:51:52<115:40:35, 31.79s/it]  2%|▏         | 323/13423 [2:52:24<115:28:55, 31.74s/it]  2%|▏         | 324/13423 [2:52:56<115:08:58, 31.65s/it]  2%|▏         | 325/13423 [2:53:28<115:28:30, 31.74s/it]  2%|▏         | 326/13423 [2:53:59<115:05:59, 31.64s/it]  2%|▏         | 327/13423 [2:54:31<115:26:30, 31.73s/it]  2%|▏         | 328/13423 [2:55:02<114:55:39, 31.60s/it]  2%|▏         | 329/13423 [2:55:35<116:22:38, 32.00s/it]  2%|▏         | 330/13423 [2:56:07<115:56:31, 31.88s/it]                                                         {'loss': 1.377, 'learning_rate': 1.6377171215880892e-05, 'epoch': 0.02}
  2%|▏         | 330/13423 [2:56:07<115:56:31, 31.88s/it]  2%|▏         | 331/13423 [2:56:40<117:02:18, 32.18s/it]  2%|▏         | 332/13423 [2:57:11<116:23:02, 32.01s/it]  2%|▏         | 333/13423 [2:57:43<116:36:06, 32.07s/it]  2%|▏         | 334/13423 [2:58:15<116:08:25, 31.94s/it]  2%|▏         | 335/13423 [2:58:47<115:41:19, 31.82s/it]  3%|▎         | 336/13423 [2:59:18<115:31:44, 31.78s/it]  3%|▎         | 337/13423 [2:59:49<114:55:16, 31.62s/it]  3%|▎         | 338/13423 [3:00:21<115:14:19, 31.70s/it]  3%|▎         | 339/13423 [3:00:53<114:55:32, 31.62s/it]  3%|▎         | 340/13423 [3:01:25<115:32:54, 31.80s/it]                                                         {'loss': 1.3402, 'learning_rate': 1.687344913151365e-05, 'epoch': 0.03}
  3%|▎         | 340/13423 [3:01:25<115:32:54, 31.80s/it]  3%|▎         | 341/13423 [3:01:57<115:13:38, 31.71s/it]  3%|▎         | 342/13423 [3:02:30<116:51:20, 32.16s/it]  3%|▎         | 343/13423 [3:03:01<116:05:52, 31.95s/it]  3%|▎         | 344/13423 [3:03:34<116:56:04, 32.19s/it]  3%|▎         | 345/13423 [3:04:06<116:18:26, 32.02s/it]  3%|▎         | 346/13423 [3:04:38<116:29:48, 32.07s/it]  3%|▎         | 347/13423 [3:05:09<115:58:52, 31.93s/it]  3%|▎         | 348/13423 [3:05:41<115:36:21, 31.83s/it]  3%|▎         | 349/13423 [3:06:13<115:18:23, 31.75s/it]  3%|▎         | 350/13423 [3:06:44<115:03:37, 31.68s/it]                                                         {'loss': 1.3442, 'learning_rate': 1.7369727047146404e-05, 'epoch': 0.03}
  3%|▎         | 350/13423 [3:06:44<115:03:37, 31.68s/it]  3%|▎         | 351/13423 [3:07:16<115:22:03, 31.77s/it]  3%|▎         | 352/13423 [3:07:47<114:54:35, 31.65s/it]  3%|▎         | 353/13423 [3:08:19<115:09:12, 31.72s/it]  3%|▎         | 354/13423 [3:08:51<114:54:12, 31.65s/it]  3%|▎         | 355/13423 [3:09:23<115:43:59, 31.88s/it]  3%|▎         | 356/13423 [3:09:55<115:42:19, 31.88s/it]  3%|▎         | 357/13423 [3:10:28<116:56:57, 32.22s/it]  3%|▎         | 358/13423 [3:11:00<116:17:41, 32.04s/it]  3%|▎         | 359/13423 [3:11:32<116:53:45, 32.21s/it]  3%|▎         | 360/13423 [3:12:04<116:02:30, 31.98s/it]                                                         {'loss': 1.3098, 'learning_rate': 1.7866004962779158e-05, 'epoch': 0.03}
  3%|▎         | 360/13423 [3:12:04<116:02:30, 31.98s/it]  3%|▎         | 361/13423 [3:12:36<116:01:53, 31.98s/it]  3%|▎         | 362/13423 [3:13:07<115:30:14, 31.84s/it]  3%|▎         | 363/13423 [3:13:39<115:13:14, 31.76s/it]  3%|▎         | 364/13423 [3:14:10<115:01:40, 31.71s/it]  3%|▎         | 365/13423 [3:14:42<114:25:01, 31.54s/it]  3%|▎         | 366/13423 [3:15:14<114:51:33, 31.67s/it]  3%|▎         | 367/13423 [3:15:45<114:44:20, 31.64s/it]  3%|▎         | 368/13423 [3:16:17<115:21:13, 31.81s/it]  3%|▎         | 369/13423 [3:16:49<115:05:38, 31.74s/it]  3%|▎         | 370/13423 [3:17:22<116:35:12, 32.15s/it]                                                         {'loss': 1.2713, 'learning_rate': 1.8362282878411912e-05, 'epoch': 0.03}
  3%|▎         | 370/13423 [3:17:22<116:35:12, 32.15s/it]  3%|▎         | 371/13423 [3:17:54<115:52:30, 31.96s/it]  3%|▎         | 372/13423 [3:18:26<116:48:00, 32.22s/it]  3%|▎         | 373/13423 [3:18:58<116:10:42, 32.05s/it]  3%|▎         | 374/13423 [3:19:30<116:14:14, 32.07s/it]  3%|▎         | 375/13423 [3:20:02<115:36:59, 31.90s/it]  3%|▎         | 376/13423 [3:20:33<115:20:16, 31.82s/it]  3%|▎         | 377/13423 [3:21:05<115:08:24, 31.77s/it]  3%|▎         | 378/13423 [3:21:36<114:46:03, 31.67s/it]  3%|▎         | 379/13423 [3:22:08<115:07:26, 31.77s/it]  3%|▎         | 380/13423 [3:22:40<114:42:30, 31.66s/it]                                                         {'loss': 1.267, 'learning_rate': 1.8858560794044667e-05, 'epoch': 0.03}
  3%|▎         | 380/13423 [3:22:40<114:42:30, 31.66s/it]  3%|▎         | 381/13423 [3:23:12<115:02:22, 31.75s/it]  3%|▎         | 382/13423 [3:23:43<114:53:50, 31.72s/it]  3%|▎         | 383/13423 [3:24:16<116:17:00, 32.10s/it]  3%|▎         | 384/13423 [3:24:48<116:01:41, 32.03s/it]  3%|▎         | 385/13423 [3:25:21<117:14:59, 32.37s/it]  3%|▎         | 386/13423 [3:25:53<116:40:59, 32.22s/it]  3%|▎         | 387/13423 [3:26:26<117:14:27, 32.38s/it]  3%|▎         | 388/13423 [3:26:58<116:24:09, 32.15s/it]  3%|▎         | 389/13423 [3:27:30<116:23:36, 32.15s/it]  3%|▎         | 390/13423 [3:28:01<115:53:19, 32.01s/it]                                                         {'loss': 1.2519, 'learning_rate': 1.935483870967742e-05, 'epoch': 0.03}
  3%|▎         | 390/13423 [3:28:01<115:53:19, 32.01s/it]  3%|▎         | 391/13423 [3:28:33<115:39:41, 31.95s/it]  3%|▎         | 392/13423 [3:29:05<115:27:34, 31.90s/it]  3%|▎         | 393/13423 [3:29:37<115:14:20, 31.84s/it]  3%|▎         | 394/13423 [3:30:09<115:15:01, 31.84s/it]  3%|▎         | 395/13423 [3:30:40<114:40:21, 31.69s/it]  3%|▎         | 396/13423 [3:31:12<114:51:39, 31.74s/it]  3%|▎         | 397/13423 [3:31:43<114:43:55, 31.71s/it]  3%|▎         | 398/13423 [3:32:16<115:22:50, 31.89s/it]  3%|▎         | 399/13423 [3:32:47<115:03:47, 31.80s/it]  3%|▎         | 400/13423 [3:33:20<116:14:34, 32.13s/it]                                                         {'loss': 1.1506, 'learning_rate': 1.9851116625310175e-05, 'epoch': 0.03}
  3%|▎         | 400/13423 [3:33:20<116:14:34, 32.13s/it]  3%|▎         | 401/13423 [3:33:52<115:45:08, 32.00s/it]  3%|▎         | 402/13423 [3:34:25<116:41:18, 32.26s/it]  3%|▎         | 403/13423 [3:34:56<116:02:36, 32.09s/it]  3%|▎         | 404/13423 [3:35:29<116:27:23, 32.20s/it]  3%|▎         | 405/13423 [3:36:00<115:41:14, 31.99s/it]  3%|▎         | 406/13423 [3:36:32<115:24:43, 31.92s/it]  3%|▎         | 407/13423 [3:37:04<115:01:06, 31.81s/it]  3%|▎         | 408/13423 [3:37:35<114:50:55, 31.77s/it]  3%|▎         | 409/13423 [3:38:07<114:54:50, 31.79s/it]  3%|▎         | 410/13423 [3:38:39<114:31:02, 31.68s/it]                                                         {'loss': 1.1687, 'learning_rate': 1.999998573592026e-05, 'epoch': 0.03}
  3%|▎         | 410/13423 [3:38:39<114:31:02, 31.68s/it]  3%|▎         | 411/13423 [3:39:10<114:36:31, 31.71s/it]  3%|▎         | 412/13423 [3:39:42<114:25:21, 31.66s/it]  3%|▎         | 413/13423 [3:40:15<115:27:45, 31.95s/it]  3%|▎         | 414/13423 [3:40:46<114:58:33, 31.82s/it]  3%|▎         | 415/13423 [3:41:19<116:28:40, 32.24s/it]  3%|▎         | 416/13423 [3:41:51<115:56:34, 32.09s/it]  3%|▎         | 417/13423 [3:42:24<116:57:56, 32.38s/it]  3%|▎         | 418/13423 [3:42:56<116:18:01, 32.19s/it]  3%|▎         | 419/13423 [3:43:29<116:45:16, 32.32s/it]  3%|▎         | 420/13423 [3:44:00<115:54:01, 32.09s/it]                                                         {'loss': 1.1332, 'learning_rate': 1.9999915871137854e-05, 'epoch': 0.03}
  3%|▎         | 420/13423 [3:44:00<115:54:01, 32.09s/it]  3%|▎         | 421/13423 [3:44:32<115:20:40, 31.94s/it]  3%|▎         | 422/13423 [3:45:03<115:05:23, 31.87s/it]  3%|▎         | 423/13423 [3:45:35<114:35:02, 31.73s/it]  3%|▎         | 424/13423 [3:46:07<114:54:12, 31.82s/it]  3%|▎         | 425/13423 [3:46:38<114:24:22, 31.69s/it]  3%|▎         | 426/13423 [3:47:10<114:34:53, 31.74s/it]  3%|▎         | 427/13423 [3:47:41<114:12:08, 31.63s/it]  3%|▎         | 428/13423 [3:48:14<115:41:55, 32.05s/it]  3%|▎         | 429/13423 [3:48:46<115:10:33, 31.91s/it]  3%|▎         | 430/13423 [3:49:19<116:04:09, 32.16s/it]                                                         {'loss': 1.1177, 'learning_rate': 1.9999787786126022e-05, 'epoch': 0.03}
  3%|▎         | 430/13423 [3:49:19<116:04:09, 32.16s/it]  3%|▎         | 431/13423 [3:49:50<115:28:39, 32.00s/it]  3%|▎         | 432/13423 [3:50:23<115:44:50, 32.08s/it]  3%|▎         | 433/13423 [3:50:54<115:03:28, 31.89s/it]  3%|▎         | 434/13423 [3:51:26<114:40:51, 31.78s/it]  3%|▎         | 435/13423 [3:51:57<114:28:55, 31.73s/it]  3%|▎         | 436/13423 [3:52:29<113:58:50, 31.60s/it]  3%|▎         | 437/13423 [3:53:00<114:16:24, 31.68s/it]  3%|▎         | 438/13423 [3:53:32<113:52:21, 31.57s/it]  3%|▎         | 439/13423 [3:54:04<114:13:45, 31.67s/it]  3%|▎         | 440/13423 [3:54:35<114:01:52, 31.62s/it]                                                         {'loss': 1.1217, 'learning_rate': 1.999960148163048e-05, 'epoch': 0.03}
  3%|▎         | 440/13423 [3:54:35<114:01:52, 31.62s/it]  3%|▎         | 441/13423 [3:55:08<115:19:08, 31.98s/it]  3%|▎         | 442/13423 [3:55:40<115:02:45, 31.91s/it]  3%|▎         | 443/13423 [3:56:12<115:43:27, 32.10s/it]  3%|▎         | 444/13423 [3:56:44<115:09:23, 31.94s/it]  3%|▎         | 445/13423 [3:57:16<114:56:11, 31.88s/it]  3%|▎         | 446/13423 [3:57:47<114:33:55, 31.78s/it]  3%|▎         | 447/13423 [3:58:19<114:21:01, 31.72s/it]  3%|▎         | 448/13423 [3:58:51<114:32:21, 31.78s/it]  3%|▎         | 449/13423 [3:59:22<114:05:14, 31.66s/it]  3%|▎         | 450/13423 [3:59:54<114:19:57, 31.73s/it]                                                         {'loss': 1.1529, 'learning_rate': 1.999935695873591e-05, 'epoch': 0.03}
  3%|▎         | 450/13423 [3:59:54<114:19:57, 31.73s/it]  3%|▎         | 451/13423 [4:00:25<113:58:02, 31.63s/it]  3%|▎         | 452/13423 [4:00:58<114:43:02, 31.84s/it]  3%|▎         | 453/13423 [4:01:29<114:31:12, 31.79s/it]  3%|▎         | 454/13423 [4:02:02<115:29:11, 32.06s/it]  3%|▎         | 455/13423 [4:02:34<115:12:18, 31.98s/it]  3%|▎         | 456/13423 [4:03:07<116:16:08, 32.28s/it]  3%|▎         | 457/13423 [4:03:38<115:21:43, 32.03s/it]  3%|▎         | 458/13423 [4:04:10<115:39:06, 32.11s/it]  3%|▎         | 459/13423 [4:04:42<115:00:19, 31.94s/it]  3%|▎         | 460/13423 [4:05:14<114:47:39, 31.88s/it]                                                         {'loss': 1.0989, 'learning_rate': 1.9999054218865935e-05, 'epoch': 0.03}
  3%|▎         | 460/13423 [4:05:14<114:47:39, 31.88s/it]  3%|▎         | 461/13423 [4:05:45<114:29:45, 31.80s/it]  3%|▎         | 462/13423 [4:06:17<114:18:27, 31.75s/it]  3%|▎         | 463/13423 [4:06:49<114:18:35, 31.75s/it]  3%|▎         | 464/13423 [4:07:20<113:52:08, 31.63s/it]  3%|▎         | 465/13423 [4:07:52<114:06:26, 31.70s/it]  3%|▎         | 466/13423 [4:08:23<113:44:24, 31.60s/it]  3%|▎         | 467/13423 [4:08:55<113:58:30, 31.67s/it]  3%|▎         | 468/13423 [4:09:26<113:30:00, 31.54s/it]  3%|▎         | 469/13423 [4:09:59<114:44:44, 31.89s/it]  4%|▎         | 470/13423 [4:10:31<114:20:34, 31.78s/it]                                                         {'loss': 1.0805, 'learning_rate': 1.9998693263783132e-05, 'epoch': 0.04}
  4%|▎         | 470/13423 [4:10:31<114:20:34, 31.78s/it]  4%|▎         | 471/13423 [4:11:03<115:09:31, 32.01s/it]  4%|▎         | 472/13423 [4:11:35<114:44:59, 31.90s/it]  4%|▎         | 473/13423 [4:12:07<114:52:10, 31.93s/it]  4%|▎         | 474/13423 [4:12:38<114:18:28, 31.78s/it]  4%|▎         | 475/13423 [4:13:10<113:57:06, 31.68s/it]  4%|▎         | 476/13423 [4:13:41<114:03:05, 31.71s/it]  4%|▎         | 477/13423 [4:14:13<113:37:21, 31.60s/it]  4%|▎         | 478/13423 [4:14:45<114:01:06, 31.71s/it]  4%|▎         | 479/13423 [4:15:16<113:31:56, 31.58s/it]  4%|▎         | 480/13423 [4:15:48<113:38:08, 31.61s/it]                                                         {'loss': 1.0621, 'learning_rate': 1.9998274095589013e-05, 'epoch': 0.04}
  4%|▎         | 480/13423 [4:15:48<113:38:08, 31.61s/it]  4%|▎         | 481/13423 [4:16:19<113:30:07, 31.57s/it]  4%|▎         | 482/13423 [4:16:52<114:34:38, 31.87s/it]  4%|▎         | 483/13423 [4:17:24<114:32:11, 31.86s/it]  4%|▎         | 484/13423 [4:17:57<115:40:32, 32.18s/it]  4%|▎         | 485/13423 [4:18:28<115:04:15, 32.02s/it]  4%|▎         | 486/13423 [4:19:01<115:26:09, 32.12s/it]  4%|▎         | 487/13423 [4:19:32<114:39:52, 31.91s/it]  4%|▎         | 488/13423 [4:20:04<114:35:04, 31.89s/it]  4%|▎         | 489/13423 [4:20:35<114:04:35, 31.75s/it]  4%|▎         | 490/13423 [4:21:07<113:45:01, 31.66s/it]                                                         {'loss': 1.0247, 'learning_rate': 1.9997796716724e-05, 'epoch': 0.04}
  4%|▎         | 490/13423 [4:21:07<113:45:01, 31.66s/it]  4%|▎         | 491/13423 [4:21:38<113:43:35, 31.66s/it]  4%|▎         | 492/13423 [4:22:10<113:13:32, 31.52s/it]  4%|▎         | 493/13423 [4:22:41<113:31:38, 31.61s/it]  4%|▎         | 494/13423 [4:23:13<113:15:04, 31.53s/it]  4%|▎         | 495/13423 [4:23:45<113:55:49, 31.73s/it]  4%|▎         | 496/13423 [4:24:16<113:42:57, 31.67s/it]  4%|▎         | 497/13423 [4:24:49<115:03:14, 32.04s/it]  4%|▎         | 498/13423 [4:25:21<114:26:50, 31.88s/it]  4%|▎         | 499/13423 [4:25:53<115:13:51, 32.10s/it]  4%|▎         | 500/13423 [4:26:25<114:37:35, 31.93s/it]                                                         {'loss': 1.0054, 'learning_rate': 1.9997261129967427e-05, 'epoch': 0.04}
  4%|▎         | 500/13423 [4:26:25<114:37:35, 31.93s/it]  4%|▎         | 501/13423 [4:26:57<114:46:16, 31.97s/it]  4%|▎         | 502/13423 [4:27:28<114:04:22, 31.78s/it]  4%|▎         | 503/13423 [4:28:00<113:49:53, 31.72s/it]  4%|▍         | 504/13423 [4:28:31<113:28:40, 31.62s/it]  4%|▍         | 505/13423 [4:29:03<112:56:44, 31.48s/it]  4%|▍         | 506/13423 [4:29:34<113:15:54, 31.57s/it]  4%|▍         | 507/13423 [4:30:06<112:54:35, 31.47s/it]  4%|▍         | 508/13423 [4:30:38<113:30:54, 31.64s/it]  4%|▍         | 509/13423 [4:31:09<113:11:20, 31.55s/it]  4%|▍         | 510/13423 [4:31:42<114:44:08, 31.99s/it]                                                         {'loss': 0.9898, 'learning_rate': 1.999666733843752e-05, 'epoch': 0.04}
  4%|▍         | 510/13423 [4:31:42<114:44:08, 31.99s/it]  4%|▍         | 511/13423 [4:32:14<114:18:11, 31.87s/it]  4%|▍         | 512/13423 [4:32:46<115:00:06, 32.07s/it]  4%|▍         | 513/13423 [4:33:18<114:31:25, 31.94s/it]  4%|▍         | 514/13423 [4:33:49<114:12:30, 31.85s/it]  4%|▍         | 515/13423 [4:34:21<113:48:44, 31.74s/it]  4%|▍         | 516/13423 [4:34:52<113:26:37, 31.64s/it]  4%|▍         | 517/13423 [4:35:24<113:12:26, 31.58s/it]  4%|▍         | 518/13423 [4:35:55<112:46:10, 31.46s/it]  4%|▍         | 519/13423 [4:36:27<113:01:34, 31.53s/it]  4%|▍         | 520/13423 [4:36:58<112:44:57, 31.46s/it]                                                         {'loss': 0.9831, 'learning_rate': 1.9996015345591374e-05, 'epoch': 0.04}
  4%|▍         | 520/13423 [4:36:58<112:44:57, 31.46s/it]  4%|▍         | 521/13423 [4:37:30<113:39:18, 31.71s/it]  4%|▍         | 522/13423 [4:38:02<113:25:21, 31.65s/it]  4%|▍         | 523/13423 [4:38:35<114:49:18, 32.04s/it]  4%|▍         | 524/13423 [4:39:06<114:15:50, 31.89s/it]  4%|▍         | 525/13423 [4:39:39<114:57:24, 32.09s/it]  4%|▍         | 526/13423 [4:40:10<114:15:06, 31.89s/it]  4%|▍         | 527/13423 [4:40:42<114:22:56, 31.93s/it]  4%|▍         | 528/13423 [4:41:13<113:45:52, 31.76s/it]  4%|▍         | 529/13423 [4:41:45<113:19:24, 31.64s/it]  4%|▍         | 530/13423 [4:42:16<113:14:28, 31.62s/it]                                                         {'loss': 0.9622, 'learning_rate': 1.9995305155224942e-05, 'epoch': 0.04}
  4%|▍         | 530/13423 [4:42:16<113:14:28, 31.62s/it]  4%|▍         | 531/13423 [4:42:48<112:52:50, 31.52s/it]  4%|▍         | 532/13423 [4:43:19<113:04:27, 31.58s/it]  4%|▍         | 533/13423 [4:43:51<112:48:36, 31.51s/it]  4%|▍         | 534/13423 [4:44:23<113:03:24, 31.58s/it]  4%|▍         | 535/13423 [4:44:54<112:43:15, 31.49s/it]  4%|▍         | 536/13423 [4:45:26<114:00:05, 31.85s/it]  4%|▍         | 537/13423 [4:45:58<113:52:19, 31.81s/it]  4%|▍         | 538/13423 [4:46:31<114:49:45, 32.08s/it]  4%|▍         | 539/13423 [4:47:03<114:21:26, 31.95s/it]  4%|▍         | 540/13423 [4:47:35<114:55:20, 32.11s/it]                                                         {'loss': 0.9379, 'learning_rate': 1.9994536771473002e-05, 'epoch': 0.04}
  4%|▍         | 540/13423 [4:47:35<114:55:20, 32.11s/it]  4%|▍         | 541/13423 [4:48:06<114:11:43, 31.91s/it]  4%|▍         | 542/13423 [4:48:38<113:58:47, 31.86s/it]  4%|▍         | 543/13423 [4:49:10<113:32:15, 31.73s/it]  4%|▍         | 544/13423 [4:49:40<112:12:13, 31.36s/it]  4%|▍         | 545/13423 [4:50:09<109:43:41, 30.67s/it]  4%|▍         | 546/13423 [4:50:38<108:09:07, 30.24s/it]  4%|▍         | 547/13423 [4:51:07<106:47:10, 29.86s/it]  4%|▍         | 548/13423 [4:51:38<107:47:37, 30.14s/it]  4%|▍         | 549/13423 [4:52:10<109:34:03, 30.64s/it]  4%|▍         | 550/13423 [4:52:42<111:07:24, 31.08s/it]                                                         {'loss': 0.9911, 'learning_rate': 1.9993710198809135e-05, 'epoch': 0.04}
  4%|▍         | 550/13423 [4:52:42<111:07:24, 31.08s/it]  4%|▍         | 551/13423 [4:53:14<111:56:41, 31.31s/it]  4%|▍         | 552/13423 [4:53:46<112:58:47, 31.60s/it]  4%|▍         | 553/13423 [4:54:18<113:04:05, 31.63s/it]  4%|▍         | 554/13423 [4:54:51<114:23:55, 32.00s/it]  4%|▍         | 555/13423 [4:55:22<113:57:44, 31.88s/it]  4%|▍         | 556/13423 [4:55:55<115:06:22, 32.21s/it]  4%|▍         | 557/13423 [4:56:27<114:29:16, 32.03s/it]  4%|▍         | 558/13423 [4:57:00<115:43:27, 32.38s/it]  4%|▍         | 559/13423 [4:57:32<115:05:34, 32.21s/it]  4%|▍         | 560/13423 [4:58:05<115:51:41, 32.43s/it]                                                         {'loss': 0.9604, 'learning_rate': 1.9992825442045714e-05, 'epoch': 0.04}
  4%|▍         | 560/13423 [4:58:05<115:51:41, 32.43s/it]  4%|▍         | 561/13423 [4:58:37<115:10:45, 32.24s/it]  4%|▍         | 562/13423 [4:59:10<116:06:33, 32.50s/it]  4%|▍         | 563/13423 [4:59:42<115:35:06, 32.36s/it]  4%|▍         | 564/13423 [5:00:15<116:07:39, 32.51s/it]  4%|▍         | 565/13423 [5:00:47<115:19:54, 32.29s/it]  4%|▍         | 566/13423 [5:01:19<115:53:43, 32.45s/it]  4%|▍         | 567/13423 [5:01:51<115:06:34, 32.23s/it]  4%|▍         | 568/13423 [5:02:24<116:01:00, 32.49s/it]  4%|▍         | 569/13423 [5:02:56<115:23:43, 32.32s/it]  4%|▍         | 570/13423 [5:03:29<116:15:54, 32.56s/it]                                                         {'loss': 0.8848, 'learning_rate': 1.999188250633385e-05, 'epoch': 0.04}
  4%|▍         | 570/13423 [5:03:29<116:15:54, 32.56s/it]  4%|▍         | 571/13423 [5:04:01<115:23:33, 32.32s/it]  4%|▍         | 572/13423 [5:04:34<116:13:24, 32.56s/it]  4%|▍         | 573/13423 [5:05:06<115:22:27, 32.32s/it]  4%|▍         | 574/13423 [5:05:39<116:04:32, 32.52s/it]  4%|▍         | 575/13423 [5:06:11<115:07:45, 32.26s/it]  4%|▍         | 576/13423 [5:06:44<115:56:42, 32.49s/it]  4%|▍         | 577/13423 [5:07:15<115:07:18, 32.26s/it]  4%|▍         | 578/13423 [5:07:48<116:06:06, 32.54s/it]  4%|▍         | 579/13423 [5:08:20<115:00:48, 32.24s/it]  4%|▍         | 580/13423 [5:08:53<115:46:10, 32.45s/it]                                                         {'loss': 0.939, 'learning_rate': 1.9990881397163388e-05, 'epoch': 0.04}
  4%|▍         | 580/13423 [5:08:53<115:46:10, 32.45s/it]  4%|▍         | 581/13423 [5:09:25<114:50:51, 32.20s/it]  4%|▍         | 582/13423 [5:09:57<115:35:12, 32.41s/it]  4%|▍         | 583/13423 [5:10:29<114:36:07, 32.13s/it]  4%|▍         | 584/13423 [5:11:01<115:04:26, 32.27s/it]  4%|▍         | 585/13423 [5:11:33<114:26:05, 32.09s/it]  4%|▍         | 586/13423 [5:12:06<115:00:44, 32.25s/it]  4%|▍         | 587/13423 [5:12:37<114:09:16, 32.02s/it]  4%|▍         | 588/13423 [5:13:10<114:35:01, 32.14s/it]  4%|▍         | 589/13423 [5:13:41<113:50:45, 31.93s/it]  4%|▍         | 590/13423 [5:14:14<114:26:19, 32.10s/it]                                                         {'loss': 0.935, 'learning_rate': 1.9989822120362853e-05, 'epoch': 0.04}
  4%|▍         | 590/13423 [5:14:14<114:26:19, 32.10s/it]  4%|▍         | 591/13423 [5:14:45<114:03:04, 32.00s/it]  4%|▍         | 592/13423 [5:15:18<114:45:59, 32.20s/it]  4%|▍         | 593/13423 [5:15:50<114:11:34, 32.04s/it]  4%|▍         | 594/13423 [5:16:22<114:40:46, 32.18s/it]  4%|▍         | 595/13423 [5:16:54<113:56:39, 31.98s/it]  4%|▍         | 596/13423 [5:17:26<114:28:30, 32.13s/it]  4%|▍         | 597/13423 [5:17:58<113:42:36, 31.92s/it]  4%|▍         | 598/13423 [5:18:30<114:15:20, 32.07s/it]  4%|▍         | 599/13423 [5:19:02<113:43:40, 31.93s/it]  4%|▍         | 600/13423 [5:19:34<114:03:08, 32.02s/it]                                                         {'loss': 0.8994, 'learning_rate': 1.9988704682099432e-05, 'epoch': 0.04}
  4%|▍         | 600/13423 [5:19:34<114:03:08, 32.02s/it]  4%|▍         | 601/13423 [5:20:05<113:30:45, 31.87s/it]  4%|▍         | 602/13423 [5:20:38<114:08:57, 32.05s/it]  4%|▍         | 603/13423 [5:21:09<113:31:48, 31.88s/it]  4%|▍         | 604/13423 [5:21:42<114:07:23, 32.05s/it]  5%|▍         | 605/13423 [5:22:13<113:29:17, 31.87s/it]  5%|▍         | 606/13423 [5:22:46<113:53:41, 31.99s/it]  5%|▍         | 607/13423 [5:23:17<113:21:19, 31.84s/it]  5%|▍         | 608/13423 [5:23:49<113:56:07, 32.01s/it]  5%|▍         | 609/13423 [5:24:21<113:28:17, 31.88s/it]  5%|▍         | 610/13423 [5:24:54<114:10:10, 32.08s/it]                                                         {'loss': 0.8889, 'learning_rate': 1.998752908887893e-05, 'epoch': 0.05}
  5%|▍         | 610/13423 [5:24:54<114:10:10, 32.08s/it]  5%|▍         | 611/13423 [5:25:25<113:44:03, 31.96s/it]  5%|▍         | 612/13423 [5:25:58<114:38:29, 32.22s/it]  5%|▍         | 613/13423 [5:26:30<113:57:05, 32.02s/it]  5%|▍         | 614/13423 [5:27:02<114:45:40, 32.25s/it]  5%|▍         | 615/13423 [5:27:34<113:57:17, 32.03s/it]  5%|▍         | 616/13423 [5:28:07<114:31:19, 32.19s/it]  5%|▍         | 617/13423 [5:28:38<113:48:26, 31.99s/it]  5%|▍         | 618/13423 [5:29:10<114:14:13, 32.12s/it]  5%|▍         | 619/13423 [5:29:42<113:42:10, 31.97s/it]  5%|▍         | 620/13423 [5:30:15<114:24:30, 32.17s/it]                                                         {'loss': 0.8374, 'learning_rate': 1.9986295347545738e-05, 'epoch': 0.05}
  5%|▍         | 620/13423 [5:30:15<114:24:30, 32.17s/it]  5%|▍         | 621/13423 [5:30:46<113:44:06, 31.98s/it]  5%|▍         | 622/13423 [5:31:19<114:14:32, 32.13s/it]  5%|▍         | 623/13423 [5:31:50<113:39:13, 31.97s/it]  5%|▍         | 624/13423 [5:32:23<114:25:03, 32.18s/it]  5%|▍         | 625/13423 [5:32:55<113:41:17, 31.98s/it]  5%|▍         | 626/13423 [5:33:27<114:26:24, 32.19s/it]  5%|▍         | 627/13423 [5:33:59<113:43:15, 31.99s/it]  5%|▍         | 628/13423 [5:34:31<114:21:12, 32.17s/it]  5%|▍         | 629/13423 [5:35:03<113:49:10, 32.03s/it]  5%|▍         | 630/13423 [5:35:36<114:26:44, 32.21s/it]                                                         {'loss': 0.8801, 'learning_rate': 1.9985003465282788e-05, 'epoch': 0.05}
  5%|▍         | 630/13423 [5:35:36<114:26:44, 32.21s/it]  5%|▍         | 631/13423 [5:36:07<113:41:14, 31.99s/it]  5%|▍         | 632/13423 [5:36:40<114:08:45, 32.13s/it]  5%|▍         | 633/13423 [5:37:11<113:28:28, 31.94s/it]  5%|▍         | 634/13423 [5:37:44<114:11:18, 32.14s/it]  5%|▍         | 635/13423 [5:38:15<113:31:30, 31.96s/it]  5%|▍         | 636/13423 [5:38:48<114:15:54, 32.17s/it]  5%|▍         | 637/13423 [5:39:19<113:30:47, 31.96s/it]  5%|▍         | 638/13423 [5:39:52<114:13:26, 32.16s/it]  5%|▍         | 639/13423 [5:40:23<113:31:28, 31.97s/it]  5%|▍         | 640/13423 [5:40:56<113:59:57, 32.10s/it]                                                         {'loss': 0.9159, 'learning_rate': 1.9983653449611507e-05, 'epoch': 0.05}
  5%|▍         | 640/13423 [5:40:56<113:59:57, 32.10s/it]  5%|▍         | 641/13423 [5:41:28<113:26:30, 31.95s/it]  5%|▍         | 642/13423 [5:42:00<114:06:33, 32.14s/it]  5%|▍         | 643/13423 [5:42:32<113:31:52, 31.98s/it]  5%|▍         | 644/13423 [5:43:04<114:02:17, 32.13s/it]  5%|▍         | 645/13423 [5:43:36<113:24:11, 31.95s/it]  5%|▍         | 646/13423 [5:44:08<113:56:58, 32.11s/it]  5%|▍         | 647/13423 [5:44:40<113:22:51, 31.95s/it]  5%|▍         | 648/13423 [5:45:13<114:18:20, 32.21s/it]  5%|▍         | 649/13423 [5:45:44<113:32:50, 32.00s/it]  5%|▍         | 650/13423 [5:46:17<114:02:07, 32.14s/it]                                                         {'loss': 0.835, 'learning_rate': 1.9982245308391787e-05, 'epoch': 0.05}
  5%|▍         | 650/13423 [5:46:17<114:02:07, 32.14s/it]  5%|▍         | 651/13423 [5:46:48<113:24:22, 31.97s/it]  5%|▍         | 652/13423 [5:47:21<113:56:54, 32.12s/it]  5%|▍         | 653/13423 [5:47:52<113:16:48, 31.93s/it]  5%|▍         | 654/13423 [5:48:24<113:37:36, 32.04s/it]  5%|▍         | 655/13423 [5:48:56<113:03:04, 31.88s/it]  5%|▍         | 656/13423 [5:49:28<113:17:59, 31.95s/it]  5%|▍         | 657/13423 [5:49:59<112:42:04, 31.78s/it]  5%|▍         | 658/13423 [5:50:31<112:57:27, 31.86s/it]  5%|▍         | 659/13423 [5:51:03<112:31:37, 31.74s/it]  5%|▍         | 660/13423 [5:51:35<112:44:38, 31.80s/it]                                                         {'loss': 0.8999, 'learning_rate': 1.9980779049821925e-05, 'epoch': 0.05}
  5%|▍         | 660/13423 [5:51:35<112:44:38, 31.80s/it]  5%|▍         | 661/13423 [5:52:06<112:27:55, 31.73s/it]  5%|▍         | 662/13423 [5:52:38<112:42:14, 31.79s/it]  5%|▍         | 663/13423 [5:53:10<112:27:14, 31.73s/it]  5%|▍         | 664/13423 [5:53:42<112:42:16, 31.80s/it]  5%|▍         | 665/13423 [5:54:13<112:24:16, 31.72s/it]  5%|▍         | 666/13423 [5:54:45<112:42:52, 31.81s/it]  5%|▍         | 667/13423 [5:55:17<112:13:25, 31.67s/it]  5%|▍         | 668/13423 [5:55:48<112:03:23, 31.63s/it]  5%|▍         | 669/13423 [5:56:20<112:00:16, 31.61s/it]  5%|▍         | 670/13423 [5:56:51<111:55:35, 31.60s/it]                                                         {'loss': 0.8619, 'learning_rate': 1.9979254682438584e-05, 'epoch': 0.05}
  5%|▍         | 670/13423 [5:56:51<111:55:35, 31.60s/it]  5%|▍         | 671/13423 [5:57:23<111:54:06, 31.59s/it]  5%|▌         | 672/13423 [5:57:54<111:27:47, 31.47s/it]  5%|▌         | 673/13423 [5:58:26<111:56:21, 31.61s/it]  5%|▌         | 674/13423 [5:58:57<111:23:01, 31.45s/it]  5%|▌         | 675/13423 [5:59:29<111:45:33, 31.56s/it]  5%|▌         | 676/13423 [6:00:00<111:22:47, 31.46s/it]  5%|▌         | 677/13423 [6:00:32<111:30:19, 31.49s/it]  5%|▌         | 678/13423 [6:01:03<111:18:28, 31.44s/it]  5%|▌         | 679/13423 [6:01:35<111:49:54, 31.59s/it]  5%|▌         | 680/13423 [6:02:06<111:34:05, 31.52s/it]                                                         {'loss': 0.8581, 'learning_rate': 1.997767221511674e-05, 'epoch': 0.05}
  5%|▌         | 680/13423 [6:02:06<111:34:05, 31.52s/it]  5%|▌         | 681/13423 [6:02:39<112:58:12, 31.92s/it]  5%|▌         | 682/13423 [6:03:11<112:21:31, 31.75s/it]  5%|▌         | 683/13423 [6:03:43<113:23:30, 32.04s/it]  5%|▌         | 684/13423 [6:04:15<112:46:57, 31.87s/it]  5%|▌         | 685/13423 [6:04:47<113:04:22, 31.96s/it]  5%|▌         | 686/13423 [6:05:19<112:38:56, 31.84s/it]  5%|▌         | 687/13423 [6:05:51<113:01:49, 31.95s/it]  5%|▌         | 688/13423 [6:06:22<112:17:16, 31.74s/it]  5%|▌         | 689/13423 [6:06:54<112:42:19, 31.86s/it]  5%|▌         | 690/13423 [6:07:26<112:09:04, 31.71s/it]                                                         {'loss': 0.8533, 'learning_rate': 1.9976031657069632e-05, 'epoch': 0.05}
  5%|▌         | 690/13423 [6:07:26<112:09:04, 31.71s/it]  5%|▌         | 691/13423 [6:07:57<112:03:05, 31.68s/it]  5%|▌         | 692/13423 [6:08:29<111:46:30, 31.61s/it]  5%|▌         | 693/13423 [6:09:00<111:23:08, 31.50s/it]  5%|▌         | 694/13423 [6:09:31<111:16:11, 31.47s/it]  5%|▌         | 695/13423 [6:10:02<110:58:28, 31.39s/it]  5%|▌         | 696/13423 [6:10:34<111:20:01, 31.49s/it]  5%|▌         | 697/13423 [6:11:05<110:58:16, 31.39s/it]  5%|▌         | 698/13423 [6:11:37<111:21:11, 31.50s/it]  5%|▌         | 699/13423 [6:12:08<111:12:48, 31.47s/it][2024-04-22 23:11:04,659] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1028670
[2024-04-22 23:11:09,475] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1028671
[2024-04-22 23:11:13,764] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1028672
[2024-04-22 23:11:17,585] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1028673
[2024-04-22 23:11:17,585] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1028674
[2024-04-22 23:11:21,716] [ERROR] [launch.py:321:sigkill_handler] ['/home/data_llm/anaconda3/envs/moellava/bin/python', '-u', '/home/data_llm/FoodHealthMMLLM/moellava/train/train_xformers.py', '--local_rank=4', '--moe_enable', 'True', '--num_experts', '4', '--top_k_experts', '2', '--capacity_factor', '1.5', '--moe_mode', 'sparse', '--use_residual', 'False', '--router_aux_loss_coef', '0.01', '--train_modules', 'fc1', 'fc2', 'wg', '--deepspeed', '../../zero2_offload.json', '--model_name_or_path', '/mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-nutv2', '--version', 'phi', '--data_path', '/mnt/data_llm/json_file/101_train_prompt1.json', '/mnt/data_llm/json_file/172_train_prompt1.json', '/mnt/data_llm/json_file/2k_train_prompt1.json', '--image_folder', '/media/LLM_data/food_recognition_dataset', '--image_tower', 'openai/clip-vit-large-patch14-336', '--image_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--check_point_file_name', '/mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-nutv2-moe-v2.json', '--output_dir', '/mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-nutv2-moe-v2', '--num_train_epochs', '1', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '5000', '--save_total_limit', '30', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--tf32', 'False', '--model_max_length', '512', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '16', '--lazy_preprocess', 'True', '--report_to', 'tensorboard', '--cache_dir', '/media/fast_data/huggingface/hub/'] exits with return code = -15
