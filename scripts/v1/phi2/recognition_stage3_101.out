nohup: 忽略输入
0,1,2,3,4
[2024-05-16 00:19:36,580] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-16 00:19:39,559] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-16 00:19:39,560] [INFO] [runner.py:555:main] cmd = /home/data_llm/anaconda3/envs/moellava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNF19 --master_addr=127.0.0.1 --master_port=2227 --enable_each_rank_log=None /home/data_llm/madehua/FoodHealthMMLLM/moellava/train/train_xformers.py --do_train --moe_enable True --num_experts 4 --top_k_experts 2 --capacity_factor 1.5 --moe_mode sparse --use_residual False --router_aux_loss_coef 0.01 --train_modules fc1 fc2 wg --deepspeed ../../zero2_offload.json --model_name_or_path /mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-v0426 --version phi --data_path /mnt/data_llm/json_file/101_train_prompt10.json --image_folder /media/LLM_data/food_recognition_dataset --image_tower /mnt/data_llm/model/clip-vit-large-patch14-336 --image_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --check_point_file_name /mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-moe-v101_0426.json --output_dir /mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-moe-v101_0426 --num_train_epochs 4 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 8 --evaluation_strategy no --save_strategy epoch --save_total_limit 5 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 512 --gradient_checkpointing True --dataloader_num_workers 64 --lazy_preprocess True --report_to tensorboard --cache_dir /media/fast_data/huggingface/hub/
[2024-05-16 00:19:40,953] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-16 00:19:43,723] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2024-05-16 00:19:43,723] [INFO] [launch.py:138:main] 0 NCCL_IB_TIMEOUT=22
[2024-05-16 00:19:43,723] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4]}
[2024-05-16 00:19:43,723] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=5, node_rank=0
[2024-05-16 00:19:43,723] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4]})
[2024-05-16 00:19:43,723] [INFO] [launch.py:163:main] dist_world_size=5
[2024-05-16 00:19:43,723] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4
[2024-05-16 00:19:47,311] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-16 00:19:47,354] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-16 00:19:47,407] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-16 00:19:47,470] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-16 00:19:47,625] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-16 00:19:48,867] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-16 00:19:48,867] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-05-16 00:19:48,867] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-16 00:19:48,931] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-16 00:19:48,931] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-05-16 00:19:48,945] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-16 00:19:48,945] [INFO] [comm.py:594:init_distributed] cdb=None
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
[2024-05-16 00:19:49,143] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-16 00:19:49,143] [INFO] [comm.py:594:init_distributed] cdb=None
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
[2024-05-16 00:19:49,250] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-05-16 00:19:49,251] [INFO] [comm.py:594:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  2.00s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.73s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.91s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.04s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.77s/it]
LLM init. firstly
 MoELLaVAPhiForCausalLM(
  (model): MoELLaVAPhiModel(
    (embed_tokens): Embedding(51200, 2560, padding_idx=50295)
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-31): 32 x PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower()
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2560, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2560, out_features=2560, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2560, out_features=51200, bias=False)
)
[2024-05-16 00:19:56,882] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.84s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.85s/it]
[2024-05-16 00:20:03,678] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-05-16 00:20:11,028] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-05-16 00:20:18,259] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-05-16 00:20:25,165] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-05-16 00:20:32,340] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-05-16 00:20:39,278] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-05-16 00:20:46,448] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-05-16 00:20:53,361] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-05-16 00:21:00,445] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-05-16 00:21:07,410] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-05-16 00:21:14,452] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-05-16 00:21:21,635] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-05-16 00:21:28,715] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-05-16 00:21:35,887] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2024-05-16 00:21:42,691] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
****************************************************************************************************
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
****************************************************************************************************
****************************************************************************************************
****************************************************************************************************
Vision encoder and proj init.
 MoELLaVAPhiForCausalLM(
  (model): MoELLaVAPhiModel(
    (embed_tokens): Embedding(51200, 2560, padding_idx=50295)
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (1): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (2): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (3): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (4): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (5): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (6): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (7): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (8): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (9): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (10): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (11): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (12): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (13): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (14): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (15): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (16): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (17): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (18): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (19): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (20): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (21): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (22): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (23): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (24): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (25): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (26): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (27): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (28): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (29): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (30): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (31): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2560, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2560, out_features=2560, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2560, out_features=51200, bias=False)
)
model.layers.0.mlp.deepspeed_moe.gate.wg.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.1.mlp.fc1.weight
model.layers.1.mlp.fc1.bias
model.layers.1.mlp.fc2.weight
model.layers.1.mlp.fc2.bias
model.layers.2.mlp.deepspeed_moe.gate.wg.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.3.mlp.fc1.weight
model.layers.3.mlp.fc1.bias
model.layers.3.mlp.fc2.weight
model.layers.3.mlp.fc2.bias
model.layers.4.mlp.deepspeed_moe.gate.wg.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.5.mlp.fc1.weight
model.layers.5.mlp.fc1.bias
model.layers.5.mlp.fc2.weight
model.layers.5.mlp.fc2.bias
model.layers.6.mlp.deepspeed_moe.gate.wg.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.7.mlp.fc1.weight
model.layers.7.mlp.fc1.bias
model.layers.7.mlp.fc2.weight
model.layers.7.mlp.fc2.bias
model.layers.8.mlp.deepspeed_moe.gate.wg.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.9.mlp.fc1.weight
model.layers.9.mlp.fc1.bias
model.layers.9.mlp.fc2.weight
model.layers.9.mlp.fc2.bias
model.layers.10.mlp.deepspeed_moe.gate.wg.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.11.mlp.fc1.weight
model.layers.11.mlp.fc1.bias
model.layers.11.mlp.fc2.weight
model.layers.11.mlp.fc2.bias
model.layers.12.mlp.deepspeed_moe.gate.wg.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.13.mlp.fc1.weight
model.layers.13.mlp.fc1.bias
model.layers.13.mlp.fc2.weight
model.layers.13.mlp.fc2.bias
model.layers.14.mlp.deepspeed_moe.gate.wg.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.15.mlp.fc1.weight
model.layers.15.mlp.fc1.bias
model.layers.15.mlp.fc2.weight
model.layers.15.mlp.fc2.bias
model.layers.16.mlp.deepspeed_moe.gate.wg.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.17.mlp.fc1.weight
model.layers.17.mlp.fc1.bias
model.layers.17.mlp.fc2.weight
model.layers.17.mlp.fc2.bias
model.layers.18.mlp.deepspeed_moe.gate.wg.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.19.mlp.fc1.weight
model.layers.19.mlp.fc1.bias
model.layers.19.mlp.fc2.weight
model.layers.19.mlp.fc2.bias
model.layers.20.mlp.deepspeed_moe.gate.wg.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.21.mlp.fc1.weight
model.layers.21.mlp.fc1.bias
model.layers.21.mlp.fc2.weight
model.layers.21.mlp.fc2.bias
model.layers.22.mlp.deepspeed_moe.gate.wg.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.23.mlp.fc1.weight
model.layers.23.mlp.fc1.bias
model.layers.23.mlp.fc2.weight
model.layers.23.mlp.fc2.bias
model.layers.24.mlp.deepspeed_moe.gate.wg.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.25.mlp.fc1.weight
model.layers.25.mlp.fc1.bias
model.layers.25.mlp.fc2.weight
model.layers.25.mlp.fc2.bias
model.layers.26.mlp.deepspeed_moe.gate.wg.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.27.mlp.fc1.weight
model.layers.27.mlp.fc1.bias
model.layers.27.mlp.fc2.weight
model.layers.27.mlp.fc2.bias
model.layers.28.mlp.deepspeed_moe.gate.wg.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.29.mlp.fc1.weight
model.layers.29.mlp.fc1.bias
model.layers.29.mlp.fc2.weight
model.layers.29.mlp.fc2.bias
model.layers.30.mlp.deepspeed_moe.gate.wg.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.31.mlp.fc1.weight
model.layers.31.mlp.fc1.bias
model.layers.31.mlp.fc2.weight
model.layers.31.mlp.fc2.bias
model.mm_projector.image_spatial_proj.0.weight
model.mm_projector.image_spatial_proj.0.bias
model.mm_projector.image_spatial_proj.2.weight
model.mm_projector.image_spatial_proj.2.bias
MoELLaVAPhiForCausalLM(
  (model): MoELLaVAPhiModel(
    (embed_tokens): Embedding(51200, 2560, padding_idx=50295)
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (1): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (2): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (3): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (4): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (5): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (6): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (7): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (8): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (9): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (10): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (11): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (12): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (13): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (14): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (15): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (16): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (17): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (18): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (19): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (20): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (21): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (22): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (23): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (24): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (25): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (26): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (27): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (28): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (29): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (30): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (31): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2560, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2560, out_features=2560, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2560, out_features=51200, bias=False)
)
Formatting inputs...Skip in lazy mode
****************************************************************************************************
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.5339653491973877 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4587230682373047 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4700958728790283 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.454141139984131 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4689691066741943 seconds
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
Rank: 4 partition count [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] and sizes[(169639936, False), (41984, False), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (10485760, True), (163840, True)] 
Rank: 2 partition count [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] and sizes[(169639936, False), (41984, False), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (10485760, True), (163840, True)] 
Rank: 3 partition count [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] and sizes[(169639936, False), (41984, False), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (10485760, True), (163840, True)] 
Rank: 0 partition count [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] and sizes[(169639936, False), (41984, False), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (10485760, True), (163840, True)] 
Rank: 1 partition count [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] and sizes[(169639936, False), (41984, False), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (10485760, True), (163840, True)] 
  0%|          | 0/1892 [00:00<?, ?it/s]/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
 50%|█████     | 947/1892 [02:20<02:20,  6.74it/s]                                                  {'loss': 0.2173, 'learning_rate': 1.0470636971911277e-05, 'epoch': 2.0}
 50%|█████     | 947/1892 [02:20<02:20,  6.74it/s] 50%|█████     | 948/1892 [03:15<03:38,  4.32it/s]                                                  {'loss': 0.2044, 'learning_rate': 1.0453534865873803e-05, 'epoch': 2.0}
 50%|█████     | 948/1892 [03:15<03:38,  4.32it/s] 50%|█████     | 949/1892 [04:10<05:28,  2.87it/s]                                                  {'loss': 0.2127, 'learning_rate': 1.0436431430489871e-05, 'epoch': 2.01}
 50%|█████     | 949/1892 [04:10<05:28,  2.87it/s] 50%|█████     | 950/1892 [05:05<08:04,  1.94it/s]                                                  {'loss': 0.197, 'learning_rate': 1.0419326715891005e-05, 'epoch': 2.01}
 50%|█████     | 950/1892 [05:05<08:04,  1.94it/s] 50%|█████     | 951/1892 [05:59<11:45,  1.33it/s]                                                  {'loss': 0.2187, 'learning_rate': 1.0402220772212472e-05, 'epoch': 2.01}
 50%|█████     | 951/1892 [05:59<11:45,  1.33it/s] 50%|█████     | 952/1892 [06:54<16:57,  1.08s/it]                                                  {'loss': 0.2122, 'learning_rate': 1.0385113649593137e-05, 'epoch': 2.01}
 50%|█████     | 952/1892 [06:54<16:57,  1.08s/it] 50%|█████     | 953/1892 [07:48<24:14,  1.55s/it]                                                  {'loss': 0.1989, 'learning_rate': 1.0368005398175326e-05, 'epoch': 2.01}
 50%|█████     | 953/1892 [07:48<24:14,  1.55s/it] 50%|█████     | 954/1892 [08:43<34:27,  2.20s/it]                                                  {'loss': 0.2127, 'learning_rate': 1.0350896068104676e-05, 'epoch': 2.02}
 50%|█████     | 954/1892 [08:43<34:27,  2.20s/it] 50%|█████     | 955/1892 [09:37<48:33,  3.11s/it]                                                  {'loss': 0.2017, 'learning_rate': 1.0333785709529982e-05, 'epoch': 2.02}
 50%|█████     | 955/1892 [09:37<48:33,  3.11s/it] 51%|█████     | 956/1892 [10:32<1:07:52,  4.35s/it]                                                    {'loss': 0.2007, 'learning_rate': 1.0316674372603052e-05, 'epoch': 2.02}
 51%|█████     | 956/1892 [10:32<1:07:52,  4.35s/it] 51%|█████     | 957/1892 [11:26<1:33:47,  6.02s/it]                                                    {'loss': 0.2046, 'learning_rate': 1.0299562107478569e-05, 'epoch': 2.02}
 51%|█████     | 957/1892 [11:26<1:33:47,  6.02s/it] 51%|█████     | 958/1892 [12:21<2:08:10,  8.23s/it]                                                    {'loss': 0.1971, 'learning_rate': 1.0282448964313925e-05, 'epoch': 2.03}
 51%|█████     | 958/1892 [12:21<2:08:10,  8.23s/it] 51%|█████     | 959/1892 [13:16<2:52:04, 11.07s/it]                                                    {'loss': 0.2238, 'learning_rate': 1.0265334993269094e-05, 'epoch': 2.03}
 51%|█████     | 959/1892 [13:16<2:52:04, 11.07s/it] 51%|█████     | 960/1892 [14:10<3:46:12, 14.56s/it]                                                    {'loss': 0.2112, 'learning_rate': 1.0248220244506476e-05, 'epoch': 2.03}
 51%|█████     | 960/1892 [14:10<3:46:12, 14.56s/it] 51%|█████     | 961/1892 [15:06<4:51:15, 18.77s/it]                                                    {'loss': 0.2127, 'learning_rate': 1.0231104768190746e-05, 'epoch': 2.03}
 51%|█████     | 961/1892 [15:06<4:51:15, 18.77s/it] 51%|█████     | 962/1892 [16:01<6:02:06, 23.36s/it]                                                    {'loss': 0.2019, 'learning_rate': 1.0213988614488721e-05, 'epoch': 2.03}
 51%|█████     | 962/1892 [16:01<6:02:06, 23.36s/it] 51%|█████     | 963/1892 [16:55<7:16:22, 28.18s/it]                                                    {'loss': 0.2315, 'learning_rate': 1.0196871833569194e-05, 'epoch': 2.04}
 51%|█████     | 963/1892 [16:55<7:16:22, 28.18s/it] 51%|█████     | 964/1892 [17:50<8:29:50, 32.96s/it]                                                    {'loss': 0.2142, 'learning_rate': 1.0179754475602797e-05, 'epoch': 2.04}
 51%|█████     | 964/1892 [17:50<8:29:50, 32.96s/it] 51%|█████     | 965/1892 [18:44<9:38:00, 37.41s/it]                                                    {'loss': 0.2242, 'learning_rate': 1.0162636590761857e-05, 'epoch': 2.04}
 51%|█████     | 965/1892 [18:44<9:38:00, 37.41s/it] 51%|█████     | 966/1892 [19:39<10:37:14, 41.29s/it]                                                     {'loss': 0.2068, 'learning_rate': 1.0145518229220243e-05, 'epoch': 2.04}
 51%|█████     | 966/1892 [19:39<10:37:14, 41.29s/it] 51%|█████     | 967/1892 [20:33<11:26:22, 44.52s/it]                                                     {'loss': 0.196, 'learning_rate': 1.012839944115323e-05, 'epoch': 2.04}
 51%|█████     | 967/1892 [20:33<11:26:22, 44.52s/it] 51%|█████     | 968/1892 [21:28<12:05:17, 47.10s/it]                                                     {'loss': 0.2112, 'learning_rate': 1.0111280276737327e-05, 'epoch': 2.05}
 51%|█████     | 968/1892 [21:28<12:05:17, 47.10s/it] 51%|█████     | 969/1892 [22:22<12:35:30, 49.11s/it]                                                     {'loss': 0.1954, 'learning_rate': 1.009416078615016e-05, 'epoch': 2.05}
 51%|█████     | 969/1892 [22:22<12:35:30, 49.11s/it] 51%|█████▏    | 970/1892 [23:17<12:58:11, 50.64s/it]                                                     {'loss': 0.2036, 'learning_rate': 1.0077041019570302e-05, 'epoch': 2.05}
 51%|█████▏    | 970/1892 [23:17<12:58:11, 50.64s/it] 51%|█████▏    | 971/1892 [24:12<13:14:52, 51.78s/it]                                                     {'loss': 0.228, 'learning_rate': 1.0059921027177143e-05, 'epoch': 2.05}
 51%|█████▏    | 971/1892 [24:12<13:14:52, 51.78s/it] 51%|█████▏    | 972/1892 [25:06<13:26:04, 52.57s/it]                                                     {'loss': 0.2037, 'learning_rate': 1.0042800859150726e-05, 'epoch': 2.05}
 51%|█████▏    | 972/1892 [25:06<13:26:04, 52.57s/it] 51%|█████▏    | 973/1892 [26:01<13:34:18, 53.16s/it]                                                     {'loss': 0.2176, 'learning_rate': 1.0025680565671616e-05, 'epoch': 2.06}
 51%|█████▏    | 973/1892 [26:01<13:34:18, 53.16s/it] 51%|█████▏    | 974/1892 [26:55<13:39:54, 53.59s/it]                                                     {'loss': 0.2467, 'learning_rate': 1.0008560196920745e-05, 'epoch': 2.06}
 51%|█████▏    | 974/1892 [26:55<13:39:54, 53.59s/it] 52%|█████▏    | 975/1892 [27:50<13:44:02, 53.92s/it]                                                     {'loss': 0.1913, 'learning_rate': 9.99143980307926e-06, 'epoch': 2.06}
 52%|█████▏    | 975/1892 [27:50<13:44:02, 53.92s/it] 52%|█████▏    | 976/1892 [28:45<13:45:55, 54.10s/it]                                                     {'loss': 0.2315, 'learning_rate': 9.974319434328387e-06, 'epoch': 2.06}
 52%|█████▏    | 976/1892 [28:45<13:45:55, 54.10s/it] 52%|█████▏    | 977/1892 [29:39<13:47:34, 54.27s/it]                                                     {'loss': 0.1914, 'learning_rate': 9.95719914084928e-06, 'epoch': 2.07}
 52%|█████▏    | 977/1892 [29:39<13:47:34, 54.27s/it] 52%|█████▏    | 978/1892 [30:34<13:47:17, 54.31s/it]                                                     {'loss': 0.2072, 'learning_rate': 9.940078972822862e-06, 'epoch': 2.07}
 52%|█████▏    | 978/1892 [30:34<13:47:17, 54.31s/it] 52%|█████▏    | 979/1892 [31:28<13:47:06, 54.36s/it]                                                     {'loss': 0.1976, 'learning_rate': 9.922958980429703e-06, 'epoch': 2.07}
 52%|█████▏    | 979/1892 [31:28<13:47:06, 54.36s/it] 52%|█████▏    | 980/1892 [32:22<13:46:18, 54.36s/it]                                                     {'loss': 0.2155, 'learning_rate': 9.905839213849843e-06, 'epoch': 2.07}
 52%|█████▏    | 980/1892 [32:23<13:46:18, 54.36s/it] 52%|█████▏    | 981/1892 [33:17<13:45:35, 54.37s/it]                                                     {'loss': 0.2243, 'learning_rate': 9.888719723262676e-06, 'epoch': 2.07}
 52%|█████▏    | 981/1892 [33:17<13:45:35, 54.37s/it] 52%|█████▏    | 982/1892 [34:11<13:44:50, 54.39s/it]                                                     {'loss': 0.1926, 'learning_rate': 9.871600558846772e-06, 'epoch': 2.08}
 52%|█████▏    | 982/1892 [34:11<13:44:50, 54.39s/it] 52%|█████▏    | 983/1892 [35:06<13:43:32, 54.36s/it]                                                     {'loss': 0.2305, 'learning_rate': 9.854481770779758e-06, 'epoch': 2.08}
 52%|█████▏    | 983/1892 [35:06<13:43:32, 54.36s/it] 52%|█████▏    | 984/1892 [36:00<13:43:14, 54.40s/it]                                                     {'loss': 0.2325, 'learning_rate': 9.837363409238145e-06, 'epoch': 2.08}
 52%|█████▏    | 984/1892 [36:00<13:43:14, 54.40s/it] 52%|█████▏    | 985/1892 [36:54<13:42:14, 54.39s/it]                                                     {'loss': 0.1967, 'learning_rate': 9.820245524397204e-06, 'epoch': 2.08}
 52%|█████▏    | 985/1892 [36:54<13:42:14, 54.39s/it] 52%|█████▏    | 986/1892 [37:49<13:41:24, 54.40s/it]                                                     {'loss': 0.195, 'learning_rate': 9.803128166430808e-06, 'epoch': 2.08}
 52%|█████▏    | 986/1892 [37:49<13:41:24, 54.40s/it] 52%|█████▏    | 987/1892 [38:43<13:40:02, 54.37s/it]                                                     {'loss': 0.2112, 'learning_rate': 9.786011385511279e-06, 'epoch': 2.09}
 52%|█████▏    | 987/1892 [38:43<13:40:02, 54.37s/it] 52%|█████▏    | 988/1892 [39:38<13:39:16, 54.38s/it]                                                     {'loss': 0.2025, 'learning_rate': 9.768895231809254e-06, 'epoch': 2.09}
 52%|█████▏    | 988/1892 [39:38<13:39:16, 54.38s/it] 52%|█████▏    | 989/1892 [40:32<13:38:08, 54.36s/it]                                                     {'loss': 0.2246, 'learning_rate': 9.751779755493529e-06, 'epoch': 2.09}
 52%|█████▏    | 989/1892 [40:32<13:38:08, 54.36s/it] 52%|█████▏    | 990/1892 [41:26<13:37:34, 54.38s/it]                                                     {'loss': 0.2255, 'learning_rate': 9.73466500673091e-06, 'epoch': 2.09}
 52%|█████▏    | 990/1892 [41:26<13:37:34, 54.38s/it] 52%|█████▏    | 991/1892 [42:21<13:36:56, 54.40s/it]                                                     {'loss': 0.2561, 'learning_rate': 9.71755103568608e-06, 'epoch': 2.1}
 52%|█████▏    | 991/1892 [42:21<13:36:56, 54.40s/it] 52%|█████▏    | 992/1892 [43:15<13:36:13, 54.41s/it]                                                     {'loss': 0.2144, 'learning_rate': 9.700437892521434e-06, 'epoch': 2.1}
 52%|█████▏    | 992/1892 [43:15<13:36:13, 54.41s/it] 52%|█████▏    | 993/1892 [44:10<13:35:19, 54.42s/it]                                                     {'loss': 0.2009, 'learning_rate': 9.68332562739695e-06, 'epoch': 2.1}
 52%|█████▏    | 993/1892 [44:10<13:35:19, 54.42s/it] 53%|█████▎    | 994/1892 [45:04<13:34:09, 54.40s/it]                                                     {'loss': 0.2119, 'learning_rate': 9.66621429047002e-06, 'epoch': 2.1}
 53%|█████▎    | 994/1892 [45:04<13:34:09, 54.40s/it] 53%|█████▎    | 995/1892 [45:58<13:33:25, 54.41s/it]                                                     {'loss': 0.2234, 'learning_rate': 9.649103931895325e-06, 'epoch': 2.1}
 53%|█████▎    | 995/1892 [45:58<13:33:25, 54.41s/it] 53%|█████▎    | 996/1892 [46:53<13:32:17, 54.40s/it]                                                     {'loss': 0.2312, 'learning_rate': 9.631994601824675e-06, 'epoch': 2.11}
 53%|█████▎    | 996/1892 [46:53<13:32:17, 54.40s/it] 53%|█████▎    | 997/1892 [47:47<13:31:08, 54.38s/it]                                                     {'loss': 0.1999, 'learning_rate': 9.614886350406865e-06, 'epoch': 2.11}
 53%|█████▎    | 997/1892 [47:47<13:31:08, 54.38s/it] 53%|█████▎    | 998/1892 [48:41<13:29:48, 54.35s/it]                                                     {'loss': 0.2168, 'learning_rate': 9.597779227787531e-06, 'epoch': 2.11}
 53%|█████▎    | 998/1892 [48:41<13:29:48, 54.35s/it] 53%|█████▎    | 999/1892 [49:36<13:29:01, 54.36s/it]                                                     {'loss': 0.219, 'learning_rate': 9.580673284108995e-06, 'epoch': 2.11}
 53%|█████▎    | 999/1892 [49:36<13:29:01, 54.36s/it] 53%|█████▎    | 1000/1892 [50:30<13:27:59, 54.35s/it]                                                      {'loss': 0.2313, 'learning_rate': 9.56356856951013e-06, 'epoch': 2.11}
 53%|█████▎    | 1000/1892 [50:30<13:27:59, 54.35s/it] 53%|█████▎    | 1001/1892 [51:25<13:27:14, 54.36s/it]                                                      {'loss': 0.208, 'learning_rate': 9.546465134126199e-06, 'epoch': 2.12}
 53%|█████▎    | 1001/1892 [51:25<13:27:14, 54.36s/it] 53%|█████▎    | 1002/1892 [52:19<13:26:25, 54.37s/it]                                                      {'loss': 0.2301, 'learning_rate': 9.529363028088725e-06, 'epoch': 2.12}
 53%|█████▎    | 1002/1892 [52:19<13:26:25, 54.37s/it] 53%|█████▎    | 1003/1892 [53:13<13:25:30, 54.37s/it]                                                      {'loss': 0.2541, 'learning_rate': 9.51226230152533e-06, 'epoch': 2.12}
 53%|█████▎    | 1003/1892 [53:13<13:25:30, 54.37s/it] 53%|█████▎    | 1004/1892 [54:08<13:25:16, 54.41s/it]                                                      {'loss': 0.2108, 'learning_rate': 9.495163004559582e-06, 'epoch': 2.12}
 53%|█████▎    | 1004/1892 [54:08<13:25:16, 54.41s/it] 53%|█████▎    | 1005/1892 [55:02<13:24:24, 54.41s/it]                                                      {'loss': 0.2106, 'learning_rate': 9.47806518731088e-06, 'epoch': 2.12}
 53%|█████▎    | 1005/1892 [55:02<13:24:24, 54.41s/it] 53%|█████▎    | 1006/1892 [55:57<13:23:40, 54.43s/it]                                                      {'loss': 0.2135, 'learning_rate': 9.460968899894267e-06, 'epoch': 2.13}
 53%|█████▎    | 1006/1892 [55:57<13:23:40, 54.43s/it] 53%|█████▎    | 1007/1892 [56:51<13:22:39, 54.42s/it]                                                      {'loss': 0.2071, 'learning_rate': 9.443874192420312e-06, 'epoch': 2.13}
 53%|█████▎    | 1007/1892 [56:51<13:22:39, 54.42s/it] 53%|█████▎    | 1008/1892 [57:45<13:21:50, 54.42s/it]                                                      {'loss': 0.2239, 'learning_rate': 9.426781114994954e-06, 'epoch': 2.13}
 53%|█████▎    | 1008/1892 [57:45<13:21:50, 54.42s/it] 53%|█████▎    | 1009/1892 [58:40<13:20:37, 54.40s/it]                                                      {'loss': 0.2252, 'learning_rate': 9.409689717719345e-06, 'epoch': 2.13}
 53%|█████▎    | 1009/1892 [58:40<13:20:37, 54.40s/it] 53%|█████▎    | 1010/1892 [59:34<13:19:43, 54.40s/it]                                                      {'loss': 0.2406, 'learning_rate': 9.392600050689723e-06, 'epoch': 2.14}
 53%|█████▎    | 1010/1892 [59:34<13:19:43, 54.40s/it] 53%|█████▎    | 1011/1892 [1:00:29<13:19:09, 54.43s/it]                                                        {'loss': 0.2002, 'learning_rate': 9.375512163997249e-06, 'epoch': 2.14}
 53%|█████▎    | 1011/1892 [1:00:29<13:19:09, 54.43s/it] 53%|█████▎    | 1012/1892 [1:01:23<13:18:08, 54.42s/it]                                                        {'loss': 0.2311, 'learning_rate': 9.358426107727862e-06, 'epoch': 2.14}
 53%|█████▎    | 1012/1892 [1:01:23<13:18:08, 54.42s/it] 54%|█████▎    | 1013/1892 [1:02:18<13:17:24, 54.43s/it]                                                        {'loss': 0.2125, 'learning_rate': 9.341341931962144e-06, 'epoch': 2.14}
 54%|█████▎    | 1013/1892 [1:02:18<13:17:24, 54.43s/it] 54%|█████▎    | 1014/1892 [1:03:12<13:15:51, 54.39s/it]                                                        {'loss': 0.1963, 'learning_rate': 9.324259686775162e-06, 'epoch': 2.14}
 54%|█████▎    | 1014/1892 [1:03:12<13:15:51, 54.39s/it] 54%|█████▎    | 1015/1892 [1:04:06<13:14:29, 54.36s/it]                                                        {'loss': 0.2135, 'learning_rate': 9.307179422236321e-06, 'epoch': 2.15}
 54%|█████▎    | 1015/1892 [1:04:06<13:14:29, 54.36s/it] 54%|█████▎    | 1016/1892 [1:05:00<13:13:15, 54.33s/it]                                                        {'loss': 0.2036, 'learning_rate': 9.290101188409227e-06, 'epoch': 2.15}
 54%|█████▎    | 1016/1892 [1:05:00<13:13:15, 54.33s/it] 54%|█████▍    | 1017/1892 [1:05:55<13:12:12, 54.32s/it]                                                        {'loss': 0.2159, 'learning_rate': 9.273025035351526e-06, 'epoch': 2.15}
 54%|█████▍    | 1017/1892 [1:05:55<13:12:12, 54.32s/it] 54%|█████▍    | 1018/1892 [1:06:49<13:10:43, 54.28s/it]                                                        {'loss': 0.2142, 'learning_rate': 9.255951013114767e-06, 'epoch': 2.15}
 54%|█████▍    | 1018/1892 [1:06:49<13:10:43, 54.28s/it] 54%|█████▍    | 1019/1892 [1:07:43<13:09:35, 54.27s/it]                                                        {'loss': 0.2003, 'learning_rate': 9.238879171744261e-06, 'epoch': 2.15}
 54%|█████▍    | 1019/1892 [1:07:43<13:09:35, 54.27s/it] 54%|█████▍    | 1020/1892 [1:08:37<13:08:24, 54.25s/it]                                                        {'loss': 0.197, 'learning_rate': 9.22180956127892e-06, 'epoch': 2.16}
 54%|█████▍    | 1020/1892 [1:08:37<13:08:24, 54.25s/it] 54%|█████▍    | 1021/1892 [1:09:32<13:07:43, 54.26s/it]                                                        {'loss': 0.2176, 'learning_rate': 9.204742231751116e-06, 'epoch': 2.16}
 54%|█████▍    | 1021/1892 [1:09:32<13:07:43, 54.26s/it] 54%|█████▍    | 1022/1892 [1:10:26<13:07:11, 54.29s/it]                                                        {'loss': 0.1965, 'learning_rate': 9.187677233186541e-06, 'epoch': 2.16}
 54%|█████▍    | 1022/1892 [1:10:26<13:07:11, 54.29s/it] 54%|█████▍    | 1023/1892 [1:11:20<13:06:01, 54.27s/it]                                                        {'loss': 0.2181, 'learning_rate': 9.170614615604047e-06, 'epoch': 2.16}
 54%|█████▍    | 1023/1892 [1:11:20<13:06:01, 54.27s/it] 54%|█████▍    | 1024/1892 [1:12:14<13:04:59, 54.26s/it]                                                        {'loss': 0.196, 'learning_rate': 9.153554429015517e-06, 'epoch': 2.16}
 54%|█████▍    | 1024/1892 [1:12:14<13:04:59, 54.26s/it] 54%|█████▍    | 1025/1892 [1:13:09<13:03:37, 54.23s/it]                                                        {'loss': 0.1909, 'learning_rate': 9.136496723425699e-06, 'epoch': 2.17}
 54%|█████▍    | 1025/1892 [1:13:09<13:03:37, 54.23s/it] 54%|█████▍    | 1026/1892 [1:14:03<13:02:46, 54.23s/it]                                                        {'loss': 0.2185, 'learning_rate': 9.119441548832074e-06, 'epoch': 2.17}
 54%|█████▍    | 1026/1892 [1:14:03<13:02:46, 54.23s/it] 54%|█████▍    | 1027/1892 [1:14:57<13:02:30, 54.28s/it]                                                        {'loss': 0.2011, 'learning_rate': 9.102388955224703e-06, 'epoch': 2.17}
 54%|█████▍    | 1027/1892 [1:14:57<13:02:30, 54.28s/it] 54%|█████▍    | 1028/1892 [1:15:53<13:06:19, 54.61s/it]                                                        {'loss': 0.2215, 'learning_rate': 9.085338992586084e-06, 'epoch': 2.17}
 54%|█████▍    | 1028/1892 [1:15:53<13:06:19, 54.61s/it] 54%|█████▍    | 1029/1892 [1:16:47<13:04:54, 54.57s/it]                                                        {'loss': 0.2065, 'learning_rate': 9.068291710891003e-06, 'epoch': 2.18}
 54%|█████▍    | 1029/1892 [1:16:47<13:04:54, 54.57s/it] 54%|█████▍    | 1030/1892 [1:17:42<13:03:14, 54.52s/it]                                                        {'loss': 0.2192, 'learning_rate': 9.051247160106387e-06, 'epoch': 2.18}
 54%|█████▍    | 1030/1892 [1:17:42<13:03:14, 54.52s/it] 54%|█████▍    | 1031/1892 [1:18:36<13:02:13, 54.51s/it]                                                        {'loss': 0.2252, 'learning_rate': 9.034205390191153e-06, 'epoch': 2.18}
 54%|█████▍    | 1031/1892 [1:18:36<13:02:13, 54.51s/it] 55%|█████▍    | 1032/1892 [1:19:30<13:01:05, 54.49s/it]                                                        {'loss': 0.2226, 'learning_rate': 9.017166451096077e-06, 'epoch': 2.18}
 55%|█████▍    | 1032/1892 [1:19:30<13:01:05, 54.49s/it] 55%|█████▍    | 1033/1892 [1:20:25<12:59:51, 54.47s/it]                                                        {'loss': 0.2249, 'learning_rate': 9.000130392763632e-06, 'epoch': 2.18}
 55%|█████▍    | 1033/1892 [1:20:25<12:59:51, 54.47s/it] 55%|█████▍    | 1034/1892 [1:21:19<12:58:41, 54.45s/it]                                                        {'loss': 0.2262, 'learning_rate': 8.983097265127849e-06, 'epoch': 2.19}
 55%|█████▍    | 1034/1892 [1:21:19<12:58:41, 54.45s/it] 55%|█████▍    | 1035/1892 [1:22:14<12:57:51, 54.46s/it]                                                        {'loss': 0.2321, 'learning_rate': 8.966067118114166e-06, 'epoch': 2.19}
 55%|█████▍    | 1035/1892 [1:22:14<12:57:51, 54.46s/it] 55%|█████▍    | 1036/1892 [1:23:08<12:56:49, 54.45s/it]                                                        {'loss': 0.2009, 'learning_rate': 8.94904000163929e-06, 'epoch': 2.19}
 55%|█████▍    | 1036/1892 [1:23:08<12:56:49, 54.45s/it] 55%|█████▍    | 1037/1892 [1:24:03<12:56:11, 54.47s/it]                                                        {'loss': 0.2218, 'learning_rate': 8.932015965611039e-06, 'epoch': 2.19}
 55%|█████▍    | 1037/1892 [1:24:03<12:56:11, 54.47s/it] 55%|█████▍    | 1038/1892 [1:24:57<12:55:43, 54.50s/it]                                                        {'loss': 0.2116, 'learning_rate': 8.914995059928206e-06, 'epoch': 2.19}
 55%|█████▍    | 1038/1892 [1:24:57<12:55:43, 54.50s/it] 55%|█████▍    | 1039/1892 [1:25:52<12:54:24, 54.47s/it]                                                        {'loss': 0.2122, 'learning_rate': 8.89797733448041e-06, 'epoch': 2.2}
 55%|█████▍    | 1039/1892 [1:25:52<12:54:24, 54.47s/it] 55%|█████▍    | 1040/1892 [1:26:46<12:53:30, 54.47s/it]                                                        {'loss': 0.1891, 'learning_rate': 8.880962839147943e-06, 'epoch': 2.2}
 55%|█████▍    | 1040/1892 [1:26:46<12:53:30, 54.47s/it] 55%|█████▌    | 1041/1892 [1:27:41<12:52:30, 54.47s/it]                                                        {'loss': 0.1961, 'learning_rate': 8.863951623801635e-06, 'epoch': 2.2}
 55%|█████▌    | 1041/1892 [1:27:41<12:52:30, 54.47s/it] 55%|█████▌    | 1042/1892 [1:28:35<12:51:44, 54.48s/it]                                                        {'loss': 0.2305, 'learning_rate': 8.846943738302697e-06, 'epoch': 2.2}
 55%|█████▌    | 1042/1892 [1:28:35<12:51:44, 54.48s/it] 55%|█████▌    | 1043/1892 [1:29:30<12:50:36, 54.46s/it]                                                        {'loss': 0.1953, 'learning_rate': 8.829939232502585e-06, 'epoch': 2.2}
 55%|█████▌    | 1043/1892 [1:29:30<12:50:36, 54.46s/it] 55%|█████▌    | 1044/1892 [1:30:24<12:49:23, 54.44s/it]                                                        {'loss': 0.1968, 'learning_rate': 8.812938156242848e-06, 'epoch': 2.21}
 55%|█████▌    | 1044/1892 [1:30:24<12:49:23, 54.44s/it] 55%|█████▌    | 1045/1892 [1:31:18<12:48:53, 54.47s/it]                                                        {'loss': 0.1981, 'learning_rate': 8.795940559354974e-06, 'epoch': 2.21}
 55%|█████▌    | 1045/1892 [1:31:18<12:48:53, 54.47s/it] 55%|█████▌    | 1046/1892 [1:32:13<12:47:49, 54.46s/it]                                                        {'loss': 0.2135, 'learning_rate': 8.778946491660265e-06, 'epoch': 2.21}
 55%|█████▌    | 1046/1892 [1:32:13<12:47:49, 54.46s/it] 55%|█████▌    | 1047/1892 [1:33:07<12:47:10, 54.47s/it]                                                        {'loss': 0.2372, 'learning_rate': 8.761956002969672e-06, 'epoch': 2.21}
 55%|█████▌    | 1047/1892 [1:33:07<12:47:10, 54.47s/it] 55%|█████▌    | 1048/1892 [1:34:02<12:45:51, 54.44s/it]                                                        {'loss': 0.2053, 'learning_rate': 8.744969143083659e-06, 'epoch': 2.22}
 55%|█████▌    | 1048/1892 [1:34:02<12:45:51, 54.44s/it] 55%|█████▌    | 1049/1892 [1:34:56<12:45:24, 54.48s/it]                                                        {'loss': 0.204, 'learning_rate': 8.72798596179205e-06, 'epoch': 2.22}
 55%|█████▌    | 1049/1892 [1:34:56<12:45:24, 54.48s/it] 55%|█████▌    | 1050/1892 [1:35:51<12:44:21, 54.47s/it]                                                        {'loss': 0.2249, 'learning_rate': 8.711006508873886e-06, 'epoch': 2.22}
 55%|█████▌    | 1050/1892 [1:35:51<12:44:21, 54.47s/it] 56%|█████▌    | 1051/1892 [1:36:45<12:43:18, 54.46s/it]                                                        {'loss': 0.232, 'learning_rate': 8.69403083409729e-06, 'epoch': 2.22}
 56%|█████▌    | 1051/1892 [1:36:45<12:43:18, 54.46s/it] 56%|█████▌    | 1052/1892 [1:37:40<12:42:26, 54.46s/it]                                                        {'loss': 0.2363, 'learning_rate': 8.677058987219294e-06, 'epoch': 2.22}
 56%|█████▌    | 1052/1892 [1:37:40<12:42:26, 54.46s/it] 56%|█████▌    | 1053/1892 [1:38:34<12:42:09, 54.50s/it]                                                        {'loss': 0.213, 'learning_rate': 8.660091017985727e-06, 'epoch': 2.23}
 56%|█████▌    | 1053/1892 [1:38:34<12:42:09, 54.50s/it] 56%|█████▌    | 1054/1892 [1:39:29<12:40:43, 54.47s/it]                                                        {'loss': 0.2064, 'learning_rate': 8.643126976131043e-06, 'epoch': 2.23}
 56%|█████▌    | 1054/1892 [1:39:29<12:40:43, 54.47s/it] 56%|█████▌    | 1055/1892 [1:40:23<12:39:10, 54.42s/it]                                                        {'loss': 0.2078, 'learning_rate': 8.626166911378184e-06, 'epoch': 2.23}
 56%|█████▌    | 1055/1892 [1:40:23<12:39:10, 54.42s/it] 56%|█████▌    | 1056/1892 [1:41:17<12:38:30, 54.44s/it]                                                        {'loss': 0.1916, 'learning_rate': 8.609210873438437e-06, 'epoch': 2.23}
 56%|█████▌    | 1056/1892 [1:41:17<12:38:30, 54.44s/it] 56%|█████▌    | 1057/1892 [1:42:12<12:37:37, 54.44s/it]                                                        {'loss': 0.2037, 'learning_rate': 8.59225891201129e-06, 'epoch': 2.23}
 56%|█████▌    | 1057/1892 [1:42:12<12:37:37, 54.44s/it] 56%|█████▌    | 1058/1892 [1:43:06<12:35:48, 54.38s/it]                                                        {'loss': 0.2135, 'learning_rate': 8.575311076784278e-06, 'epoch': 2.24}
 56%|█████▌    | 1058/1892 [1:43:06<12:35:48, 54.38s/it] 56%|█████▌    | 1059/1892 [1:44:01<12:35:04, 54.39s/it]                                                        {'loss': 0.2165, 'learning_rate': 8.558367417432838e-06, 'epoch': 2.24}
 56%|█████▌    | 1059/1892 [1:44:01<12:35:04, 54.39s/it] 56%|█████▌    | 1060/1892 [1:44:55<12:34:08, 54.39s/it]                                                        {'loss': 0.2012, 'learning_rate': 8.541427983620173e-06, 'epoch': 2.24}
 56%|█████▌    | 1060/1892 [1:44:55<12:34:08, 54.39s/it] 56%|█████▌    | 1061/1892 [1:45:49<12:33:33, 54.41s/it]                                                        {'loss': 0.2158, 'learning_rate': 8.524492824997103e-06, 'epoch': 2.24}
 56%|█████▌    | 1061/1892 [1:45:49<12:33:33, 54.41s/it] 56%|█████▌    | 1062/1892 [1:46:44<12:31:59, 54.36s/it]                                                        {'loss': 0.1916, 'learning_rate': 8.507561991201908e-06, 'epoch': 2.24}
 56%|█████▌    | 1062/1892 [1:46:44<12:31:59, 54.36s/it] 56%|█████▌    | 1063/1892 [1:47:38<12:31:27, 54.39s/it]                                                        {'loss': 0.2133, 'learning_rate': 8.490635531860204e-06, 'epoch': 2.25}
 56%|█████▌    | 1063/1892 [1:47:38<12:31:27, 54.39s/it] 56%|█████▌    | 1064/1892 [1:48:32<12:30:35, 54.39s/it]                                                        {'loss': 0.1951, 'learning_rate': 8.473713496584772e-06, 'epoch': 2.25}
 56%|█████▌    | 1064/1892 [1:48:32<12:30:35, 54.39s/it] 56%|█████▋    | 1065/1892 [1:49:27<12:30:07, 54.42s/it]                                                        {'loss': 0.1885, 'learning_rate': 8.456795934975433e-06, 'epoch': 2.25}
 56%|█████▋    | 1065/1892 [1:49:27<12:30:07, 54.42s/it] 56%|█████▋    | 1066/1892 [1:50:21<12:29:05, 54.41s/it]                                                        {'loss': 0.2059, 'learning_rate': 8.439882896618894e-06, 'epoch': 2.25}
 56%|█████▋    | 1066/1892 [1:50:21<12:29:05, 54.41s/it] 56%|█████▋    | 1067/1892 [1:51:16<12:28:31, 54.44s/it]                                                        {'loss': 0.2157, 'learning_rate': 8.422974431088607e-06, 'epoch': 2.26}
 56%|█████▋    | 1067/1892 [1:51:16<12:28:31, 54.44s/it] 56%|█████▋    | 1068/1892 [1:52:10<12:27:06, 54.40s/it]                                                        {'loss': 0.2037, 'learning_rate': 8.406070587944612e-06, 'epoch': 2.26}
 56%|█████▋    | 1068/1892 [1:52:10<12:27:06, 54.40s/it] 57%|█████▋    | 1069/1892 [1:53:05<12:26:26, 54.42s/it]                                                        {'loss': 0.2166, 'learning_rate': 8.389171416733412e-06, 'epoch': 2.26}
 57%|█████▋    | 1069/1892 [1:53:05<12:26:26, 54.42s/it] 57%|█████▋    | 1070/1892 [1:53:59<12:25:27, 54.41s/it]                                                        {'loss': 0.1957, 'learning_rate': 8.372276966987809e-06, 'epoch': 2.26}
 57%|█████▋    | 1070/1892 [1:53:59<12:25:27, 54.41s/it] 57%|█████▋    | 1071/1892 [1:54:53<12:24:07, 54.38s/it]                                                        {'loss': 0.2087, 'learning_rate': 8.355387288226764e-06, 'epoch': 2.26}
 57%|█████▋    | 1071/1892 [1:54:53<12:24:07, 54.38s/it] 57%|█████▋    | 1072/1892 [1:55:48<12:23:12, 54.38s/it]                                                        {'loss': 0.2128, 'learning_rate': 8.338502429955264e-06, 'epoch': 2.27}
 57%|█████▋    | 1072/1892 [1:55:48<12:23:12, 54.38s/it] 57%|█████▋    | 1073/1892 [1:56:42<12:21:58, 54.36s/it]                                                        {'loss': 0.2287, 'learning_rate': 8.321622441664155e-06, 'epoch': 2.27}
 57%|█████▋    | 1073/1892 [1:56:42<12:21:58, 54.36s/it] 57%|█████▋    | 1074/1892 [1:57:36<12:21:04, 54.36s/it]                                                        {'loss': 0.2044, 'learning_rate': 8.304747372830009e-06, 'epoch': 2.27}
 57%|█████▋    | 1074/1892 [1:57:36<12:21:04, 54.36s/it] 57%|█████▋    | 1075/1892 [1:58:31<12:19:46, 54.33s/it]                                                        {'loss': 0.2148, 'learning_rate': 8.287877272914994e-06, 'epoch': 2.27}
 57%|█████▋    | 1075/1892 [1:58:31<12:19:46, 54.33s/it] 57%|█████▋    | 1076/1892 [1:59:25<12:18:35, 54.31s/it]                                                        {'loss': 0.2011, 'learning_rate': 8.271012191366695e-06, 'epoch': 2.27}
 57%|█████▋    | 1076/1892 [1:59:25<12:18:35, 54.31s/it] 57%|█████▋    | 1077/1892 [2:00:19<12:18:00, 54.33s/it]                                                        {'loss': 0.201, 'learning_rate': 8.254152177618e-06, 'epoch': 2.28}
 57%|█████▋    | 1077/1892 [2:00:19<12:18:00, 54.33s/it] 57%|█████▋    | 1078/1892 [2:01:14<12:16:59, 54.32s/it]                                                        {'loss': 0.2121, 'learning_rate': 8.237297281086935e-06, 'epoch': 2.28}
 57%|█████▋    | 1078/1892 [2:01:14<12:16:59, 54.32s/it] 57%|█████▋    | 1079/1892 [2:02:08<12:16:27, 54.35s/it]                                                        {'loss': 0.2006, 'learning_rate': 8.220447551176536e-06, 'epoch': 2.28}
 57%|█████▋    | 1079/1892 [2:02:08<12:16:27, 54.35s/it] 57%|█████▋    | 1080/1892 [2:03:02<12:15:49, 54.37s/it]                                                        {'loss': 0.2135, 'learning_rate': 8.203603037274685e-06, 'epoch': 2.28}
 57%|█████▋    | 1080/1892 [2:03:02<12:15:49, 54.37s/it] 57%|█████▋    | 1081/1892 [2:03:57<12:15:23, 54.41s/it]                                                        {'loss': 0.1944, 'learning_rate': 8.186763788753983e-06, 'epoch': 2.29}
 57%|█████▋    | 1081/1892 [2:03:57<12:15:23, 54.41s/it] 57%|█████▋    | 1082/1892 [2:04:51<12:14:51, 54.43s/it]                                                        {'loss': 0.2057, 'learning_rate': 8.169929854971598e-06, 'epoch': 2.29}
 57%|█████▋    | 1082/1892 [2:04:51<12:14:51, 54.43s/it] 57%|█████▋    | 1083/1892 [2:05:46<12:13:57, 54.43s/it]                                                        {'loss': 0.2197, 'learning_rate': 8.153101285269112e-06, 'epoch': 2.29}
 57%|█████▋    | 1083/1892 [2:05:46<12:13:57, 54.43s/it] 57%|█████▋    | 1084/1892 [2:06:40<12:12:34, 54.40s/it]                                                        {'loss': 0.2294, 'learning_rate': 8.136278128972394e-06, 'epoch': 2.29}
 57%|█████▋    | 1084/1892 [2:06:40<12:12:34, 54.40s/it] 57%|█████▋    | 1085/1892 [2:07:35<12:11:56, 54.42s/it]                                                        {'loss': 0.2057, 'learning_rate': 8.119460435391442e-06, 'epoch': 2.29}
 57%|█████▋    | 1085/1892 [2:07:35<12:11:56, 54.42s/it] 57%|█████▋    | 1086/1892 [2:08:29<12:10:59, 54.42s/it]                                                        {'loss': 0.2042, 'learning_rate': 8.102648253820235e-06, 'epoch': 2.3}
 57%|█████▋    | 1086/1892 [2:08:29<12:10:59, 54.42s/it] 57%|█████▋    | 1087/1892 [2:09:23<12:09:48, 54.40s/it]                                                        {'loss': 0.2153, 'learning_rate': 8.085841633536611e-06, 'epoch': 2.3}
 57%|█████▋    | 1087/1892 [2:09:23<12:09:48, 54.40s/it] 58%|█████▊    | 1088/1892 [2:10:18<12:09:04, 54.41s/it]                                                        {'loss': 0.2178, 'learning_rate': 8.069040623802094e-06, 'epoch': 2.3}
 58%|█████▊    | 1088/1892 [2:10:18<12:09:04, 54.41s/it] 58%|█████▊    | 1089/1892 [2:11:12<12:08:51, 54.46s/it]                                                        {'loss': 0.2016, 'learning_rate': 8.052245273861771e-06, 'epoch': 2.3}
 58%|█████▊    | 1089/1892 [2:11:12<12:08:51, 54.46s/it] 58%|█████▊    | 1090/1892 [2:12:07<12:07:40, 54.44s/it]                                                        {'loss': 0.2109, 'learning_rate': 8.035455632944135e-06, 'epoch': 2.3}
 58%|█████▊    | 1090/1892 [2:12:07<12:07:40, 54.44s/it] 58%|█████▊    | 1091/1892 [2:13:01<12:06:49, 54.44s/it]                                                        {'loss': 0.2131, 'learning_rate': 8.018671750260948e-06, 'epoch': 2.31}
 58%|█████▊    | 1091/1892 [2:13:01<12:06:49, 54.44s/it] 58%|█████▊    | 1092/1892 [2:13:56<12:05:46, 54.43s/it]                                                        {'loss': 0.204, 'learning_rate': 8.001893675007098e-06, 'epoch': 2.31}
 58%|█████▊    | 1092/1892 [2:13:56<12:05:46, 54.43s/it] 58%|█████▊    | 1093/1892 [2:14:50<12:04:49, 54.43s/it]                                                        {'loss': 0.1922, 'learning_rate': 7.98512145636044e-06, 'epoch': 2.31}
 58%|█████▊    | 1093/1892 [2:14:50<12:04:49, 54.43s/it] 58%|█████▊    | 1094/1892 [2:15:44<12:03:41, 54.41s/it]                                                        {'loss': 0.2078, 'learning_rate': 7.968355143481674e-06, 'epoch': 2.31}
 58%|█████▊    | 1094/1892 [2:15:44<12:03:41, 54.41s/it] 58%|█████▊    | 1095/1892 [2:16:39<12:03:07, 54.44s/it]                                                        {'loss': 0.2082, 'learning_rate': 7.951594785514181e-06, 'epoch': 2.31}
 58%|█████▊    | 1095/1892 [2:16:39<12:03:07, 54.44s/it] 58%|█████▊    | 1096/1892 [2:17:33<12:02:09, 54.43s/it]                                                        {'loss': 0.2153, 'learning_rate': 7.934840431583898e-06, 'epoch': 2.32}
 58%|█████▊    | 1096/1892 [2:17:33<12:02:09, 54.43s/it] 58%|█████▊    | 1097/1892 [2:18:28<12:01:25, 54.45s/it]                                                        {'loss': 0.2091, 'learning_rate': 7.91809213079915e-06, 'epoch': 2.32}
 58%|█████▊    | 1097/1892 [2:18:28<12:01:25, 54.45s/it] 58%|█████▊    | 1098/1892 [2:19:22<12:00:32, 54.45s/it]                                                        {'loss': 0.2109, 'learning_rate': 7.901349932250536e-06, 'epoch': 2.32}
 58%|█████▊    | 1098/1892 [2:19:22<12:00:32, 54.45s/it] 58%|█████▊    | 1099/1892 [2:20:17<11:59:24, 54.43s/it]                                                        {'loss': 0.1918, 'learning_rate': 7.884613885010755e-06, 'epoch': 2.32}
 58%|█████▊    | 1099/1892 [2:20:17<11:59:24, 54.43s/it] 58%|█████▊    | 1100/1892 [2:21:11<11:58:35, 54.44s/it]                                                        {'loss': 0.2197, 'learning_rate': 7.867884038134478e-06, 'epoch': 2.33}
 58%|█████▊    | 1100/1892 [2:21:11<11:58:35, 54.44s/it] 58%|█████▊    | 1101/1892 [2:22:05<11:57:04, 54.39s/it]                                                        {'loss': 0.2303, 'learning_rate': 7.85116044065821e-06, 'epoch': 2.33}
 58%|█████▊    | 1101/1892 [2:22:05<11:57:04, 54.39s/it] 58%|█████▊    | 1102/1892 [2:23:00<11:55:56, 54.38s/it]                                                        {'loss': 0.2136, 'learning_rate': 7.834443141600131e-06, 'epoch': 2.33}
 58%|█████▊    | 1102/1892 [2:23:00<11:55:56, 54.38s/it] 58%|█████▊    | 1103/1892 [2:23:54<11:55:24, 54.40s/it]                                                        {'loss': 0.2052, 'learning_rate': 7.817732189959965e-06, 'epoch': 2.33}
 58%|█████▊    | 1103/1892 [2:23:54<11:55:24, 54.40s/it] 58%|█████▊    | 1104/1892 [2:24:49<11:54:12, 54.38s/it]                                                        {'loss': 0.2216, 'learning_rate': 7.801027634718829e-06, 'epoch': 2.33}
 58%|█████▊    | 1104/1892 [2:24:49<11:54:12, 54.38s/it] 58%|█████▊    | 1105/1892 [2:25:43<11:53:40, 54.41s/it]                                                        {'loss': 0.2227, 'learning_rate': 7.784329524839089e-06, 'epoch': 2.34}
 58%|█████▊    | 1105/1892 [2:25:43<11:53:40, 54.41s/it] 58%|█████▊    | 1106/1892 [2:26:37<11:52:42, 54.41s/it]                                                        {'loss': 0.2096, 'learning_rate': 7.767637909264227e-06, 'epoch': 2.34}
 58%|█████▊    | 1106/1892 [2:26:37<11:52:42, 54.41s/it] 59%|█████▊    | 1107/1892 [2:27:32<11:51:35, 54.39s/it]                                                        {'loss': 0.2209, 'learning_rate': 7.750952836918679e-06, 'epoch': 2.34}
 59%|█████▊    | 1107/1892 [2:27:32<11:51:35, 54.39s/it] 59%|█████▊    | 1108/1892 [2:28:26<11:50:52, 54.40s/it]                                                        {'loss': 0.2025, 'learning_rate': 7.73427435670771e-06, 'epoch': 2.34}
 59%|█████▊    | 1108/1892 [2:28:26<11:50:52, 54.40s/it] 59%|█████▊    | 1109/1892 [2:29:21<11:50:13, 54.42s/it]                                                        {'loss': 0.2246, 'learning_rate': 7.71760251751726e-06, 'epoch': 2.34}
 59%|█████▊    | 1109/1892 [2:29:21<11:50:13, 54.42s/it] 59%|█████▊    | 1110/1892 [2:30:15<11:49:47, 54.46s/it]                                                        {'loss': 0.1975, 'learning_rate': 7.700937368213807e-06, 'epoch': 2.35}
 59%|█████▊    | 1110/1892 [2:30:15<11:49:47, 54.46s/it] 59%|█████▊    | 1111/1892 [2:31:10<11:48:54, 54.46s/it]                                                        {'loss': 0.2017, 'learning_rate': 7.684278957644212e-06, 'epoch': 2.35}
 59%|█████▊    | 1111/1892 [2:31:10<11:48:54, 54.46s/it] 59%|█████▉    | 1112/1892 [2:32:04<11:48:26, 54.49s/it]                                                        {'loss': 0.2069, 'learning_rate': 7.667627334635595e-06, 'epoch': 2.35}
 59%|█████▉    | 1112/1892 [2:32:04<11:48:26, 54.49s/it] 59%|█████▉    | 1113/1892 [2:32:59<11:46:58, 54.45s/it]                                                        {'loss': 0.2199, 'learning_rate': 7.650982547995178e-06, 'epoch': 2.35}
 59%|█████▉    | 1113/1892 [2:32:59<11:46:58, 54.45s/it] 59%|█████▉    | 1114/1892 [2:33:53<11:46:48, 54.51s/it]                                                        {'loss': 0.2058, 'learning_rate': 7.634344646510136e-06, 'epoch': 2.35}
 59%|█████▉    | 1114/1892 [2:33:53<11:46:48, 54.51s/it] 59%|█████▉    | 1115/1892 [2:34:48<11:45:43, 54.50s/it]                                                        {'loss': 0.2189, 'learning_rate': 7.617713678947474e-06, 'epoch': 2.36}
 59%|█████▉    | 1115/1892 [2:34:48<11:45:43, 54.50s/it] 59%|█████▉    | 1116/1892 [2:35:42<11:44:39, 54.48s/it]                                                        {'loss': 0.2015, 'learning_rate': 7.6010896940538705e-06, 'epoch': 2.36}
 59%|█████▉    | 1116/1892 [2:35:42<11:44:39, 54.48s/it] 59%|█████▉    | 1117/1892 [2:36:37<11:43:13, 54.44s/it]                                                        {'loss': 0.2111, 'learning_rate': 7.584472740555533e-06, 'epoch': 2.36}
 59%|█████▉    | 1117/1892 [2:36:37<11:43:13, 54.44s/it] 59%|█████▉    | 1118/1892 [2:37:31<11:42:47, 54.48s/it]                                                        {'loss': 0.2082, 'learning_rate': 7.567862867158065e-06, 'epoch': 2.36}
 59%|█████▉    | 1118/1892 [2:37:31<11:42:47, 54.48s/it] 59%|█████▉    | 1119/1892 [2:38:26<11:41:43, 54.47s/it]                                                        {'loss': 0.194, 'learning_rate': 7.551260122546313e-06, 'epoch': 2.37}
 59%|█████▉    | 1119/1892 [2:38:26<11:41:43, 54.47s/it] 59%|█████▉    | 1120/1892 [2:39:20<11:40:22, 54.43s/it]                                                        {'loss': 0.2003, 'learning_rate': 7.534664555384235e-06, 'epoch': 2.37}
 59%|█████▉    | 1120/1892 [2:39:20<11:40:22, 54.43s/it] 59%|█████▉    | 1121/1892 [2:40:14<11:39:42, 54.45s/it]                                                        {'loss': 0.2239, 'learning_rate': 7.51807621431474e-06, 'epoch': 2.37}
 59%|█████▉    | 1121/1892 [2:40:14<11:39:42, 54.45s/it] 59%|█████▉    | 1122/1892 [2:41:09<11:38:49, 54.45s/it]                                                        {'loss': 0.2127, 'learning_rate': 7.5014951479595684e-06, 'epoch': 2.37}
 59%|█████▉    | 1122/1892 [2:41:09<11:38:49, 54.45s/it] 59%|█████▉    | 1123/1892 [2:42:03<11:38:18, 54.48s/it]                                                        {'loss': 0.2329, 'learning_rate': 7.484921404919135e-06, 'epoch': 2.37}
 59%|█████▉    | 1123/1892 [2:42:03<11:38:18, 54.48s/it] 59%|█████▉    | 1124/1892 [2:42:58<11:37:16, 54.48s/it]                                                        {'loss': 0.208, 'learning_rate': 7.468355033772383e-06, 'epoch': 2.38}
 59%|█████▉    | 1124/1892 [2:42:58<11:37:16, 54.48s/it] 59%|█████▉    | 1125/1892 [2:43:52<11:36:01, 54.45s/it]                                                        {'loss': 0.196, 'learning_rate': 7.451796083076656e-06, 'epoch': 2.38}
 59%|█████▉    | 1125/1892 [2:43:52<11:36:01, 54.45s/it] 60%|█████▉    | 1126/1892 [2:44:47<11:35:35, 54.48s/it]                                                        {'loss': 0.2024, 'learning_rate': 7.435244601367542e-06, 'epoch': 2.38}
 60%|█████▉    | 1126/1892 [2:44:47<11:35:35, 54.48s/it] 60%|█████▉    | 1127/1892 [2:45:41<11:34:59, 54.51s/it]                                                        {'loss': 0.1885, 'learning_rate': 7.418700637158742e-06, 'epoch': 2.38}
 60%|█████▉    | 1127/1892 [2:45:41<11:34:59, 54.51s/it] 60%|█████▉    | 1128/1892 [2:46:36<11:33:47, 54.49s/it]                                                        {'loss': 0.2053, 'learning_rate': 7.402164238941916e-06, 'epoch': 2.38}
 60%|█████▉    | 1128/1892 [2:46:36<11:33:47, 54.49s/it] 60%|█████▉    | 1129/1892 [2:47:30<11:33:09, 54.51s/it]                                                        {'loss': 0.2026, 'learning_rate': 7.38563545518655e-06, 'epoch': 2.39}
 60%|█████▉    | 1129/1892 [2:47:30<11:33:09, 54.51s/it] 60%|█████▉    | 1130/1892 [2:48:25<11:31:54, 54.48s/it]                                                        {'loss': 0.2252, 'learning_rate': 7.369114334339818e-06, 'epoch': 2.39}
 60%|█████▉    | 1130/1892 [2:48:25<11:31:54, 54.48s/it] 60%|█████▉    | 1131/1892 [2:49:19<11:30:40, 54.45s/it]                                                        {'loss': 0.2403, 'learning_rate': 7.352600924826423e-06, 'epoch': 2.39}
 60%|█████▉    | 1131/1892 [2:49:19<11:30:40, 54.45s/it] 60%|█████▉    | 1132/1892 [2:50:14<11:29:34, 54.44s/it]                                                        {'loss': 0.2072, 'learning_rate': 7.336095275048474e-06, 'epoch': 2.39}
 60%|█████▉    | 1132/1892 [2:50:14<11:29:34, 54.44s/it] 60%|█████▉    | 1133/1892 [2:51:08<11:28:35, 54.43s/it]                                                        {'loss': 0.203, 'learning_rate': 7.319597433385327e-06, 'epoch': 2.39}
 60%|█████▉    | 1133/1892 [2:51:08<11:28:35, 54.43s/it] 60%|█████▉    | 1134/1892 [2:52:03<11:28:28, 54.50s/it]                                                        {'loss': 0.2207, 'learning_rate': 7.303107448193462e-06, 'epoch': 2.4}
 60%|█████▉    | 1134/1892 [2:52:03<11:28:28, 54.50s/it] 60%|█████▉    | 1135/1892 [2:52:57<11:27:35, 54.50s/it]                                                        {'loss': 0.2173, 'learning_rate': 7.286625367806325e-06, 'epoch': 2.4}
 60%|█████▉    | 1135/1892 [2:52:57<11:27:35, 54.50s/it] 60%|██████    | 1136/1892 [2:53:52<11:26:36, 54.49s/it]                                                        {'loss': 0.1991, 'learning_rate': 7.270151240534191e-06, 'epoch': 2.4}
 60%|██████    | 1136/1892 [2:53:52<11:26:36, 54.49s/it] 60%|██████    | 1137/1892 [2:54:46<11:25:58, 54.51s/it]                                                        {'loss': 0.2281, 'learning_rate': 7.253685114664029e-06, 'epoch': 2.4}
 60%|██████    | 1137/1892 [2:54:46<11:25:58, 54.51s/it] 60%|██████    | 1138/1892 [2:55:41<11:24:35, 54.48s/it]                                                        {'loss': 0.2141, 'learning_rate': 7.237227038459349e-06, 'epoch': 2.41}
 60%|██████    | 1138/1892 [2:55:41<11:24:35, 54.48s/it] 60%|██████    | 1139/1892 [2:56:35<11:23:40, 54.48s/it]                                                        {'loss': 0.2022, 'learning_rate': 7.220777060160074e-06, 'epoch': 2.41}
 60%|██████    | 1139/1892 [2:56:35<11:23:40, 54.48s/it] 60%|██████    | 1140/1892 [2:57:30<11:22:58, 54.49s/it]                                                        {'loss': 0.1929, 'learning_rate': 7.204335227982383e-06, 'epoch': 2.41}
 60%|██████    | 1140/1892 [2:57:30<11:22:58, 54.49s/it] 60%|██████    | 1141/1892 [2:58:24<11:21:59, 54.49s/it]                                                        {'loss': 0.2452, 'learning_rate': 7.187901590118589e-06, 'epoch': 2.41}
 60%|██████    | 1141/1892 [2:58:24<11:21:59, 54.49s/it] 60%|██████    | 1142/1892 [2:59:19<11:21:21, 54.51s/it]                                                        {'loss': 0.2159, 'learning_rate': 7.171476194736975e-06, 'epoch': 2.41}
 60%|██████    | 1142/1892 [2:59:19<11:21:21, 54.51s/it] 60%|██████    | 1143/1892 [3:00:13<11:20:18, 54.50s/it]                                                        {'loss': 0.2115, 'learning_rate': 7.1550590899816685e-06, 'epoch': 2.42}
 60%|██████    | 1143/1892 [3:00:13<11:20:18, 54.50s/it] 60%|██████    | 1144/1892 [3:01:07<11:18:53, 54.46s/it]                                                        {'loss': 0.2195, 'learning_rate': 7.1386503239725025e-06, 'epoch': 2.42}
 60%|██████    | 1144/1892 [3:01:07<11:18:53, 54.46s/it] 61%|██████    | 1145/1892 [3:02:02<11:17:51, 54.45s/it]                                                        {'loss': 0.2246, 'learning_rate': 7.1222499448048635e-06, 'epoch': 2.42}
 61%|██████    | 1145/1892 [3:02:02<11:17:51, 54.45s/it] 61%|██████    | 1146/1892 [3:02:56<11:17:16, 54.47s/it]                                                        {'loss': 0.2006, 'learning_rate': 7.1058580005495526e-06, 'epoch': 2.42}
 61%|██████    | 1146/1892 [3:02:56<11:17:16, 54.47s/it] 61%|██████    | 1147/1892 [3:03:51<11:16:06, 54.45s/it]                                                        {'loss': 0.2049, 'learning_rate': 7.089474539252656e-06, 'epoch': 2.42}
 61%|██████    | 1147/1892 [3:03:51<11:16:06, 54.45s/it] 61%|██████    | 1148/1892 [3:04:45<11:14:47, 54.42s/it]                                                        {'loss': 0.2024, 'learning_rate': 7.073099608935387e-06, 'epoch': 2.43}
 61%|██████    | 1148/1892 [3:04:45<11:14:47, 54.42s/it] 61%|██████    | 1149/1892 [3:05:40<11:13:36, 54.40s/it]                                                        {'loss': 0.2249, 'learning_rate': 7.056733257593959e-06, 'epoch': 2.43}
 61%|██████    | 1149/1892 [3:05:40<11:13:36, 54.40s/it] 61%|██████    | 1150/1892 [3:06:34<11:14:35, 54.55s/it]                                                        {'loss': 0.2061, 'learning_rate': 7.040375533199439e-06, 'epoch': 2.43}
 61%|██████    | 1150/1892 [3:06:34<11:14:35, 54.55s/it] 61%|██████    | 1151/1892 [3:07:29<11:13:03, 54.50s/it]                                                        {'loss': 0.209, 'learning_rate': 7.024026483697606e-06, 'epoch': 2.43}
 61%|██████    | 1151/1892 [3:07:29<11:13:03, 54.50s/it] 61%|██████    | 1152/1892 [3:08:23<11:12:08, 54.50s/it]                                                        {'loss': 0.2218, 'learning_rate': 7.00768615700881e-06, 'epoch': 2.44}
 61%|██████    | 1152/1892 [3:08:23<11:12:08, 54.50s/it] 61%|██████    | 1153/1892 [3:09:18<11:10:46, 54.46s/it]                                                        {'loss': 0.1975, 'learning_rate': 6.991354601027846e-06, 'epoch': 2.44}
 61%|██████    | 1153/1892 [3:09:18<11:10:46, 54.46s/it] 61%|██████    | 1154/1892 [3:10:12<11:09:11, 54.41s/it]                                                        {'loss': 0.2042, 'learning_rate': 6.975031863623787e-06, 'epoch': 2.44}
 61%|██████    | 1154/1892 [3:10:12<11:09:11, 54.41s/it] 61%|██████    | 1155/1892 [3:11:06<11:07:52, 54.37s/it]                                                        {'loss': 0.2042, 'learning_rate': 6.958717992639859e-06, 'epoch': 2.44}
 61%|██████    | 1155/1892 [3:11:06<11:07:52, 54.37s/it] 61%|██████    | 1156/1892 [3:12:01<11:06:34, 54.34s/it]                                                        {'loss': 0.2006, 'learning_rate': 6.942413035893307e-06, 'epoch': 2.44}
 61%|██████    | 1156/1892 [3:12:01<11:06:34, 54.34s/it] 61%|██████    | 1157/1892 [3:12:55<11:05:52, 54.36s/it]                                                        {'loss': 0.2058, 'learning_rate': 6.92611704117525e-06, 'epoch': 2.45}
 61%|██████    | 1157/1892 [3:12:55<11:05:52, 54.36s/it] 61%|██████    | 1158/1892 [3:13:49<11:05:10, 54.37s/it]                                                        {'loss': 0.212, 'learning_rate': 6.909830056250527e-06, 'epoch': 2.45}
 61%|██████    | 1158/1892 [3:13:49<11:05:10, 54.37s/it] 61%|██████▏   | 1159/1892 [3:14:44<11:04:18, 54.38s/it]                                                        {'loss': 0.2328, 'learning_rate': 6.8935521288575806e-06, 'epoch': 2.45}
 61%|██████▏   | 1159/1892 [3:14:44<11:04:18, 54.38s/it] 61%|██████▏   | 1160/1892 [3:15:38<11:03:48, 54.41s/it]                                                        {'loss': 0.2265, 'learning_rate': 6.8772833067082945e-06, 'epoch': 2.45}
 61%|██████▏   | 1160/1892 [3:15:38<11:03:48, 54.41s/it] 61%|██████▏   | 1161/1892 [3:16:33<11:03:05, 54.43s/it]                                                        {'loss': 0.2135, 'learning_rate': 6.861023637487873e-06, 'epoch': 2.45}
 61%|██████▏   | 1161/1892 [3:16:33<11:03:05, 54.43s/it] 61%|██████▏   | 1162/1892 [3:17:27<11:02:09, 54.42s/it]                                                        {'loss': 0.1921, 'learning_rate': 6.844773168854686e-06, 'epoch': 2.46}
 61%|██████▏   | 1162/1892 [3:17:27<11:02:09, 54.42s/it] 61%|██████▏   | 1163/1892 [3:18:22<11:01:24, 54.44s/it]                                                        {'loss': 0.1956, 'learning_rate': 6.828531948440142e-06, 'epoch': 2.46}
 61%|██████▏   | 1163/1892 [3:18:22<11:01:24, 54.44s/it] 62%|██████▏   | 1164/1892 [3:19:16<11:00:09, 54.41s/it]                                                        {'loss': 0.1982, 'learning_rate': 6.8123000238485345e-06, 'epoch': 2.46}
 62%|██████▏   | 1164/1892 [3:19:16<11:00:09, 54.41s/it] 62%|██████▏   | 1165/1892 [3:20:10<10:59:04, 54.39s/it]                                                        {'loss': 0.2066, 'learning_rate': 6.7960774426569166e-06, 'epoch': 2.46}
 62%|██████▏   | 1165/1892 [3:20:10<10:59:04, 54.39s/it] 62%|██████▏   | 1166/1892 [3:21:05<10:58:16, 54.40s/it]                                                        {'loss': 0.2068, 'learning_rate': 6.779864252414954e-06, 'epoch': 2.46}
 62%|██████▏   | 1166/1892 [3:21:05<10:58:16, 54.40s/it] 62%|██████▏   | 1167/1892 [3:21:59<10:57:13, 54.39s/it]                                                        {'loss': 0.2019, 'learning_rate': 6.763660500644783e-06, 'epoch': 2.47}
 62%|██████▏   | 1167/1892 [3:21:59<10:57:13, 54.39s/it] 62%|██████▏   | 1168/1892 [3:22:53<10:56:22, 54.40s/it]                                                        {'loss': 0.2126, 'learning_rate': 6.747466234840884e-06, 'epoch': 2.47}
 62%|██████▏   | 1168/1892 [3:22:53<10:56:22, 54.40s/it] 62%|██████▏   | 1169/1892 [3:23:48<10:55:47, 54.42s/it]                                                        {'loss': 0.2051, 'learning_rate': 6.73128150246992e-06, 'epoch': 2.47}
 62%|██████▏   | 1169/1892 [3:23:48<10:55:47, 54.42s/it] 62%|██████▏   | 1170/1892 [3:24:42<10:55:05, 54.44s/it]                                                        {'loss': 0.2076, 'learning_rate': 6.7151063509706174e-06, 'epoch': 2.47}
 62%|██████▏   | 1170/1892 [3:24:42<10:55:05, 54.44s/it] 62%|██████▏   | 1171/1892 [3:25:37<10:53:58, 54.42s/it]                                                        {'loss': 0.2017, 'learning_rate': 6.698940827753625e-06, 'epoch': 2.48}
 62%|██████▏   | 1171/1892 [3:25:37<10:53:58, 54.42s/it] 62%|██████▏   | 1172/1892 [3:26:31<10:53:07, 54.43s/it]                                                        {'loss': 0.2069, 'learning_rate': 6.682784980201363e-06, 'epoch': 2.48}
 62%|██████▏   | 1172/1892 [3:26:31<10:53:07, 54.43s/it] 62%|██████▏   | 1173/1892 [3:27:26<10:52:15, 54.43s/it]                                                        {'loss': 0.2677, 'learning_rate': 6.666638855667899e-06, 'epoch': 2.48}
 62%|██████▏   | 1173/1892 [3:27:26<10:52:15, 54.43s/it] 62%|██████▏   | 1174/1892 [3:28:20<10:51:16, 54.42s/it]                                                        {'loss': 0.2302, 'learning_rate': 6.65050250147879e-06, 'epoch': 2.48}
 62%|██████▏   | 1174/1892 [3:28:20<10:51:16, 54.42s/it] 62%|██████▏   | 1175/1892 [3:29:15<10:50:22, 54.42s/it]                                                        {'loss': 0.2155, 'learning_rate': 6.634375964930968e-06, 'epoch': 2.48}
 62%|██████▏   | 1175/1892 [3:29:15<10:50:22, 54.42s/it] 62%|██████▏   | 1176/1892 [3:30:09<10:48:56, 54.38s/it]                                                        {'loss': 0.2098, 'learning_rate': 6.618259293292584e-06, 'epoch': 2.49}
 62%|██████▏   | 1176/1892 [3:30:09<10:48:56, 54.38s/it] 62%|██████▏   | 1177/1892 [3:31:03<10:48:31, 54.42s/it]                                                        {'loss': 0.2369, 'learning_rate': 6.60215253380287e-06, 'epoch': 2.49}
 62%|██████▏   | 1177/1892 [3:31:03<10:48:31, 54.42s/it] 62%|██████▏   | 1178/1892 [3:31:58<10:47:38, 54.42s/it]                                                        {'loss': 0.219, 'learning_rate': 6.58605573367201e-06, 'epoch': 2.49}
 62%|██████▏   | 1178/1892 [3:31:58<10:47:38, 54.42s/it] 62%|██████▏   | 1179/1892 [3:32:52<10:47:04, 54.45s/it]                                                        {'loss': 0.2048, 'learning_rate': 6.569968940080993e-06, 'epoch': 2.49}
 62%|██████▏   | 1179/1892 [3:32:52<10:47:04, 54.45s/it] 62%|██████▏   | 1180/1892 [3:33:47<10:46:15, 54.46s/it]                                                        {'loss': 0.2068, 'learning_rate': 6.553892200181482e-06, 'epoch': 2.49}
 62%|██████▏   | 1180/1892 [3:33:47<10:46:15, 54.46s/it] 62%|██████▏   | 1181/1892 [3:34:41<10:45:22, 54.46s/it]                                                        {'loss': 0.2211, 'learning_rate': 6.537825561095665e-06, 'epoch': 2.5}
 62%|██████▏   | 1181/1892 [3:34:41<10:45:22, 54.46s/it] 62%|██████▏   | 1182/1892 [3:35:36<10:44:40, 54.48s/it]                                                        {'loss': 0.2096, 'learning_rate': 6.521769069916136e-06, 'epoch': 2.5}
 62%|██████▏   | 1182/1892 [3:35:36<10:44:40, 54.48s/it] 63%|██████▎   | 1183/1892 [3:36:30<10:43:44, 54.48s/it]                                                        {'loss': 0.2147, 'learning_rate': 6.505722773705729e-06, 'epoch': 2.5}
 63%|██████▎   | 1183/1892 [3:36:30<10:43:44, 54.48s/it] 63%|██████▎   | 1184/1892 [3:37:25<10:42:26, 54.44s/it]                                                        {'loss': 0.2039, 'learning_rate': 6.489686719497405e-06, 'epoch': 2.5}
 63%|██████▎   | 1184/1892 [3:37:25<10:42:26, 54.44s/it] 63%|██████▎   | 1185/1892 [3:38:19<10:41:47, 54.47s/it]                                                        {'loss': 0.2152, 'learning_rate': 6.473660954294105e-06, 'epoch': 2.5}
 63%|██████▎   | 1185/1892 [3:38:19<10:41:47, 54.47s/it] 63%|██████▎   | 1186/1892 [3:39:13<10:40:25, 54.43s/it]                                                        {'loss': 0.2292, 'learning_rate': 6.45764552506861e-06, 'epoch': 2.51}
 63%|██████▎   | 1186/1892 [3:39:13<10:40:25, 54.43s/it] 63%|██████▎   | 1187/1892 [3:40:08<10:39:47, 54.45s/it]                                                        {'loss': 0.218, 'learning_rate': 6.4416404787634045e-06, 'epoch': 2.51}
 63%|██████▎   | 1187/1892 [3:40:08<10:39:47, 54.45s/it] 63%|██████▎   | 1188/1892 [3:41:02<10:38:38, 54.43s/it]                                                        {'loss': 0.2047, 'learning_rate': 6.425645862290544e-06, 'epoch': 2.51}
 63%|██████▎   | 1188/1892 [3:41:02<10:38:38, 54.43s/it] 63%|██████▎   | 1189/1892 [3:41:57<10:37:52, 54.44s/it]                                                        {'loss': 0.2238, 'learning_rate': 6.409661722531509e-06, 'epoch': 2.51}
 63%|██████▎   | 1189/1892 [3:41:57<10:37:52, 54.44s/it] 63%|██████▎   | 1190/1892 [3:42:51<10:36:49, 54.43s/it]                                                        {'loss': 0.2019, 'learning_rate': 6.3936881063370735e-06, 'epoch': 2.52}
 63%|██████▎   | 1190/1892 [3:42:51<10:36:49, 54.43s/it] 63%|██████▎   | 1191/1892 [3:43:46<10:35:46, 54.42s/it]                                                        {'loss': 0.2217, 'learning_rate': 6.377725060527166e-06, 'epoch': 2.52}
 63%|██████▎   | 1191/1892 [3:43:46<10:35:46, 54.42s/it] 63%|██████▎   | 1192/1892 [3:44:40<10:34:52, 54.42s/it]                                                        {'loss': 0.2125, 'learning_rate': 6.361772631890735e-06, 'epoch': 2.52}
 63%|██████▎   | 1192/1892 [3:44:40<10:34:52, 54.42s/it] 63%|██████▎   | 1193/1892 [3:45:34<10:33:51, 54.41s/it]                                                        {'loss': 0.1971, 'learning_rate': 6.345830867185601e-06, 'epoch': 2.52}
 63%|██████▎   | 1193/1892 [3:45:34<10:33:51, 54.41s/it] 63%|██████▎   | 1194/1892 [3:46:29<10:33:05, 54.42s/it]                                                        {'loss': 0.2234, 'learning_rate': 6.32989981313834e-06, 'epoch': 2.52}
 63%|██████▎   | 1194/1892 [3:46:29<10:33:05, 54.42s/it] 63%|██████▎   | 1195/1892 [3:47:23<10:32:02, 54.41s/it]                                                        {'loss': 0.2127, 'learning_rate': 6.313979516444123e-06, 'epoch': 2.53}
 63%|██████▎   | 1195/1892 [3:47:23<10:32:02, 54.41s/it] 63%|██████▎   | 1196/1892 [3:48:18<10:31:10, 54.41s/it]                                                        {'loss': 0.2117, 'learning_rate': 6.2980700237666005e-06, 'epoch': 2.53}
 63%|██████▎   | 1196/1892 [3:48:18<10:31:10, 54.41s/it] 63%|██████▎   | 1197/1892 [3:49:12<10:30:26, 54.43s/it]                                                        {'loss': 0.1937, 'learning_rate': 6.282171381737742e-06, 'epoch': 2.53}
 63%|██████▎   | 1197/1892 [3:49:12<10:30:26, 54.43s/it] 63%|██████▎   | 1198/1892 [3:50:07<10:29:52, 54.46s/it]                                                        {'loss': 0.2117, 'learning_rate': 6.2662836369577266e-06, 'epoch': 2.53}
 63%|██████▎   | 1198/1892 [3:50:07<10:29:52, 54.46s/it] 63%|██████▎   | 1199/1892 [3:51:01<10:28:25, 54.41s/it]                                                        {'loss': 0.2177, 'learning_rate': 6.250406835994784e-06, 'epoch': 2.53}
 63%|██████▎   | 1199/1892 [3:51:01<10:28:25, 54.41s/it] 63%|██████▎   | 1200/1892 [3:51:55<10:27:47, 54.43s/it]                                                        {'loss': 0.206, 'learning_rate': 6.234541025385075e-06, 'epoch': 2.54}
 63%|██████▎   | 1200/1892 [3:51:55<10:27:47, 54.43s/it] 63%|██████▎   | 1201/1892 [3:52:50<10:27:08, 54.46s/it]                                                        {'loss': 0.2078, 'learning_rate': 6.218686251632533e-06, 'epoch': 2.54}
 63%|██████▎   | 1201/1892 [3:52:50<10:27:08, 54.46s/it] 64%|██████▎   | 1202/1892 [3:53:44<10:26:12, 54.45s/it]                                                        {'loss': 0.1999, 'learning_rate': 6.202842561208759e-06, 'epoch': 2.54}
 64%|██████▎   | 1202/1892 [3:53:44<10:26:12, 54.45s/it] 64%|██████▎   | 1203/1892 [3:54:39<10:25:19, 54.45s/it]                                                        {'loss': 0.2136, 'learning_rate': 6.187010000552851e-06, 'epoch': 2.54}
 64%|██████▎   | 1203/1892 [3:54:39<10:25:19, 54.45s/it] 64%|██████▎   | 1204/1892 [3:55:33<10:24:01, 54.42s/it]                                                        {'loss': 0.2201, 'learning_rate': 6.1711886160712995e-06, 'epoch': 2.54}
 64%|██████▎   | 1204/1892 [3:55:33<10:24:01, 54.42s/it] 64%|██████▎   | 1205/1892 [3:56:27<10:22:37, 54.38s/it]                                                        {'loss': 0.2223, 'learning_rate': 6.155378454137826e-06, 'epoch': 2.55}
 64%|██████▎   | 1205/1892 [3:56:27<10:22:37, 54.38s/it] 64%|██████▎   | 1206/1892 [3:57:22<10:21:31, 54.36s/it]                                                        {'loss': 0.2006, 'learning_rate': 6.139579561093263e-06, 'epoch': 2.55}
 64%|██████▎   | 1206/1892 [3:57:22<10:21:31, 54.36s/it] 64%|██████▍   | 1207/1892 [3:58:16<10:21:33, 54.44s/it]                                                        {'loss': 0.2142, 'learning_rate': 6.123791983245411e-06, 'epoch': 2.55}
 64%|██████▍   | 1207/1892 [3:58:16<10:21:33, 54.44s/it] 64%|██████▍   | 1208/1892 [3:59:11<10:20:35, 54.44s/it]                                                        {'loss': 0.214, 'learning_rate': 6.108015766868906e-06, 'epoch': 2.55}
 64%|██████▍   | 1208/1892 [3:59:11<10:20:35, 54.44s/it] 64%|██████▍   | 1209/1892 [4:00:05<10:19:36, 54.43s/it]                                                        {'loss': 0.2295, 'learning_rate': 6.092250958205085e-06, 'epoch': 2.56}
 64%|██████▍   | 1209/1892 [4:00:05<10:19:36, 54.43s/it] 64%|██████▍   | 1210/1892 [4:01:00<10:18:58, 54.45s/it]                                                        {'loss': 0.2243, 'learning_rate': 6.076497603461844e-06, 'epoch': 2.56}
 64%|██████▍   | 1210/1892 [4:01:00<10:18:58, 54.45s/it] 64%|██████▍   | 1211/1892 [4:01:54<10:17:37, 54.42s/it]                                                        {'loss': 0.2081, 'learning_rate': 6.060755748813503e-06, 'epoch': 2.56}
 64%|██████▍   | 1211/1892 [4:01:54<10:17:37, 54.42s/it] 64%|██████▍   | 1212/1892 [4:02:48<10:16:38, 54.41s/it]                                                        {'loss': 0.1902, 'learning_rate': 6.045025440400684e-06, 'epoch': 2.56}
 64%|██████▍   | 1212/1892 [4:02:48<10:16:38, 54.41s/it] 64%|██████▍   | 1213/1892 [4:03:43<10:15:25, 54.38s/it]                                                        {'loss': 0.217, 'learning_rate': 6.029306724330161e-06, 'epoch': 2.56}
 64%|██████▍   | 1213/1892 [4:03:43<10:15:25, 54.38s/it] 64%|██████▍   | 1214/1892 [4:04:37<10:14:36, 54.39s/it]                                                        {'loss': 0.2337, 'learning_rate': 6.01359964667473e-06, 'epoch': 2.57}
 64%|██████▍   | 1214/1892 [4:04:37<10:14:36, 54.39s/it] 64%|██████▍   | 1215/1892 [4:05:32<10:13:43, 54.39s/it]                                                        {'loss': 0.2145, 'learning_rate': 5.99790425347307e-06, 'epoch': 2.57}
 64%|██████▍   | 1215/1892 [4:05:32<10:13:43, 54.39s/it] 64%|██████▍   | 1216/1892 [4:06:26<10:13:41, 54.47s/it]                                                        {'loss': 0.2141, 'learning_rate': 5.982220590729624e-06, 'epoch': 2.57}
 64%|██████▍   | 1216/1892 [4:06:26<10:13:41, 54.47s/it] 64%|██████▍   | 1217/1892 [4:07:21<10:12:24, 54.44s/it]                                                        {'loss': 0.2348, 'learning_rate': 5.966548704414436e-06, 'epoch': 2.57}
 64%|██████▍   | 1217/1892 [4:07:21<10:12:24, 54.44s/it] 64%|██████▍   | 1218/1892 [4:08:15<10:11:35, 54.44s/it]                                                        {'loss': 0.2198, 'learning_rate': 5.950888640463047e-06, 'epoch': 2.57}
 64%|██████▍   | 1218/1892 [4:08:15<10:11:35, 54.44s/it] 64%|██████▍   | 1219/1892 [4:09:09<10:10:19, 54.41s/it]                                                        {'loss': 0.2115, 'learning_rate': 5.935240444776338e-06, 'epoch': 2.58}
 64%|██████▍   | 1219/1892 [4:09:09<10:10:19, 54.41s/it] 64%|██████▍   | 1220/1892 [4:10:04<10:09:30, 54.42s/it]                                                        {'loss': 0.2099, 'learning_rate': 5.919604163220401e-06, 'epoch': 2.58}
 64%|██████▍   | 1220/1892 [4:10:04<10:09:30, 54.42s/it] 65%|██████▍   | 1221/1892 [4:10:58<10:08:09, 54.38s/it]                                                        {'loss': 0.2073, 'learning_rate': 5.9039798416264145e-06, 'epoch': 2.58}
 65%|██████▍   | 1221/1892 [4:10:58<10:08:09, 54.38s/it] 65%|██████▍   | 1222/1892 [4:11:52<10:06:54, 54.35s/it]                                                        {'loss': 0.2119, 'learning_rate': 5.8883675257904936e-06, 'epoch': 2.58}
 65%|██████▍   | 1222/1892 [4:11:52<10:06:54, 54.35s/it] 65%|██████▍   | 1223/1892 [4:12:47<10:06:45, 54.42s/it]                                                        {'loss': 0.2124, 'learning_rate': 5.872767261473573e-06, 'epoch': 2.59}
 65%|██████▍   | 1223/1892 [4:12:47<10:06:45, 54.42s/it] 65%|██████▍   | 1224/1892 [4:13:42<10:06:27, 54.47s/it]                                                        {'loss': 0.2072, 'learning_rate': 5.857179094401251e-06, 'epoch': 2.59}
 65%|██████▍   | 1224/1892 [4:13:42<10:06:27, 54.47s/it] 65%|██████▍   | 1225/1892 [4:14:36<10:05:26, 54.46s/it]                                                        {'loss': 0.2114, 'learning_rate': 5.841603070263674e-06, 'epoch': 2.59}
 65%|██████▍   | 1225/1892 [4:14:36<10:05:26, 54.46s/it] 65%|██████▍   | 1226/1892 [4:15:30<10:04:22, 54.45s/it]                                                        {'loss': 0.1904, 'learning_rate': 5.826039234715399e-06, 'epoch': 2.59}
 65%|██████▍   | 1226/1892 [4:15:30<10:04:22, 54.45s/it] 65%|██████▍   | 1227/1892 [4:16:25<10:03:29, 54.45s/it]                                                        {'loss': 0.2187, 'learning_rate': 5.810487633375261e-06, 'epoch': 2.59}
 65%|██████▍   | 1227/1892 [4:16:25<10:03:29, 54.45s/it] 65%|██████▍   | 1228/1892 [4:17:19<10:02:22, 54.43s/it]                                                        {'loss': 0.1973, 'learning_rate': 5.79494831182622e-06, 'epoch': 2.6}
 65%|██████▍   | 1228/1892 [4:17:19<10:02:22, 54.43s/it] 65%|██████▍   | 1229/1892 [4:18:14<10:01:48, 54.46s/it]                                                        {'loss': 0.2384, 'learning_rate': 5.779421315615259e-06, 'epoch': 2.6}
 65%|██████▍   | 1229/1892 [4:18:14<10:01:48, 54.46s/it] 65%|██████▌   | 1230/1892 [4:19:08<10:00:34, 54.43s/it]                                                        {'loss': 0.2185, 'learning_rate': 5.763906690253225e-06, 'epoch': 2.6}
 65%|██████▌   | 1230/1892 [4:19:08<10:00:34, 54.43s/it] 65%|██████▌   | 1231/1892 [4:20:03<10:00:33, 54.51s/it]                                                        {'loss': 0.2306, 'learning_rate': 5.748404481214711e-06, 'epoch': 2.6}
 65%|██████▌   | 1231/1892 [4:20:03<10:00:33, 54.51s/it] 65%|██████▌   | 1232/1892 [4:20:57<9:59:25, 54.49s/it]                                                        {'loss': 0.2029, 'learning_rate': 5.732914733937917e-06, 'epoch': 2.6}
 65%|██████▌   | 1232/1892 [4:20:57<9:59:25, 54.49s/it] 65%|██████▌   | 1233/1892 [4:21:52<9:58:38, 54.50s/it]                                                       {'loss': 0.215, 'learning_rate': 5.7174374938245045e-06, 'epoch': 2.61}
 65%|██████▌   | 1233/1892 [4:21:52<9:58:38, 54.50s/it] 65%|██████▌   | 1234/1892 [4:22:46<9:57:12, 54.46s/it]                                                       {'loss': 0.2175, 'learning_rate': 5.7019728062394906e-06, 'epoch': 2.61}
 65%|██████▌   | 1234/1892 [4:22:46<9:57:12, 54.46s/it] 65%|██████▌   | 1235/1892 [4:23:41<9:56:20, 54.46s/it]                                                       {'loss': 0.2319, 'learning_rate': 5.6865207165110945e-06, 'epoch': 2.61}
 65%|██████▌   | 1235/1892 [4:23:41<9:56:20, 54.46s/it] 65%|██████▌   | 1236/1892 [4:24:35<9:55:20, 54.45s/it]                                                       {'loss': 0.2093, 'learning_rate': 5.671081269930612e-06, 'epoch': 2.61}
 65%|██████▌   | 1236/1892 [4:24:35<9:55:20, 54.45s/it] 65%|██████▌   | 1237/1892 [4:25:29<9:53:52, 54.40s/it]                                                       {'loss': 0.2038, 'learning_rate': 5.655654511752274e-06, 'epoch': 2.61}
 65%|██████▌   | 1237/1892 [4:25:29<9:53:52, 54.40s/it] 65%|██████▌   | 1238/1892 [4:26:24<9:52:39, 54.37s/it]                                                       {'loss': 0.2201, 'learning_rate': 5.6402404871931225e-06, 'epoch': 2.62}
 65%|██████▌   | 1238/1892 [4:26:24<9:52:39, 54.37s/it] 65%|██████▌   | 1239/1892 [4:27:18<9:51:48, 54.38s/it]                                                       {'loss': 0.2144, 'learning_rate': 5.624839241432884e-06, 'epoch': 2.62}
 65%|██████▌   | 1239/1892 [4:27:18<9:51:48, 54.38s/it] 66%|██████▌   | 1240/1892 [4:28:12<9:50:42, 54.36s/it]                                                       {'loss': 0.1967, 'learning_rate': 5.609450819613822e-06, 'epoch': 2.62}
 66%|██████▌   | 1240/1892 [4:28:12<9:50:42, 54.36s/it] 66%|██████▌   | 1241/1892 [4:29:07<9:49:47, 54.36s/it]                                                       {'loss': 0.2227, 'learning_rate': 5.594075266840615e-06, 'epoch': 2.62}
 66%|██████▌   | 1241/1892 [4:29:07<9:49:47, 54.36s/it] 66%|██████▌   | 1242/1892 [4:30:01<9:49:16, 54.39s/it]                                                       {'loss': 0.217, 'learning_rate': 5.578712628180225e-06, 'epoch': 2.63}
 66%|██████▌   | 1242/1892 [4:30:01<9:49:16, 54.39s/it] 66%|██████▌   | 1243/1892 [4:30:55<9:47:59, 54.36s/it]                                                       {'loss': 0.2217, 'learning_rate': 5.563362948661748e-06, 'epoch': 2.63}
 66%|██████▌   | 1243/1892 [4:30:55<9:47:59, 54.36s/it] 66%|██████▌   | 1244/1892 [4:31:50<9:46:54, 54.34s/it]                                                       {'loss': 0.2252, 'learning_rate': 5.548026273276312e-06, 'epoch': 2.63}
 66%|██████▌   | 1244/1892 [4:31:50<9:46:54, 54.34s/it] 66%|██████▌   | 1245/1892 [4:32:44<9:46:00, 54.34s/it]                                                       {'loss': 0.231, 'learning_rate': 5.532702646976919e-06, 'epoch': 2.63}
 66%|██████▌   | 1245/1892 [4:32:44<9:46:00, 54.34s/it] 66%|██████▌   | 1246/1892 [4:33:39<9:45:16, 54.36s/it]                                                       {'loss': 0.2073, 'learning_rate': 5.51739211467833e-06, 'epoch': 2.63}
 66%|██████▌   | 1246/1892 [4:33:39<9:45:16, 54.36s/it] 66%|██████▌   | 1247/1892 [4:34:33<9:44:20, 54.36s/it]                                                       {'loss': 0.2274, 'learning_rate': 5.502094721256916e-06, 'epoch': 2.64}
 66%|██████▌   | 1247/1892 [4:34:33<9:44:20, 54.36s/it] 66%|██████▌   | 1248/1892 [4:35:27<9:43:39, 54.38s/it]                                                       {'loss': 0.2339, 'learning_rate': 5.486810511550548e-06, 'epoch': 2.64}
 66%|██████▌   | 1248/1892 [4:35:27<9:43:39, 54.38s/it] 66%|██████▌   | 1249/1892 [4:36:22<9:42:45, 54.38s/it]                                                       {'loss': 0.2188, 'learning_rate': 5.4715395303584475e-06, 'epoch': 2.64}
 66%|██████▌   | 1249/1892 [4:36:22<9:42:45, 54.38s/it] 66%|██████▌   | 1250/1892 [4:37:16<9:41:45, 54.37s/it]                                                       {'loss': 0.2041, 'learning_rate': 5.456281822441065e-06, 'epoch': 2.64}
 66%|██████▌   | 1250/1892 [4:37:16<9:41:45, 54.37s/it] 66%|██████▌   | 1251/1892 [4:38:10<9:40:43, 54.36s/it]                                                       {'loss': 0.21, 'learning_rate': 5.441037432519951e-06, 'epoch': 2.64}
 66%|██████▌   | 1251/1892 [4:38:10<9:40:43, 54.36s/it] 66%|██████▌   | 1252/1892 [4:39:05<9:39:57, 54.37s/it]                                                       {'loss': 0.1962, 'learning_rate': 5.425806405277609e-06, 'epoch': 2.65}
 66%|██████▌   | 1252/1892 [4:39:05<9:39:57, 54.37s/it] 66%|██████▌   | 1253/1892 [4:39:59<9:39:20, 54.40s/it]                                                       {'loss': 0.2374, 'learning_rate': 5.410588785357378e-06, 'epoch': 2.65}
 66%|██████▌   | 1253/1892 [4:39:59<9:39:20, 54.40s/it] 66%|██████▋   | 1254/1892 [4:40:54<9:38:17, 54.39s/it]                                                       {'loss': 0.2061, 'learning_rate': 5.3953846173633064e-06, 'epoch': 2.65}
 66%|██████▋   | 1254/1892 [4:40:54<9:38:17, 54.39s/it] 66%|██████▋   | 1255/1892 [4:41:48<9:37:53, 54.43s/it]                                                       {'loss': 0.2091, 'learning_rate': 5.380193945860007e-06, 'epoch': 2.65}
 66%|██████▋   | 1255/1892 [4:41:48<9:37:53, 54.43s/it] 66%|██████▋   | 1256/1892 [4:42:42<9:36:44, 54.41s/it]                                                       {'loss': 0.237, 'learning_rate': 5.365016815372541e-06, 'epoch': 2.65}
 66%|██████▋   | 1256/1892 [4:42:42<9:36:44, 54.41s/it] 66%|██████▋   | 1257/1892 [4:43:37<9:36:11, 54.44s/it]                                                       {'loss': 0.2186, 'learning_rate': 5.3498532703862685e-06, 'epoch': 2.66}
 66%|██████▋   | 1257/1892 [4:43:37<9:36:11, 54.44s/it] 66%|██████▋   | 1258/1892 [4:44:31<9:34:44, 54.39s/it]                                                       {'loss': 0.2168, 'learning_rate': 5.334703355346737e-06, 'epoch': 2.66}
 66%|██████▋   | 1258/1892 [4:44:31<9:34:44, 54.39s/it] 67%|██████▋   | 1259/1892 [4:45:26<9:33:54, 54.40s/it]                                                       {'loss': 0.2114, 'learning_rate': 5.319567114659543e-06, 'epoch': 2.66}
 67%|██████▋   | 1259/1892 [4:45:26<9:33:54, 54.40s/it] 67%|██████▋   | 1260/1892 [4:46:20<9:33:00, 54.40s/it]                                                       {'loss': 0.2221, 'learning_rate': 5.304444592690201e-06, 'epoch': 2.66}
 67%|██████▋   | 1260/1892 [4:46:20<9:33:00, 54.40s/it] 67%|██████▋   | 1261/1892 [4:47:15<9:32:28, 54.43s/it]                                                       {'loss': 0.2008, 'learning_rate': 5.28933583376402e-06, 'epoch': 2.67}
 67%|██████▋   | 1261/1892 [4:47:15<9:32:28, 54.43s/it] 67%|██████▋   | 1262/1892 [4:48:09<9:31:19, 54.41s/it]                                                       {'loss': 0.2168, 'learning_rate': 5.274240882165958e-06, 'epoch': 2.67}
 67%|██████▋   | 1262/1892 [4:48:09<9:31:19, 54.41s/it] 67%|██████▋   | 1263/1892 [4:49:03<9:30:06, 54.38s/it]                                                       {'loss': 0.2282, 'learning_rate': 5.259159782140507e-06, 'epoch': 2.67}
 67%|██████▋   | 1263/1892 [4:49:03<9:30:06, 54.38s/it] 67%|██████▋   | 1264/1892 [4:49:58<9:29:19, 54.39s/it]                                                       {'loss': 0.2057, 'learning_rate': 5.244092577891564e-06, 'epoch': 2.67}
 67%|██████▋   | 1264/1892 [4:49:58<9:29:19, 54.39s/it] 67%|██████▋   | 1265/1892 [4:50:52<9:28:23, 54.39s/it]                                                       {'loss': 0.2173, 'learning_rate': 5.229039313582298e-06, 'epoch': 2.67}
 67%|██████▋   | 1265/1892 [4:50:52<9:28:23, 54.39s/it] 67%|██████▋   | 1266/1892 [4:51:46<9:27:25, 54.39s/it]                                                       {'loss': 0.2149, 'learning_rate': 5.214000033335006e-06, 'epoch': 2.68}
 67%|██████▋   | 1266/1892 [4:51:46<9:27:25, 54.39s/it] 67%|██████▋   | 1267/1892 [4:52:41<9:26:53, 54.42s/it]                                                       {'loss': 0.1992, 'learning_rate': 5.198974781231003e-06, 'epoch': 2.68}
 67%|██████▋   | 1267/1892 [4:52:41<9:26:53, 54.42s/it] 67%|██████▋   | 1268/1892 [4:53:36<9:27:25, 54.56s/it]                                                       {'loss': 0.1928, 'learning_rate': 5.183963601310491e-06, 'epoch': 2.68}
 67%|██████▋   | 1268/1892 [4:53:36<9:27:25, 54.56s/it] 67%|██████▋   | 1269/1892 [4:54:31<9:26:51, 54.59s/it]                                                       {'loss': 0.2054, 'learning_rate': 5.168966537572421e-06, 'epoch': 2.68}
 67%|██████▋   | 1269/1892 [4:54:31<9:26:51, 54.59s/it] 67%|██████▋   | 1270/1892 [4:55:25<9:25:13, 54.52s/it]                                                       {'loss': 0.21, 'learning_rate': 5.153983633974371e-06, 'epoch': 2.68}
 67%|██████▋   | 1270/1892 [4:55:25<9:25:13, 54.52s/it] 67%|██████▋   | 1271/1892 [4:56:19<9:23:53, 54.48s/it]                                                       {'loss': 0.2015, 'learning_rate': 5.139014934432416e-06, 'epoch': 2.69}
 67%|██████▋   | 1271/1892 [4:56:19<9:23:53, 54.48s/it] 67%|██████▋   | 1272/1892 [4:57:14<9:23:06, 54.49s/it]                                                       {'loss': 0.1974, 'learning_rate': 5.124060482820986e-06, 'epoch': 2.69}
 67%|██████▋   | 1272/1892 [4:57:14<9:23:06, 54.49s/it] 67%|██████▋   | 1273/1892 [4:58:08<9:21:58, 54.47s/it]                                                       {'loss': 0.1948, 'learning_rate': 5.109120322972764e-06, 'epoch': 2.69}
 67%|██████▋   | 1273/1892 [4:58:08<9:21:58, 54.47s/it] 67%|██████▋   | 1274/1892 [4:59:03<9:20:52, 54.45s/it]                                                       {'loss': 0.2273, 'learning_rate': 5.094194498678535e-06, 'epoch': 2.69}
 67%|██████▋   | 1274/1892 [4:59:03<9:20:52, 54.45s/it] 67%|██████▋   | 1275/1892 [4:59:57<9:20:06, 54.47s/it]                                                       {'loss': 0.235, 'learning_rate': 5.07928305368707e-06, 'epoch': 2.69}
 67%|██████▋   | 1275/1892 [4:59:57<9:20:06, 54.47s/it] 67%|██████▋   | 1276/1892 [5:00:52<9:19:17, 54.48s/it]                                                       {'loss': 0.1969, 'learning_rate': 5.064386031704984e-06, 'epoch': 2.7}
 67%|██████▋   | 1276/1892 [5:00:52<9:19:17, 54.48s/it] 67%|██████▋   | 1277/1892 [5:01:46<9:17:55, 54.43s/it]                                                       {'loss': 0.1945, 'learning_rate': 5.049503476396627e-06, 'epoch': 2.7}
 67%|██████▋   | 1277/1892 [5:01:46<9:17:55, 54.43s/it] 68%|██████▊   | 1278/1892 [5:02:40<9:16:51, 54.42s/it]                                                       {'loss': 0.2365, 'learning_rate': 5.034635431383942e-06, 'epoch': 2.7}
 68%|██████▊   | 1278/1892 [5:02:40<9:16:51, 54.42s/it] 68%|██████▊   | 1279/1892 [5:03:35<9:15:45, 54.40s/it]                                                       {'loss': 0.2136, 'learning_rate': 5.019781940246344e-06, 'epoch': 2.7}
 68%|██████▊   | 1279/1892 [5:03:35<9:15:45, 54.40s/it] 68%|██████▊   | 1280/1892 [5:04:29<9:14:49, 54.39s/it]                                                       {'loss': 0.2032, 'learning_rate': 5.004943046520583e-06, 'epoch': 2.71}
 68%|██████▊   | 1280/1892 [5:04:29<9:14:49, 54.39s/it] 68%|██████▊   | 1281/1892 [5:05:23<9:13:50, 54.39s/it]                                                       {'loss': 0.2319, 'learning_rate': 4.990118793700625e-06, 'epoch': 2.71}
 68%|██████▊   | 1281/1892 [5:05:23<9:13:50, 54.39s/it] 68%|██████▊   | 1282/1892 [5:06:18<9:12:51, 54.38s/it]                                                       {'loss': 0.2131, 'learning_rate': 4.9753092252375245e-06, 'epoch': 2.71}
 68%|██████▊   | 1282/1892 [5:06:18<9:12:51, 54.38s/it] 68%|██████▊   | 1283/1892 [5:07:12<9:12:17, 54.41s/it]                                                       {'loss': 0.211, 'learning_rate': 4.960514384539298e-06, 'epoch': 2.71}
 68%|██████▊   | 1283/1892 [5:07:12<9:12:17, 54.41s/it] 68%|██████▊   | 1284/1892 [5:08:07<9:11:08, 54.39s/it]                                                       {'loss': 0.2047, 'learning_rate': 4.945734314970787e-06, 'epoch': 2.71}
 68%|██████▊   | 1284/1892 [5:08:07<9:11:08, 54.39s/it] 68%|██████▊   | 1285/1892 [5:09:01<9:10:25, 54.41s/it]                                                       {'loss': 0.2052, 'learning_rate': 4.930969059853546e-06, 'epoch': 2.72}
 68%|██████▊   | 1285/1892 [5:09:01<9:10:25, 54.41s/it] 68%|██████▊   | 1286/1892 [5:09:56<9:09:43, 54.43s/it]                                                       {'loss': 0.2232, 'learning_rate': 4.916218662465695e-06, 'epoch': 2.72}
 68%|██████▊   | 1286/1892 [5:09:56<9:09:43, 54.43s/it] 68%|██████▊   | 1287/1892 [5:10:50<9:08:52, 54.43s/it]                                                       {'loss': 0.2237, 'learning_rate': 4.901483166041815e-06, 'epoch': 2.72}
 68%|██████▊   | 1287/1892 [5:10:50<9:08:52, 54.43s/it] 68%|██████▊   | 1288/1892 [5:11:44<9:07:41, 54.41s/it]                                                       {'loss': 0.1964, 'learning_rate': 4.886762613772808e-06, 'epoch': 2.72}
 68%|██████▊   | 1288/1892 [5:11:44<9:07:41, 54.41s/it] 68%|██████▊   | 1289/1892 [5:12:39<9:06:53, 54.42s/it]                                                       {'loss': 0.2354, 'learning_rate': 4.872057048805777e-06, 'epoch': 2.72}
 68%|██████▊   | 1289/1892 [5:12:39<9:06:53, 54.42s/it] 68%|██████▊   | 1290/1892 [5:13:33<9:05:45, 54.40s/it]                                                       {'loss': 0.2027, 'learning_rate': 4.857366514243885e-06, 'epoch': 2.73}
 68%|██████▊   | 1290/1892 [5:13:33<9:05:45, 54.40s/it] 68%|██████▊   | 1291/1892 [5:14:28<9:04:59, 54.41s/it]                                                       {'loss': 0.2128, 'learning_rate': 4.842691053146252e-06, 'epoch': 2.73}
 68%|██████▊   | 1291/1892 [5:14:28<9:04:59, 54.41s/it] 68%|██████▊   | 1292/1892 [5:15:22<9:04:23, 54.44s/it]                                                       {'loss': 0.2021, 'learning_rate': 4.828030708527814e-06, 'epoch': 2.73}
 68%|██████▊   | 1292/1892 [5:15:22<9:04:23, 54.44s/it] 68%|██████▊   | 1293/1892 [5:16:17<9:03:30, 54.44s/it]                                                       {'loss': 0.209, 'learning_rate': 4.813385523359191e-06, 'epoch': 2.73}
 68%|██████▊   | 1293/1892 [5:16:17<9:03:30, 54.44s/it] 68%|██████▊   | 1294/1892 [5:17:11<9:02:40, 54.45s/it]                                                       {'loss': 0.2216, 'learning_rate': 4.7987555405665776e-06, 'epoch': 2.73}
 68%|██████▊   | 1294/1892 [5:17:11<9:02:40, 54.45s/it] 68%|██████▊   | 1295/1892 [5:18:05<9:01:43, 54.44s/it]                                                       {'loss': 0.2363, 'learning_rate': 4.784140803031612e-06, 'epoch': 2.74}
 68%|██████▊   | 1295/1892 [5:18:05<9:01:43, 54.44s/it] 68%|██████▊   | 1296/1892 [5:19:00<9:00:24, 54.40s/it]                                                       {'loss': 0.1961, 'learning_rate': 4.7695413535912335e-06, 'epoch': 2.74}
 68%|██████▊   | 1296/1892 [5:19:00<9:00:24, 54.40s/it] 69%|██████▊   | 1297/1892 [5:19:54<8:59:20, 54.39s/it]                                                       {'loss': 0.2143, 'learning_rate': 4.7549572350375864e-06, 'epoch': 2.74}
 69%|██████▊   | 1297/1892 [5:19:54<8:59:20, 54.39s/it] 69%|██████▊   | 1298/1892 [5:20:48<8:58:24, 54.38s/it]                                                       {'loss': 0.1925, 'learning_rate': 4.740388490117869e-06, 'epoch': 2.74}
 69%|██████▊   | 1298/1892 [5:20:48<8:58:24, 54.38s/it] 69%|██████▊   | 1299/1892 [5:21:43<8:57:20, 54.37s/it]                                                       {'loss': 0.2025, 'learning_rate': 4.725835161534226e-06, 'epoch': 2.75}
 69%|██████▊   | 1299/1892 [5:21:43<8:57:20, 54.37s/it] 69%|██████▊   | 1300/1892 [5:22:37<8:56:24, 54.37s/it]                                                       {'loss': 0.2046, 'learning_rate': 4.711297291943604e-06, 'epoch': 2.75}
 69%|██████▊   | 1300/1892 [5:22:37<8:56:24, 54.37s/it] 69%|██████▉   | 1301/1892 [5:23:32<8:55:43, 54.39s/it]                                                       {'loss': 0.2272, 'learning_rate': 4.696774923957649e-06, 'epoch': 2.75}
 69%|██████▉   | 1301/1892 [5:23:32<8:55:43, 54.39s/it] 69%|██████▉   | 1302/1892 [5:24:26<8:55:22, 54.44s/it]                                                       {'loss': 0.1948, 'learning_rate': 4.682268100142567e-06, 'epoch': 2.75}
 69%|██████▉   | 1302/1892 [5:24:26<8:55:22, 54.44s/it] 69%|██████▉   | 1303/1892 [5:25:20<8:54:01, 54.40s/it]                                                       {'loss': 0.2098, 'learning_rate': 4.667776863019e-06, 'epoch': 2.75}
 69%|██████▉   | 1303/1892 [5:25:20<8:54:01, 54.40s/it] 69%|██████▉   | 1304/1892 [5:26:15<8:53:04, 54.40s/it]                                                       {'loss': 0.2034, 'learning_rate': 4.653301255061914e-06, 'epoch': 2.76}
 69%|██████▉   | 1304/1892 [5:26:15<8:53:04, 54.40s/it] 69%|██████▉   | 1305/1892 [5:27:09<8:51:49, 54.36s/it]                                                       {'loss': 0.1947, 'learning_rate': 4.638841318700448e-06, 'epoch': 2.76}
 69%|██████▉   | 1305/1892 [5:27:09<8:51:49, 54.36s/it] 69%|██████▉   | 1306/1892 [5:28:03<8:50:49, 54.35s/it]                                                       {'loss': 0.2067, 'learning_rate': 4.6243970963178235e-06, 'epoch': 2.76}
 69%|██████▉   | 1306/1892 [5:28:03<8:50:49, 54.35s/it] 69%|██████▉   | 1307/1892 [5:28:58<8:49:37, 54.32s/it]                                                       {'loss': 0.2001, 'learning_rate': 4.609968630251187e-06, 'epoch': 2.76}
 69%|██████▉   | 1307/1892 [5:28:58<8:49:37, 54.32s/it] 69%|██████▉   | 1308/1892 [5:29:52<8:48:46, 54.33s/it]                                                       {'loss': 0.2238, 'learning_rate': 4.595555962791515e-06, 'epoch': 2.76}
 69%|██████▉   | 1308/1892 [5:29:52<8:48:46, 54.33s/it] 69%|██████▉   | 1309/1892 [5:30:46<8:47:34, 54.30s/it]                                                       {'loss': 0.23, 'learning_rate': 4.581159136183475e-06, 'epoch': 2.77}
 69%|██████▉   | 1309/1892 [5:30:46<8:47:34, 54.30s/it] 69%|██████▉   | 1310/1892 [5:31:41<8:46:50, 54.31s/it]                                                       {'loss': 0.2128, 'learning_rate': 4.566778192625296e-06, 'epoch': 2.77}
 69%|██████▉   | 1310/1892 [5:31:41<8:46:50, 54.31s/it] 69%|██████▉   | 1311/1892 [5:32:35<8:46:07, 54.33s/it]                                                       {'loss': 0.2358, 'learning_rate': 4.552413174268658e-06, 'epoch': 2.77}
 69%|██████▉   | 1311/1892 [5:32:35<8:46:07, 54.33s/it] 69%|██████▉   | 1312/1892 [5:33:29<8:45:17, 54.34s/it]                                                       {'loss': 0.2024, 'learning_rate': 4.538064123218565e-06, 'epoch': 2.77}
 69%|██████▉   | 1312/1892 [5:33:29<8:45:17, 54.34s/it] 69%|██████▉   | 1313/1892 [5:34:24<8:44:22, 54.34s/it]                                                       {'loss': 0.1908, 'learning_rate': 4.5237310815332165e-06, 'epoch': 2.78}
 69%|██████▉   | 1313/1892 [5:34:24<8:44:22, 54.34s/it] 69%|██████▉   | 1314/1892 [5:35:18<8:43:58, 54.39s/it]                                                       {'loss': 0.2265, 'learning_rate': 4.50941409122389e-06, 'epoch': 2.78}
 69%|██████▉   | 1314/1892 [5:35:18<8:43:58, 54.39s/it] 70%|██████▉   | 1315/1892 [5:36:12<8:42:40, 54.35s/it]                                                       {'loss': 0.2103, 'learning_rate': 4.4951131942548084e-06, 'epoch': 2.78}
 70%|██████▉   | 1315/1892 [5:36:12<8:42:40, 54.35s/it] 70%|██████▉   | 1316/1892 [5:37:07<8:41:53, 54.36s/it]                                                       {'loss': 0.1944, 'learning_rate': 4.48082843254303e-06, 'epoch': 2.78}
 70%|██████▉   | 1316/1892 [5:37:07<8:41:53, 54.36s/it] 70%|██████▉   | 1317/1892 [5:38:01<8:40:56, 54.36s/it]                                                       {'loss': 0.2347, 'learning_rate': 4.466559847958318e-06, 'epoch': 2.78}
 70%|██████▉   | 1317/1892 [5:38:01<8:40:56, 54.36s/it] 70%|██████▉   | 1318/1892 [5:38:56<8:41:22, 54.50s/it]                                                       {'loss': 0.1998, 'learning_rate': 4.452307482323024e-06, 'epoch': 2.79}
 70%|██████▉   | 1318/1892 [5:38:56<8:41:22, 54.50s/it] 70%|██████▉   | 1319/1892 [5:39:50<8:40:03, 54.46s/it]                                                       {'loss': 0.2035, 'learning_rate': 4.438071377411946e-06, 'epoch': 2.79}
 70%|██████▉   | 1319/1892 [5:39:50<8:40:03, 54.46s/it] 70%|██████▉   | 1320/1892 [5:40:45<8:38:50, 54.42s/it]                                                       {'loss': 0.1966, 'learning_rate': 4.4238515749522395e-06, 'epoch': 2.79}
 70%|██████▉   | 1320/1892 [5:40:45<8:38:50, 54.42s/it] 70%|██████▉   | 1321/1892 [5:41:39<8:37:42, 54.40s/it]                                                       {'loss': 0.2192, 'learning_rate': 4.409648116623258e-06, 'epoch': 2.79}
 70%|██████▉   | 1321/1892 [5:41:39<8:37:42, 54.40s/it] 70%|██████▉   | 1322/1892 [5:42:33<8:36:32, 54.37s/it]                                                       {'loss': 0.207, 'learning_rate': 4.395461044056462e-06, 'epoch': 2.79}
 70%|██████▉   | 1322/1892 [5:42:33<8:36:32, 54.37s/it] 70%|██████▉   | 1323/1892 [5:43:28<8:35:18, 54.34s/it]                                                       {'loss': 0.2035, 'learning_rate': 4.3812903988352795e-06, 'epoch': 2.8}
 70%|██████▉   | 1323/1892 [5:43:28<8:35:18, 54.34s/it] 70%|██████▉   | 1324/1892 [5:44:22<8:34:26, 54.34s/it]                                                       {'loss': 0.2271, 'learning_rate': 4.367136222494993e-06, 'epoch': 2.8}
 70%|██████▉   | 1324/1892 [5:44:22<8:34:26, 54.34s/it] 70%|███████   | 1325/1892 [5:45:16<8:33:39, 54.36s/it]                                                       {'loss': 0.2003, 'learning_rate': 4.352998556522603e-06, 'epoch': 2.8}
 70%|███████   | 1325/1892 [5:45:16<8:33:39, 54.36s/it] 70%|███████   | 1326/1892 [5:46:11<8:32:51, 54.37s/it]                                                       {'loss': 0.185, 'learning_rate': 4.338877442356725e-06, 'epoch': 2.8}
 70%|███████   | 1326/1892 [5:46:11<8:32:51, 54.37s/it] 70%|███████   | 1327/1892 [5:47:05<8:31:45, 54.35s/it]                                                       {'loss': 0.2269, 'learning_rate': 4.32477292138746e-06, 'epoch': 2.8}
 70%|███████   | 1327/1892 [5:47:05<8:31:45, 54.35s/it] 70%|███████   | 1328/1892 [5:48:00<8:31:10, 54.38s/it]                                                       {'loss': 0.2141, 'learning_rate': 4.3106850349562746e-06, 'epoch': 2.81}
 70%|███████   | 1328/1892 [5:48:00<8:31:10, 54.38s/it] 70%|███████   | 1329/1892 [5:48:54<8:30:11, 54.37s/it]                                                       {'loss': 0.1956, 'learning_rate': 4.296613824355866e-06, 'epoch': 2.81}
 70%|███████   | 1329/1892 [5:48:54<8:30:11, 54.37s/it] 70%|███████   | 1330/1892 [5:49:48<8:29:17, 54.37s/it]                                                       {'loss': 0.2037, 'learning_rate': 4.282559330830068e-06, 'epoch': 2.81}
 70%|███████   | 1330/1892 [5:49:48<8:29:17, 54.37s/it] 70%|███████   | 1331/1892 [5:50:43<8:28:17, 54.36s/it]                                                       {'loss': 0.207, 'learning_rate': 4.268521595573706e-06, 'epoch': 2.81}
 70%|███████   | 1331/1892 [5:50:43<8:28:17, 54.36s/it] 70%|███████   | 1332/1892 [5:51:37<8:26:55, 54.31s/it]                                                       {'loss': 0.2118, 'learning_rate': 4.254500659732496e-06, 'epoch': 2.82}
 70%|███████   | 1332/1892 [5:51:37<8:26:55, 54.31s/it] 70%|███████   | 1333/1892 [5:52:31<8:26:03, 54.32s/it]                                                       {'loss': 0.2289, 'learning_rate': 4.240496564402897e-06, 'epoch': 2.82}
 70%|███████   | 1333/1892 [5:52:31<8:26:03, 54.32s/it] 71%|███████   | 1334/1892 [5:53:25<8:25:02, 54.31s/it]                                                       {'loss': 0.2072, 'learning_rate': 4.226509350632025e-06, 'epoch': 2.82}
 71%|███████   | 1334/1892 [5:53:25<8:25:02, 54.31s/it] 71%|███████   | 1335/1892 [5:54:20<8:24:21, 54.33s/it]                                                       {'loss': 0.2095, 'learning_rate': 4.2125390594174985e-06, 'epoch': 2.82}
 71%|███████   | 1335/1892 [5:54:20<8:24:21, 54.33s/it] 71%|███████   | 1336/1892 [5:55:14<8:23:33, 54.34s/it]                                                       {'loss': 0.2278, 'learning_rate': 4.198585731707348e-06, 'epoch': 2.82}
 71%|███████   | 1336/1892 [5:55:14<8:23:33, 54.34s/it] 71%|███████   | 1337/1892 [5:56:09<8:22:44, 54.35s/it]                                                       {'loss': 0.1922, 'learning_rate': 4.184649408399876e-06, 'epoch': 2.83}
 71%|███████   | 1337/1892 [5:56:09<8:22:44, 54.35s/it] 71%|███████   | 1338/1892 [5:57:03<8:21:55, 54.36s/it]                                                       {'loss': 0.2066, 'learning_rate': 4.170730130343548e-06, 'epoch': 2.83}
 71%|███████   | 1338/1892 [5:57:03<8:21:55, 54.36s/it] 71%|███████   | 1339/1892 [5:57:57<8:21:36, 54.42s/it]                                                       {'loss': 0.2028, 'learning_rate': 4.156827938336859e-06, 'epoch': 2.83}
 71%|███████   | 1339/1892 [5:57:58<8:21:36, 54.42s/it] 71%|███████   | 1340/1892 [5:58:52<8:20:57, 54.45s/it]                                                       {'loss': 0.2078, 'learning_rate': 4.142942873128234e-06, 'epoch': 2.83}
 71%|███████   | 1340/1892 [5:58:52<8:20:57, 54.45s/it] 71%|███████   | 1341/1892 [5:59:46<8:20:01, 54.45s/it]                                                       {'loss': 0.2112, 'learning_rate': 4.1290749754158936e-06, 'epoch': 2.83}
 71%|███████   | 1341/1892 [5:59:46<8:20:01, 54.45s/it] 71%|███████   | 1342/1892 [6:00:41<8:18:51, 54.42s/it]                                                       {'loss': 0.2005, 'learning_rate': 4.1152242858477435e-06, 'epoch': 2.84}
 71%|███████   | 1342/1892 [6:00:41<8:18:51, 54.42s/it] 71%|███████   | 1343/1892 [6:01:35<8:17:42, 54.39s/it]                                                       {'loss': 0.2313, 'learning_rate': 4.1013908450212385e-06, 'epoch': 2.84}
 71%|███████   | 1343/1892 [6:01:35<8:17:42, 54.39s/it] 71%|███████   | 1344/1892 [6:02:30<8:16:47, 54.39s/it]                                                       {'loss': 0.2127, 'learning_rate': 4.087574693483289e-06, 'epoch': 2.84}
 71%|███████   | 1344/1892 [6:02:30<8:16:47, 54.39s/it] 71%|███████   | 1345/1892 [6:03:24<8:15:57, 54.40s/it]                                                       {'loss': 0.2116, 'learning_rate': 4.0737758717301256e-06, 'epoch': 2.84}
 71%|███████   | 1345/1892 [6:03:24<8:15:57, 54.40s/it] 71%|███████   | 1346/1892 [6:04:18<8:14:54, 54.39s/it]                                                       {'loss': 0.2024, 'learning_rate': 4.0599944202071815e-06, 'epoch': 2.84}
 71%|███████   | 1346/1892 [6:04:18<8:14:54, 54.39s/it] 71%|███████   | 1347/1892 [6:05:13<8:14:01, 54.39s/it]                                                       {'loss': 0.1836, 'learning_rate': 4.046230379308982e-06, 'epoch': 2.85}
 71%|███████   | 1347/1892 [6:05:13<8:14:01, 54.39s/it] 71%|███████   | 1348/1892 [6:06:07<8:13:07, 54.39s/it]                                                       {'loss': 0.1955, 'learning_rate': 4.032483789379013e-06, 'epoch': 2.85}
 71%|███████   | 1348/1892 [6:06:07<8:13:07, 54.39s/it] 71%|███████▏  | 1349/1892 [6:07:02<8:12:48, 54.45s/it]                                                       {'loss': 0.2215, 'learning_rate': 4.018754690709609e-06, 'epoch': 2.85}
 71%|███████▏  | 1349/1892 [6:07:02<8:12:48, 54.45s/it] 71%|███████▏  | 1350/1892 [6:07:56<8:11:51, 54.45s/it]                                                       {'loss': 0.2048, 'learning_rate': 4.005043123541845e-06, 'epoch': 2.85}
 71%|███████▏  | 1350/1892 [6:07:56<8:11:51, 54.45s/it] 71%|███████▏  | 1351/1892 [6:08:50<8:10:36, 54.41s/it]                                                       {'loss': 0.1959, 'learning_rate': 3.991349128065406e-06, 'epoch': 2.86}
 71%|███████▏  | 1351/1892 [6:08:50<8:10:36, 54.41s/it] 71%|███████▏  | 1352/1892 [6:09:45<8:09:24, 54.38s/it]                                                       {'loss': 0.2241, 'learning_rate': 3.977672744418475e-06, 'epoch': 2.86}
 71%|███████▏  | 1352/1892 [6:09:45<8:09:24, 54.38s/it] 72%|███████▏  | 1353/1892 [6:10:39<8:08:21, 54.36s/it]                                                       {'loss': 0.2315, 'learning_rate': 3.9640140126876045e-06, 'epoch': 2.86}
 72%|███████▏  | 1353/1892 [6:10:39<8:08:21, 54.36s/it] 72%|███████▏  | 1354/1892 [6:11:33<8:07:28, 54.37s/it]                                                       {'loss': 0.2017, 'learning_rate': 3.950372972907618e-06, 'epoch': 2.86}
 72%|███████▏  | 1354/1892 [6:11:33<8:07:28, 54.37s/it] 72%|███████▏  | 1355/1892 [6:12:28<8:07:05, 54.42s/it]                                                       {'loss': 0.2218, 'learning_rate': 3.936749665061478e-06, 'epoch': 2.86}
 72%|███████▏  | 1355/1892 [6:12:28<8:07:05, 54.42s/it] 72%|███████▏  | 1356/1892 [6:13:22<8:05:52, 54.39s/it]                                                       {'loss': 0.221, 'learning_rate': 3.923144129080174e-06, 'epoch': 2.87}
 72%|███████▏  | 1356/1892 [6:13:22<8:05:52, 54.39s/it] 72%|███████▏  | 1357/1892 [6:14:17<8:05:37, 54.46s/it]                                                       {'loss': 0.192, 'learning_rate': 3.909556404842609e-06, 'epoch': 2.87}
 72%|███████▏  | 1357/1892 [6:14:17<8:05:37, 54.46s/it] 72%|███████▏  | 1358/1892 [6:15:11<8:04:52, 54.48s/it]                                                       {'loss': 0.2269, 'learning_rate': 3.8959865321754674e-06, 'epoch': 2.87}
 72%|███████▏  | 1358/1892 [6:15:11<8:04:52, 54.48s/it] 72%|███████▏  | 1359/1892 [6:16:06<8:03:48, 54.46s/it]                                                       {'loss': 0.1892, 'learning_rate': 3.882434550853119e-06, 'epoch': 2.87}
 72%|███████▏  | 1359/1892 [6:16:06<8:03:48, 54.46s/it] 72%|███████▏  | 1360/1892 [6:17:00<8:02:43, 54.44s/it]                                                       {'loss': 0.2072, 'learning_rate': 3.86890050059749e-06, 'epoch': 2.87}
 72%|███████▏  | 1360/1892 [6:17:00<8:02:43, 54.44s/it] 72%|███████▏  | 1361/1892 [6:17:55<8:01:46, 54.44s/it]                                                       {'loss': 0.2003, 'learning_rate': 3.855384421077951e-06, 'epoch': 2.88}
 72%|███████▏  | 1361/1892 [6:17:55<8:01:46, 54.44s/it] 72%|███████▏  | 1362/1892 [6:18:49<8:00:54, 54.44s/it]                                                       {'loss': 0.2386, 'learning_rate': 3.841886351911195e-06, 'epoch': 2.88}
 72%|███████▏  | 1362/1892 [6:18:49<8:00:54, 54.44s/it] 72%|███████▏  | 1363/1892 [6:19:44<7:59:53, 54.43s/it]                                                       {'loss': 0.1865, 'learning_rate': 3.828406332661123e-06, 'epoch': 2.88}
 72%|███████▏  | 1363/1892 [6:19:44<7:59:53, 54.43s/it] 72%|███████▏  | 1364/1892 [6:20:38<7:59:07, 54.45s/it]                                                       {'loss': 0.192, 'learning_rate': 3.814944402838738e-06, 'epoch': 2.88}
 72%|███████▏  | 1364/1892 [6:20:38<7:59:07, 54.45s/it] 72%|███████▏  | 1365/1892 [6:21:32<7:58:05, 54.43s/it]                                                       {'loss': 0.2049, 'learning_rate': 3.801500601902017e-06, 'epoch': 2.88}
 72%|███████▏  | 1365/1892 [6:21:32<7:58:05, 54.43s/it] 72%|███████▏  | 1366/1892 [6:22:27<7:57:28, 54.46s/it]                                                       {'loss': 0.1955, 'learning_rate': 3.7880749692558026e-06, 'epoch': 2.89}
 72%|███████▏  | 1366/1892 [6:22:27<7:57:28, 54.46s/it] 72%|███████▏  | 1367/1892 [6:23:21<7:56:29, 54.46s/it]                                                       {'loss': 0.2157, 'learning_rate': 3.774667544251683e-06, 'epoch': 2.89}
 72%|███████▏  | 1367/1892 [6:23:21<7:56:29, 54.46s/it] 72%|███████▏  | 1368/1892 [6:24:16<7:55:28, 54.44s/it]                                                       {'loss': 0.1937, 'learning_rate': 3.761278366187875e-06, 'epoch': 2.89}
 72%|███████▏  | 1368/1892 [6:24:16<7:55:28, 54.44s/it] 72%|███████▏  | 1369/1892 [6:25:10<7:54:31, 54.44s/it]                                                       {'loss': 0.2128, 'learning_rate': 3.747907474309116e-06, 'epoch': 2.89}
 72%|███████▏  | 1369/1892 [6:25:10<7:54:31, 54.44s/it] 72%|███████▏  | 1370/1892 [6:26:05<7:53:38, 54.44s/it]                                                       {'loss': 0.1998, 'learning_rate': 3.734554907806546e-06, 'epoch': 2.9}
 72%|███████▏  | 1370/1892 [6:26:05<7:53:38, 54.44s/it] 72%|███████▏  | 1371/1892 [6:26:59<7:52:39, 54.43s/it]                                                       {'loss': 0.21, 'learning_rate': 3.721220705817592e-06, 'epoch': 2.9}
 72%|███████▏  | 1371/1892 [6:26:59<7:52:39, 54.43s/it] 73%|███████▎  | 1372/1892 [6:27:54<7:51:41, 54.43s/it]                                                       {'loss': 0.2137, 'learning_rate': 3.7079049074258465e-06, 'epoch': 2.9}
 73%|███████▎  | 1372/1892 [6:27:54<7:51:41, 54.43s/it] 73%|███████▎  | 1373/1892 [6:28:48<7:50:31, 54.40s/it]                                                       {'loss': 0.1994, 'learning_rate': 3.6946075516609658e-06, 'epoch': 2.9}
 73%|███████▎  | 1373/1892 [6:28:48<7:50:31, 54.40s/it] 73%|███████▎  | 1374/1892 [6:29:42<7:50:02, 54.45s/it]                                                       {'loss': 0.2017, 'learning_rate': 3.6813286774985513e-06, 'epoch': 2.9}
 73%|███████▎  | 1374/1892 [6:29:42<7:50:02, 54.45s/it] 73%|███████▎  | 1375/1892 [6:30:37<7:49:07, 54.44s/it]                                                       {'loss': 0.2059, 'learning_rate': 3.6680683238600303e-06, 'epoch': 2.91}
 73%|███████▎  | 1375/1892 [6:30:37<7:49:07, 54.44s/it] 73%|███████▎  | 1376/1892 [6:31:31<7:48:11, 54.44s/it]                                                       {'loss': 0.2013, 'learning_rate': 3.654826529612541e-06, 'epoch': 2.91}
 73%|███████▎  | 1376/1892 [6:31:31<7:48:11, 54.44s/it] 73%|███████▎  | 1377/1892 [6:32:26<7:47:18, 54.44s/it]                                                       {'loss': 0.2307, 'learning_rate': 3.6416033335688306e-06, 'epoch': 2.91}
 73%|███████▎  | 1377/1892 [6:32:26<7:47:18, 54.44s/it] 73%|███████▎  | 1378/1892 [6:33:20<7:46:19, 54.43s/it]                                                       {'loss': 0.1927, 'learning_rate': 3.628398774487125e-06, 'epoch': 2.91}
 73%|███████▎  | 1378/1892 [6:33:20<7:46:19, 54.43s/it] 73%|███████▎  | 1379/1892 [6:34:15<7:45:33, 54.45s/it]                                                       {'loss': 0.1951, 'learning_rate': 3.6152128910710305e-06, 'epoch': 2.91}
 73%|███████▎  | 1379/1892 [6:34:15<7:45:33, 54.45s/it] 73%|███████▎  | 1380/1892 [6:35:09<7:44:42, 54.46s/it]                                                       {'loss': 0.2002, 'learning_rate': 3.6020457219694103e-06, 'epoch': 2.92}
 73%|███████▎  | 1380/1892 [6:35:09<7:44:42, 54.46s/it] 73%|███████▎  | 1381/1892 [6:36:04<7:44:14, 54.51s/it]                                                       {'loss': 0.2148, 'learning_rate': 3.5888973057762798e-06, 'epoch': 2.92}
 73%|███████▎  | 1381/1892 [6:36:04<7:44:14, 54.51s/it] 73%|███████▎  | 1382/1892 [6:36:58<7:43:04, 54.48s/it]                                                       {'loss': 0.2266, 'learning_rate': 3.5757676810306775e-06, 'epoch': 2.92}
 73%|███████▎  | 1382/1892 [6:36:58<7:43:04, 54.48s/it] 73%|███████▎  | 1383/1892 [6:37:53<7:42:26, 54.51s/it]                                                       {'loss': 0.2182, 'learning_rate': 3.56265688621657e-06, 'epoch': 2.92}
 73%|███████▎  | 1383/1892 [6:37:53<7:42:26, 54.51s/it] 73%|███████▎  | 1384/1892 [6:38:47<7:41:27, 54.50s/it]                                                       {'loss': 0.1974, 'learning_rate': 3.5495649597627323e-06, 'epoch': 2.93}
 73%|███████▎  | 1384/1892 [6:38:47<7:41:27, 54.50s/it] 73%|███████▎  | 1385/1892 [6:39:42<7:40:18, 54.48s/it]                                                       {'loss': 0.2138, 'learning_rate': 3.5364919400426366e-06, 'epoch': 2.93}
 73%|███████▎  | 1385/1892 [6:39:42<7:40:18, 54.48s/it] 73%|███████▎  | 1386/1892 [6:40:36<7:39:18, 54.46s/it]                                                       {'loss': 0.2134, 'learning_rate': 3.523437865374327e-06, 'epoch': 2.93}
 73%|███████▎  | 1386/1892 [6:40:36<7:39:18, 54.46s/it] 73%|███████▎  | 1387/1892 [6:41:31<7:38:18, 54.45s/it]                                                       {'loss': 0.209, 'learning_rate': 3.5104027740203305e-06, 'epoch': 2.93}
 73%|███████▎  | 1387/1892 [6:41:31<7:38:18, 54.45s/it] 73%|███████▎  | 1388/1892 [6:42:25<7:37:17, 54.44s/it]                                                       {'loss': 0.2021, 'learning_rate': 3.4973867041875253e-06, 'epoch': 2.93}
 73%|███████▎  | 1388/1892 [6:42:25<7:37:17, 54.44s/it] 73%|███████▎  | 1389/1892 [6:43:19<7:36:36, 54.47s/it]                                                       {'loss': 0.187, 'learning_rate': 3.4843896940270438e-06, 'epoch': 2.94}
 73%|███████▎  | 1389/1892 [6:43:19<7:36:36, 54.47s/it] 73%|███████▎  | 1390/1892 [6:44:14<7:35:11, 54.41s/it]                                                       {'loss': 0.2067, 'learning_rate': 3.4714117816341407e-06, 'epoch': 2.94}
 73%|███████▎  | 1390/1892 [6:44:14<7:35:11, 54.41s/it] 74%|███████▎  | 1391/1892 [6:45:08<7:34:17, 54.41s/it]                                                       {'loss': 0.2163, 'learning_rate': 3.458453005048107e-06, 'epoch': 2.94}
 74%|███████▎  | 1391/1892 [6:45:08<7:34:17, 54.41s/it] 74%|███████▎  | 1392/1892 [6:46:03<7:33:38, 54.44s/it]                                                       {'loss': 0.2222, 'learning_rate': 3.445513402252132e-06, 'epoch': 2.94}
 74%|███████▎  | 1392/1892 [6:46:03<7:33:38, 54.44s/it] 74%|███████▎  | 1393/1892 [6:46:57<7:32:27, 54.40s/it]                                                       {'loss': 0.202, 'learning_rate': 3.4325930111732154e-06, 'epoch': 2.94}
 74%|███████▎  | 1393/1892 [6:46:57<7:32:27, 54.40s/it] 74%|███████▎  | 1394/1892 [6:47:51<7:31:26, 54.39s/it]                                                       {'loss': 0.1859, 'learning_rate': 3.419691869682041e-06, 'epoch': 2.95}
 74%|███████▎  | 1394/1892 [6:47:51<7:31:26, 54.39s/it] 74%|███████▎  | 1395/1892 [6:48:46<7:30:05, 54.34s/it]                                                       {'loss': 0.2245, 'learning_rate': 3.406810015592876e-06, 'epoch': 2.95}
 74%|███████▎  | 1395/1892 [6:48:46<7:30:05, 54.34s/it] 74%|███████▍  | 1396/1892 [6:49:40<7:28:48, 54.29s/it]                                                       {'loss': 0.2127, 'learning_rate': 3.393947486663445e-06, 'epoch': 2.95}
 74%|███████▍  | 1396/1892 [6:49:40<7:28:48, 54.29s/it] 74%|███████▍  | 1397/1892 [6:50:34<7:27:43, 54.27s/it]                                                       {'loss': 0.2154, 'learning_rate': 3.3811043205948366e-06, 'epoch': 2.95}
 74%|███████▍  | 1397/1892 [6:50:34<7:27:43, 54.27s/it] 74%|███████▍  | 1398/1892 [6:51:28<7:26:53, 54.28s/it]                                                       {'loss': 0.2272, 'learning_rate': 3.3682805550313837e-06, 'epoch': 2.95}
 74%|███████▍  | 1398/1892 [6:51:28<7:26:53, 54.28s/it] 74%|███████▍  | 1399/1892 [6:52:23<7:25:57, 54.27s/it]                                                       {'loss': 0.227, 'learning_rate': 3.3554762275605534e-06, 'epoch': 2.96}
 74%|███████▍  | 1399/1892 [6:52:23<7:25:57, 54.27s/it] 74%|███████▍  | 1400/1892 [6:53:17<7:24:55, 54.26s/it]                                                       {'loss': 0.2124, 'learning_rate': 3.3426913757128434e-06, 'epoch': 2.96}
 74%|███████▍  | 1400/1892 [6:53:17<7:24:55, 54.26s/it] 74%|███████▍  | 1401/1892 [6:54:11<7:23:59, 54.26s/it]                                                       {'loss': 0.2354, 'learning_rate': 3.329926036961656e-06, 'epoch': 2.96}
 74%|███████▍  | 1401/1892 [6:54:11<7:23:59, 54.26s/it] 74%|███████▍  | 1402/1892 [6:55:05<7:23:10, 54.27s/it]                                                       {'loss': 0.2306, 'learning_rate': 3.3171802487232087e-06, 'epoch': 2.96}
 74%|███████▍  | 1402/1892 [6:55:05<7:23:10, 54.27s/it] 74%|███████▍  | 1403/1892 [6:55:59<7:22:07, 54.25s/it]                                                       {'loss': 0.2346, 'learning_rate': 3.3044540483564146e-06, 'epoch': 2.97}
 74%|███████▍  | 1403/1892 [6:55:59<7:22:07, 54.25s/it] 74%|███████▍  | 1404/1892 [6:56:54<7:21:09, 54.24s/it]                                                       {'loss': 0.2045, 'learning_rate': 3.2917474731627653e-06, 'epoch': 2.97}
 74%|███████▍  | 1404/1892 [6:56:54<7:21:09, 54.24s/it] 74%|███████▍  | 1405/1892 [6:57:48<7:20:15, 54.24s/it]                                                       {'loss': 0.2101, 'learning_rate': 3.2790605603862404e-06, 'epoch': 2.97}
 74%|███████▍  | 1405/1892 [6:57:48<7:20:15, 54.24s/it] 74%|███████▍  | 1406/1892 [6:58:42<7:19:00, 54.20s/it]                                                       {'loss': 0.2102, 'learning_rate': 3.266393347213176e-06, 'epoch': 2.97}
 74%|███████▍  | 1406/1892 [6:58:42<7:19:00, 54.20s/it] 74%|███████▍  | 1407/1892 [6:59:36<7:18:18, 54.22s/it]                                                       {'loss': 0.2198, 'learning_rate': 3.2537458707721735e-06, 'epoch': 2.97}
 74%|███████▍  | 1407/1892 [6:59:36<7:18:18, 54.22s/it] 74%|███████▍  | 1408/1892 [7:00:31<7:17:26, 54.23s/it]                                                       {'loss': 0.2142, 'learning_rate': 3.2411181681339867e-06, 'epoch': 2.98}
 74%|███████▍  | 1408/1892 [7:00:31<7:17:26, 54.23s/it] 74%|███████▍  | 1409/1892 [7:01:25<7:16:40, 54.25s/it]                                                       {'loss': 0.2344, 'learning_rate': 3.228510276311405e-06, 'epoch': 2.98}
 74%|███████▍  | 1409/1892 [7:01:25<7:16:40, 54.25s/it] 75%|███████▍  | 1410/1892 [7:02:19<7:15:38, 54.23s/it]                                                       {'loss': 0.2073, 'learning_rate': 3.215922232259159e-06, 'epoch': 2.98}
 75%|███████▍  | 1410/1892 [7:02:19<7:15:38, 54.23s/it] 75%|███████▍  | 1411/1892 [7:03:13<7:14:43, 54.23s/it]                                                       {'loss': 0.2317, 'learning_rate': 3.203354072873791e-06, 'epoch': 2.98}
 75%|███████▍  | 1411/1892 [7:03:13<7:14:43, 54.23s/it] 75%|███████▍  | 1412/1892 [7:04:08<7:13:54, 54.24s/it]                                                       {'loss': 0.2165, 'learning_rate': 3.19080583499357e-06, 'epoch': 2.98}
 75%|███████▍  | 1412/1892 [7:04:08<7:13:54, 54.24s/it] 75%|███████▍  | 1413/1892 [7:05:02<7:12:35, 54.19s/it]                                                       {'loss': 0.2168, 'learning_rate': 3.178277555398371e-06, 'epoch': 2.99}
 75%|███████▍  | 1413/1892 [7:05:02<7:12:35, 54.19s/it] 75%|███████▍  | 1414/1892 [7:05:56<7:11:42, 54.19s/it]                                                       {'loss': 0.1869, 'learning_rate': 3.1657692708095746e-06, 'epoch': 2.99}
 75%|███████▍  | 1414/1892 [7:05:56<7:11:42, 54.19s/it] 75%|███████▍  | 1415/1892 [7:06:50<7:10:42, 54.18s/it]                                                       {'loss': 0.2279, 'learning_rate': 3.1532810178899386e-06, 'epoch': 2.99}
 75%|███████▍  | 1415/1892 [7:06:50<7:10:42, 54.18s/it] 75%|███████▍  | 1416/1892 [7:07:44<7:10:03, 54.21s/it]                                                       {'loss': 0.2117, 'learning_rate': 3.140812833243524e-06, 'epoch': 2.99}
 75%|███████▍  | 1416/1892 [7:07:44<7:10:03, 54.21s/it] 75%|███████▍  | 1417/1892 [7:08:38<7:08:52, 54.17s/it]                                                       {'loss': 0.2105, 'learning_rate': 3.128364753415565e-06, 'epoch': 2.99}
 75%|███████▍  | 1417/1892 [7:08:38<7:08:52, 54.17s/it] 75%|███████▍  | 1418/1892 [7:09:33<7:08:09, 54.20s/it]                                                       {'loss': 0.2122, 'learning_rate': 3.1159368148923586e-06, 'epoch': 3.0}
 75%|███████▍  | 1418/1892 [7:09:33<7:08:09, 54.20s/it] 75%|███████▌  | 1419/1892 [7:10:27<7:07:27, 54.22s/it]                                                       {'loss': 0.2091, 'learning_rate': 3.1035290541011753e-06, 'epoch': 3.0}
 75%|███████▌  | 1419/1892 [7:10:27<7:07:27, 54.22s/it]/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 75%|███████▌  | 1420/1892 [7:13:07<11:16:31, 86.00s/it]                                                        {'loss': 0.2363, 'learning_rate': 3.0911415074101446e-06, 'epoch': 3.0}
 75%|███████▌  | 1420/1892 [7:13:07<11:16:31, 86.00s/it] 75%|███████▌  | 1421/1892 [7:14:02<10:02:01, 76.69s/it]                                                        {'loss': 0.2169, 'learning_rate': 3.078774211128136e-06, 'epoch': 3.0}
 75%|███████▌  | 1421/1892 [7:14:02<10:02:01, 76.69s/it] 75%|███████▌  | 1422/1892 [7:14:57<9:08:55, 70.08s/it]                                                        {'loss': 0.2271, 'learning_rate': 3.0664272015046735e-06, 'epoch': 3.01}
 75%|███████▌  | 1422/1892 [7:14:57<9:08:55, 70.08s/it] 75%|███████▌  | 1423/1892 [7:15:51<8:31:50, 65.48s/it]                                                       {'loss': 0.2252, 'learning_rate': 3.054100514729815e-06, 'epoch': 3.01}
 75%|███████▌  | 1423/1892 [7:15:51<8:31:50, 65.48s/it] 75%|███████▌  | 1424/1892 [7:16:46<8:05:44, 62.28s/it]                                                       {'loss': 0.2305, 'learning_rate': 3.041794186934055e-06, 'epoch': 3.01}
 75%|███████▌  | 1424/1892 [7:16:46<8:05:44, 62.28s/it] 75%|███████▌  | 1425/1892 [7:17:41<7:47:30, 60.07s/it]                                                       {'loss': 0.1991, 'learning_rate': 3.029508254188205e-06, 'epoch': 3.01}
 75%|███████▌  | 1425/1892 [7:17:41<7:47:30, 60.07s/it] 75%|███████▌  | 1426/1892 [7:18:36<7:34:53, 58.57s/it]                                                       {'loss': 0.2121, 'learning_rate': 3.017242752503304e-06, 'epoch': 3.01}
 75%|███████▌  | 1426/1892 [7:18:36<7:34:53, 58.57s/it] 75%|███████▌  | 1427/1892 [7:19:31<7:25:17, 57.46s/it]                                                       {'loss': 0.1953, 'learning_rate': 3.004997717830508e-06, 'epoch': 3.02}
 75%|███████▌  | 1427/1892 [7:19:31<7:25:17, 57.46s/it] 75%|███████▌  | 1428/1892 [7:20:26<7:18:14, 56.67s/it]                                                       {'loss': 0.2111, 'learning_rate': 2.9927731860609752e-06, 'epoch': 3.02}
 75%|███████▌  | 1428/1892 [7:20:26<7:18:14, 56.67s/it] 76%|███████▌  | 1429/1892 [7:21:21<7:13:18, 56.15s/it]                                                       {'loss': 0.1812, 'learning_rate': 2.9805691930257784e-06, 'epoch': 3.02}
 76%|███████▌  | 1429/1892 [7:21:21<7:13:18, 56.15s/it] 76%|███████▌  | 1430/1892 [7:22:16<7:09:18, 55.75s/it]                                                       {'loss': 0.1995, 'learning_rate': 2.968385774495779e-06, 'epoch': 3.02}
 76%|███████▌  | 1430/1892 [7:22:16<7:09:18, 55.75s/it] 76%|███████▌  | 1431/1892 [7:23:10<7:06:15, 55.48s/it]                                                       {'loss': 0.2003, 'learning_rate': 2.9562229661815334e-06, 'epoch': 3.02}
 76%|███████▌  | 1431/1892 [7:23:10<7:06:15, 55.48s/it] 76%|███████▌  | 1432/1892 [7:24:05<7:03:36, 55.25s/it]                                                       {'loss': 0.1945, 'learning_rate': 2.944080803733197e-06, 'epoch': 3.03}
 76%|███████▌  | 1432/1892 [7:24:05<7:03:36, 55.25s/it] 76%|███████▌  | 1433/1892 [7:25:00<7:01:45, 55.13s/it]                                                       {'loss': 0.2397, 'learning_rate': 2.9319593227404043e-06, 'epoch': 3.03}
 76%|███████▌  | 1433/1892 [7:25:00<7:01:45, 55.13s/it] 76%|███████▌  | 1434/1892 [7:25:55<7:00:19, 55.06s/it]                                                       {'loss': 0.2114, 'learning_rate': 2.919858558732175e-06, 'epoch': 3.03}
 76%|███████▌  | 1434/1892 [7:25:55<7:00:19, 55.06s/it] 76%|███████▌  | 1435/1892 [7:26:50<6:59:08, 55.03s/it]                                                       {'loss': 0.251, 'learning_rate': 2.9077785471767962e-06, 'epoch': 3.03}
 76%|███████▌  | 1435/1892 [7:26:50<6:59:08, 55.03s/it] 76%|███████▌  | 1436/1892 [7:27:45<6:57:45, 54.97s/it]                                                       {'loss': 0.218, 'learning_rate': 2.895719323481738e-06, 'epoch': 3.03}
 76%|███████▌  | 1436/1892 [7:27:45<6:57:45, 54.97s/it] 76%|███████▌  | 1437/1892 [7:28:40<6:56:33, 54.93s/it]                                                       {'loss': 0.2214, 'learning_rate': 2.883680922993536e-06, 'epoch': 3.04}
 76%|███████▌  | 1437/1892 [7:28:40<6:56:33, 54.93s/it] 76%|███████▌  | 1438/1892 [7:29:34<6:55:24, 54.90s/it]                                                       {'loss': 0.1973, 'learning_rate': 2.8716633809976923e-06, 'epoch': 3.04}
 76%|███████▌  | 1438/1892 [7:29:34<6:55:24, 54.90s/it] 76%|███████▌  | 1439/1892 [7:30:29<6:54:28, 54.90s/it]                                                       {'loss': 0.1955, 'learning_rate': 2.859666732718568e-06, 'epoch': 3.04}
 76%|███████▌  | 1439/1892 [7:30:29<6:54:28, 54.90s/it] 76%|███████▌  | 1440/1892 [7:31:24<6:53:28, 54.89s/it]                                                       {'loss': 0.2024, 'learning_rate': 2.8476910133192837e-06, 'epoch': 3.04}
 76%|███████▌  | 1440/1892 [7:31:24<6:53:28, 54.89s/it] 76%|███████▌  | 1441/1892 [7:32:19<6:52:19, 54.86s/it]                                                       {'loss': 0.2075, 'learning_rate': 2.835736257901621e-06, 'epoch': 3.05}
 76%|███████▌  | 1441/1892 [7:32:19<6:52:19, 54.86s/it] 76%|███████▌  | 1442/1892 [7:33:14<6:51:12, 54.83s/it]                                                       {'loss': 0.2012, 'learning_rate': 2.823802501505909e-06, 'epoch': 3.05}
 76%|███████▌  | 1442/1892 [7:33:14<6:51:12, 54.83s/it] 76%|███████▋  | 1443/1892 [7:34:09<6:50:18, 54.83s/it]                                                       {'loss': 0.1904, 'learning_rate': 2.8118897791109313e-06, 'epoch': 3.05}
 76%|███████▋  | 1443/1892 [7:34:09<6:50:18, 54.83s/it] 76%|███████▋  | 1444/1892 [7:35:03<6:49:17, 54.82s/it]                                                       {'loss': 0.1885, 'learning_rate': 2.799998125633815e-06, 'epoch': 3.05}
 76%|███████▋  | 1444/1892 [7:35:03<6:49:17, 54.82s/it] 76%|███████▋  | 1445/1892 [7:35:58<6:48:22, 54.82s/it]                                                       {'loss': 0.1952, 'learning_rate': 2.7881275759299297e-06, 'epoch': 3.05}
 76%|███████▋  | 1445/1892 [7:35:58<6:48:22, 54.82s/it] 76%|███████▋  | 1446/1892 [7:36:53<6:47:19, 54.80s/it]                                                       {'loss': 0.2055, 'learning_rate': 2.776278164792796e-06, 'epoch': 3.06}
 76%|███████▋  | 1446/1892 [7:36:53<6:47:19, 54.80s/it] 76%|███████▋  | 1447/1892 [7:37:48<6:46:33, 54.82s/it]                                                       {'loss': 0.2058, 'learning_rate': 2.7644499269539728e-06, 'epoch': 3.06}
 76%|███████▋  | 1447/1892 [7:37:48<6:46:33, 54.82s/it] 77%|███████▋  | 1448/1892 [7:38:43<6:45:51, 54.85s/it]                                                       {'loss': 0.229, 'learning_rate': 2.752642897082961e-06, 'epoch': 3.06}
 77%|███████▋  | 1448/1892 [7:38:43<6:45:51, 54.85s/it] 77%|███████▋  | 1449/1892 [7:39:37<6:44:51, 54.83s/it]                                                       {'loss': 0.1957, 'learning_rate': 2.7408571097870893e-06, 'epoch': 3.06}
 77%|███████▋  | 1449/1892 [7:39:37<6:44:51, 54.83s/it] 77%|███████▋  | 1450/1892 [7:40:32<6:43:41, 54.80s/it]                                                       {'loss': 0.2495, 'learning_rate': 2.729092599611434e-06, 'epoch': 3.06}
 77%|███████▋  | 1450/1892 [7:40:32<6:43:41, 54.80s/it] 77%|███████▋  | 1451/1892 [7:41:27<6:42:58, 54.83s/it]                                                       {'loss': 0.2204, 'learning_rate': 2.7173494010387003e-06, 'epoch': 3.07}
 77%|███████▋  | 1451/1892 [7:41:27<6:42:58, 54.83s/it] 77%|███████▋  | 1452/1892 [7:42:22<6:41:58, 54.81s/it]                                                       {'loss': 0.2105, 'learning_rate': 2.70562754848913e-06, 'epoch': 3.07}
 77%|███████▋  | 1452/1892 [7:42:22<6:41:58, 54.81s/it] 77%|███████▋  | 1453/1892 [7:43:17<6:41:22, 54.86s/it]                                                       {'loss': 0.1994, 'learning_rate': 2.6939270763204005e-06, 'epoch': 3.07}
 77%|███████▋  | 1453/1892 [7:43:17<6:41:22, 54.86s/it] 77%|███████▋  | 1454/1892 [7:44:12<6:40:33, 54.87s/it]                                                       {'loss': 0.2359, 'learning_rate': 2.6822480188275125e-06, 'epoch': 3.07}
 77%|███████▋  | 1454/1892 [7:44:12<6:40:33, 54.87s/it] 77%|███████▋  | 1455/1892 [7:45:07<6:39:34, 54.86s/it]                                                       {'loss': 0.2086, 'learning_rate': 2.670590410242707e-06, 'epoch': 3.07}
 77%|███████▋  | 1455/1892 [7:45:07<6:39:34, 54.86s/it] 77%|███████▋  | 1456/1892 [7:46:01<6:38:38, 54.86s/it]                                                       {'loss': 0.2135, 'learning_rate': 2.6589542847353534e-06, 'epoch': 3.08}
 77%|███████▋  | 1456/1892 [7:46:01<6:38:38, 54.86s/it] 77%|███████▋  | 1457/1892 [7:46:56<6:37:41, 54.85s/it]                                                       {'loss': 0.2101, 'learning_rate': 2.6473396764118575e-06, 'epoch': 3.08}
 77%|███████▋  | 1457/1892 [7:46:56<6:37:41, 54.85s/it] 77%|███████▋  | 1458/1892 [7:47:51<6:36:40, 54.84s/it]                                                       {'loss': 0.1966, 'learning_rate': 2.635746619315549e-06, 'epoch': 3.08}
 77%|███████▋  | 1458/1892 [7:47:51<6:36:40, 54.84s/it] 77%|███████▋  | 1459/1892 [7:48:46<6:35:55, 54.86s/it]                                                       {'loss': 0.2299, 'learning_rate': 2.6241751474265873e-06, 'epoch': 3.08}
 77%|███████▋  | 1459/1892 [7:48:46<6:35:55, 54.86s/it] 77%|███████▋  | 1460/1892 [7:49:41<6:34:58, 54.86s/it]                                                       {'loss': 0.223, 'learning_rate': 2.6126252946618748e-06, 'epoch': 3.09}
 77%|███████▋  | 1460/1892 [7:49:41<6:34:58, 54.86s/it] 77%|███████▋  | 1461/1892 [7:50:36<6:34:01, 54.85s/it]                                                       {'loss': 0.2003, 'learning_rate': 2.601097094874939e-06, 'epoch': 3.09}
 77%|███████▋  | 1461/1892 [7:50:36<6:34:01, 54.85s/it] 77%|███████▋  | 1462/1892 [7:51:31<6:33:07, 54.85s/it]                                                       {'loss': 0.184, 'learning_rate': 2.589590581855843e-06, 'epoch': 3.09}
 77%|███████▋  | 1462/1892 [7:51:31<6:33:07, 54.85s/it] 77%|███████▋  | 1463/1892 [7:52:25<6:32:02, 54.83s/it]                                                       {'loss': 0.217, 'learning_rate': 2.5781057893310866e-06, 'epoch': 3.09}
 77%|███████▋  | 1463/1892 [7:52:25<6:32:02, 54.83s/it] 77%|███████▋  | 1464/1892 [7:53:20<6:31:01, 54.82s/it]                                                       {'loss': 0.2194, 'learning_rate': 2.566642750963496e-06, 'epoch': 3.09}
 77%|███████▋  | 1464/1892 [7:53:20<6:31:01, 54.82s/it] 77%|███████▋  | 1465/1892 [7:54:15<6:30:19, 54.85s/it]                                                       {'loss': 0.2041, 'learning_rate': 2.5552015003521447e-06, 'epoch': 3.1}
 77%|███████▋  | 1465/1892 [7:54:15<6:30:19, 54.85s/it] 77%|███████▋  | 1466/1892 [7:55:10<6:29:30, 54.86s/it]                                                       {'loss': 0.2024, 'learning_rate': 2.543782071032238e-06, 'epoch': 3.1}
 77%|███████▋  | 1466/1892 [7:55:10<6:29:30, 54.86s/it] 78%|███████▊  | 1467/1892 [7:56:05<6:28:30, 54.85s/it]                                                       {'loss': 0.2164, 'learning_rate': 2.53238449647503e-06, 'epoch': 3.1}
 78%|███████▊  | 1467/1892 [7:56:05<6:28:30, 54.85s/it] 78%|███████▊  | 1468/1892 [7:57:00<6:27:36, 54.85s/it]                                                       {'loss': 0.2205, 'learning_rate': 2.5210088100877027e-06, 'epoch': 3.1}
 78%|███████▊  | 1468/1892 [7:57:00<6:27:36, 54.85s/it] 78%|███████▊  | 1469/1892 [7:57:54<6:26:32, 54.83s/it]                                                       {'loss': 0.2158, 'learning_rate': 2.5096550452132915e-06, 'epoch': 3.1}
 78%|███████▊  | 1469/1892 [7:57:54<6:26:32, 54.83s/it] 78%|███████▊  | 1470/1892 [7:58:49<6:25:44, 54.84s/it]                                                       {'loss': 0.2093, 'learning_rate': 2.49832323513058e-06, 'epoch': 3.11}
 78%|███████▊  | 1470/1892 [7:58:49<6:25:44, 54.84s/it] 78%|███████▊  | 1471/1892 [7:59:44<6:24:52, 54.85s/it]                                                       {'loss': 0.2306, 'learning_rate': 2.4870134130539936e-06, 'epoch': 3.11}
 78%|███████▊  | 1471/1892 [7:59:44<6:24:52, 54.85s/it] 78%|███████▊  | 1472/1892 [8:00:39<6:24:03, 54.87s/it]                                                       {'loss': 0.205, 'learning_rate': 2.4757256121335182e-06, 'epoch': 3.11}
 78%|███████▊  | 1472/1892 [8:00:39<6:24:03, 54.87s/it] 78%|███████▊  | 1473/1892 [8:01:34<6:22:59, 54.84s/it]                                                       {'loss': 0.2264, 'learning_rate': 2.464459865454584e-06, 'epoch': 3.11}
 78%|███████▊  | 1473/1892 [8:01:34<6:22:59, 54.84s/it] 78%|███████▊  | 1474/1892 [8:02:28<6:21:42, 54.79s/it]                                                       {'loss': 0.2274, 'learning_rate': 2.4532162060379795e-06, 'epoch': 3.12}
 78%|███████▊  | 1474/1892 [8:02:28<6:21:42, 54.79s/it] 78%|███████▊  | 1475/1892 [8:03:23<6:20:53, 54.81s/it]                                                       {'loss': 0.2063, 'learning_rate': 2.4419946668397607e-06, 'epoch': 3.12}
 78%|███████▊  | 1475/1892 [8:03:23<6:20:53, 54.81s/it] 78%|███████▊  | 1476/1892 [8:04:18<6:20:09, 54.83s/it]                                                       {'loss': 0.2236, 'learning_rate': 2.430795280751145e-06, 'epoch': 3.12}
 78%|███████▊  | 1476/1892 [8:04:18<6:20:09, 54.83s/it] 78%|███████▊  | 1477/1892 [8:05:13<6:19:15, 54.83s/it]                                                       {'loss': 0.2157, 'learning_rate': 2.419618080598417e-06, 'epoch': 3.12}
 78%|███████▊  | 1477/1892 [8:05:13<6:19:15, 54.83s/it] 78%|███████▊  | 1478/1892 [8:06:08<6:18:28, 54.85s/it]                                                       {'loss': 0.2048, 'learning_rate': 2.408463099142827e-06, 'epoch': 3.12}
 78%|███████▊  | 1478/1892 [8:06:08<6:18:28, 54.85s/it] 78%|███████▊  | 1479/1892 [8:07:03<6:17:36, 54.86s/it]                                                       {'loss': 0.2272, 'learning_rate': 2.397330369080508e-06, 'epoch': 3.13}
 78%|███████▊  | 1479/1892 [8:07:03<6:17:36, 54.86s/it] 78%|███████▊  | 1480/1892 [8:07:58<6:16:40, 54.86s/it]                                                       {'loss': 0.2121, 'learning_rate': 2.38621992304237e-06, 'epoch': 3.13}
 78%|███████▊  | 1480/1892 [8:07:58<6:16:40, 54.86s/it] 78%|███████▊  | 1481/1892 [8:08:52<6:15:34, 54.83s/it]                                                       {'loss': 0.2036, 'learning_rate': 2.3751317935940055e-06, 'epoch': 3.13}
 78%|███████▊  | 1481/1892 [8:08:52<6:15:34, 54.83s/it] 78%|███████▊  | 1482/1892 [8:09:47<6:14:46, 54.84s/it]                                                       {'loss': 0.2117, 'learning_rate': 2.3640660132356e-06, 'epoch': 3.13}
 78%|███████▊  | 1482/1892 [8:09:47<6:14:46, 54.84s/it] 78%|███████▊  | 1483/1892 [8:10:42<6:13:54, 54.85s/it]                                                       {'loss': 0.1964, 'learning_rate': 2.353022614401821e-06, 'epoch': 3.13}
 78%|███████▊  | 1483/1892 [8:10:42<6:13:54, 54.85s/it] 78%|███████▊  | 1484/1892 [8:11:37<6:12:52, 54.83s/it]                                                       {'loss': 0.2028, 'learning_rate': 2.3420016294617465e-06, 'epoch': 3.14}
 78%|███████▊  | 1484/1892 [8:11:37<6:12:52, 54.83s/it] 78%|███████▊  | 1485/1892 [8:12:32<6:11:52, 54.82s/it]                                                       {'loss': 0.1921, 'learning_rate': 2.3310030907187542e-06, 'epoch': 3.14}
 78%|███████▊  | 1485/1892 [8:12:32<6:11:52, 54.82s/it] 79%|███████▊  | 1486/1892 [8:13:27<6:11:04, 54.84s/it]                                                       {'loss': 0.2003, 'learning_rate': 2.320027030410422e-06, 'epoch': 3.14}
 79%|███████▊  | 1486/1892 [8:13:27<6:11:04, 54.84s/it] 79%|███████▊  | 1487/1892 [8:14:21<6:09:57, 54.81s/it]                                                       {'loss': 0.2266, 'learning_rate': 2.3090734807084545e-06, 'epoch': 3.14}
 79%|███████▊  | 1487/1892 [8:14:21<6:09:57, 54.81s/it] 79%|███████▊  | 1488/1892 [8:15:16<6:08:54, 54.79s/it]                                                       {'loss': 0.23, 'learning_rate': 2.298142473718564e-06, 'epoch': 3.14}
 79%|███████▊  | 1488/1892 [8:15:16<6:08:54, 54.79s/it] 79%|███████▊  | 1489/1892 [8:16:11<6:07:53, 54.77s/it]                                                       {'loss': 0.2342, 'learning_rate': 2.287234041480396e-06, 'epoch': 3.15}
 79%|███████▊  | 1489/1892 [8:16:11<6:07:53, 54.77s/it] 79%|███████▉  | 1490/1892 [8:17:06<6:06:54, 54.76s/it]                                                       {'loss': 0.1979, 'learning_rate': 2.276348215967428e-06, 'epoch': 3.15}
 79%|███████▉  | 1490/1892 [8:17:06<6:06:54, 54.76s/it] 79%|███████▉  | 1491/1892 [8:18:00<6:05:55, 54.75s/it]                                                       {'loss': 0.2315, 'learning_rate': 2.2654850290868725e-06, 'epoch': 3.15}
 79%|███████▉  | 1491/1892 [8:18:00<6:05:55, 54.75s/it] 79%|███████▉  | 1492/1892 [8:18:55<6:05:09, 54.77s/it]                                                       {'loss': 0.2074, 'learning_rate': 2.2546445126795822e-06, 'epoch': 3.15}
 79%|███████▉  | 1492/1892 [8:18:55<6:05:09, 54.77s/it] 79%|███████▉  | 1493/1892 [8:19:50<6:04:09, 54.76s/it]                                                       {'loss': 0.2127, 'learning_rate': 2.2438266985199707e-06, 'epoch': 3.16}
 79%|███████▉  | 1493/1892 [8:19:50<6:04:09, 54.76s/it] 79%|███████▉  | 1494/1892 [8:20:45<6:03:14, 54.76s/it]                                                       {'loss': 0.2212, 'learning_rate': 2.2330316183159007e-06, 'epoch': 3.16}
 79%|███████▉  | 1494/1892 [8:20:45<6:03:14, 54.76s/it] 79%|███████▉  | 1495/1892 [8:21:39<6:02:22, 54.77s/it]                                                       {'loss': 0.1984, 'learning_rate': 2.222259303708606e-06, 'epoch': 3.16}
 79%|███████▉  | 1495/1892 [8:21:39<6:02:22, 54.77s/it] 79%|███████▉  | 1496/1892 [8:22:34<6:01:47, 54.82s/it]                                                       {'loss': 0.1993, 'learning_rate': 2.211509786272592e-06, 'epoch': 3.16}
 79%|███████▉  | 1496/1892 [8:22:34<6:01:47, 54.82s/it] 79%|███████▉  | 1497/1892 [8:23:29<6:00:38, 54.78s/it]                                                       {'loss': 0.2047, 'learning_rate': 2.2007830975155366e-06, 'epoch': 3.16}
 79%|███████▉  | 1497/1892 [8:23:29<6:00:38, 54.78s/it] 79%|███████▉  | 1498/1892 [8:24:24<5:59:44, 54.78s/it]                                                       {'loss': 0.2142, 'learning_rate': 2.1900792688782124e-06, 'epoch': 3.17}
 79%|███████▉  | 1498/1892 [8:24:24<5:59:44, 54.78s/it] 79%|███████▉  | 1499/1892 [8:25:19<5:58:42, 54.76s/it]                                                       {'loss': 0.2146, 'learning_rate': 2.1793983317343892e-06, 'epoch': 3.17}
 79%|███████▉  | 1499/1892 [8:25:19<5:58:42, 54.76s/it] 79%|███████▉  | 1500/1892 [8:26:13<5:58:03, 54.81s/it]                                                       {'loss': 0.2253, 'learning_rate': 2.1687403173907305e-06, 'epoch': 3.17}
 79%|███████▉  | 1500/1892 [8:26:13<5:58:03, 54.81s/it] 79%|███████▉  | 1501/1892 [8:27:08<5:57:12, 54.81s/it]                                                       {'loss': 0.2144, 'learning_rate': 2.15810525708672e-06, 'epoch': 3.17}
 79%|███████▉  | 1501/1892 [8:27:08<5:57:12, 54.81s/it] 79%|███████▉  | 1502/1892 [8:28:03<5:56:14, 54.81s/it]                                                       {'loss': 0.2032, 'learning_rate': 2.1474931819945555e-06, 'epoch': 3.17}
 79%|███████▉  | 1502/1892 [8:28:03<5:56:14, 54.81s/it] 79%|███████▉  | 1503/1892 [8:28:58<5:55:09, 54.78s/it]                                                       {'loss': 0.1942, 'learning_rate': 2.136904123219067e-06, 'epoch': 3.18}
 79%|███████▉  | 1503/1892 [8:28:58<5:55:09, 54.78s/it] 79%|███████▉  | 1504/1892 [8:29:53<5:54:44, 54.86s/it]                                                       {'loss': 0.2072, 'learning_rate': 2.126338111797621e-06, 'epoch': 3.18}
 79%|███████▉  | 1504/1892 [8:29:53<5:54:44, 54.86s/it] 80%|███████▉  | 1505/1892 [8:30:48<5:53:47, 54.85s/it]                                                       {'loss': 0.209, 'learning_rate': 2.1157951787000298e-06, 'epoch': 3.18}
 80%|███████▉  | 1505/1892 [8:30:48<5:53:47, 54.85s/it] 80%|███████▉  | 1506/1892 [8:31:42<5:52:44, 54.83s/it]                                                       {'loss': 0.1976, 'learning_rate': 2.1052753548284653e-06, 'epoch': 3.18}
 80%|███████▉  | 1506/1892 [8:31:42<5:52:44, 54.83s/it] 80%|███████▉  | 1507/1892 [8:32:37<5:51:45, 54.82s/it]                                                       {'loss': 0.2174, 'learning_rate': 2.0947786710173545e-06, 'epoch': 3.18}
 80%|███████▉  | 1507/1892 [8:32:37<5:51:45, 54.82s/it] 80%|███████▉  | 1508/1892 [8:33:32<5:50:49, 54.82s/it]                                                       {'loss': 0.2178, 'learning_rate': 2.0843051580333083e-06, 'epoch': 3.19}
 80%|███████▉  | 1508/1892 [8:33:32<5:50:49, 54.82s/it] 80%|███████▉  | 1509/1892 [8:34:27<5:49:51, 54.81s/it]                                                       {'loss': 0.2151, 'learning_rate': 2.07385484657502e-06, 'epoch': 3.19}
 80%|███████▉  | 1509/1892 [8:34:27<5:49:51, 54.81s/it] 80%|███████▉  | 1510/1892 [8:35:22<5:48:49, 54.79s/it]                                                       {'loss': 0.2049, 'learning_rate': 2.0634277672731775e-06, 'epoch': 3.19}
 80%|███████▉  | 1510/1892 [8:35:22<5:48:49, 54.79s/it] 80%|███████▉  | 1511/1892 [8:36:16<5:47:58, 54.80s/it]                                                       {'loss': 0.2091, 'learning_rate': 2.053023950690368e-06, 'epoch': 3.19}
 80%|███████▉  | 1511/1892 [8:36:16<5:47:58, 54.80s/it] 80%|███████▉  | 1512/1892 [8:37:11<5:47:04, 54.80s/it]                                                       {'loss': 0.2182, 'learning_rate': 2.0426434273210016e-06, 'epoch': 3.2}
 80%|███████▉  | 1512/1892 [8:37:11<5:47:04, 54.80s/it] 80%|███████▉  | 1513/1892 [8:38:06<5:46:05, 54.79s/it]                                                       {'loss': 0.2074, 'learning_rate': 2.0322862275912126e-06, 'epoch': 3.2}
 80%|███████▉  | 1513/1892 [8:38:06<5:46:05, 54.79s/it] 80%|████████  | 1514/1892 [8:39:01<5:45:11, 54.79s/it]                                                       {'loss': 0.2029, 'learning_rate': 2.021952381858765e-06, 'epoch': 3.2}
 80%|████████  | 1514/1892 [8:39:01<5:45:11, 54.79s/it] 80%|████████  | 1515/1892 [8:39:56<5:44:25, 54.82s/it]                                                       {'loss': 0.2171, 'learning_rate': 2.0116419204129776e-06, 'epoch': 3.2}
 80%|████████  | 1515/1892 [8:39:56<5:44:25, 54.82s/it] 80%|████████  | 1516/1892 [8:40:50<5:43:32, 54.82s/it]                                                       {'loss': 0.2112, 'learning_rate': 2.0013548734746304e-06, 'epoch': 3.2}
 80%|████████  | 1516/1892 [8:40:50<5:43:32, 54.82s/it] 80%|████████  | 1517/1892 [8:41:45<5:42:27, 54.79s/it]                                                       {'loss': 0.2063, 'learning_rate': 1.991091271195862e-06, 'epoch': 3.21}
 80%|████████  | 1517/1892 [8:41:45<5:42:27, 54.79s/it] 80%|████████  | 1518/1892 [8:42:40<5:41:36, 54.80s/it]                                                       {'loss': 0.2259, 'learning_rate': 1.980851143660103e-06, 'epoch': 3.21}
 80%|████████  | 1518/1892 [8:42:40<5:41:36, 54.80s/it] 80%|████████  | 1519/1892 [8:43:35<5:40:37, 54.79s/it]                                                       {'loss': 0.1988, 'learning_rate': 1.9706345208819744e-06, 'epoch': 3.21}
 80%|████████  | 1519/1892 [8:43:35<5:40:37, 54.79s/it] 80%|████████  | 1520/1892 [8:44:30<5:39:41, 54.79s/it]                                                       {'loss': 0.2011, 'learning_rate': 1.960441432807206e-06, 'epoch': 3.21}
 80%|████████  | 1520/1892 [8:44:30<5:39:41, 54.79s/it] 80%|████████  | 1521/1892 [8:45:24<5:38:47, 54.79s/it]                                                       {'loss': 0.2206, 'learning_rate': 1.950271909312539e-06, 'epoch': 3.21}
 80%|████████  | 1521/1892 [8:45:24<5:38:47, 54.79s/it] 80%|████████  | 1522/1892 [8:46:19<5:37:45, 54.77s/it]                                                       {'loss': 0.2328, 'learning_rate': 1.9401259802056495e-06, 'epoch': 3.22}
 80%|████████  | 1522/1892 [8:46:19<5:37:45, 54.77s/it] 80%|████████  | 1523/1892 [8:47:14<5:36:53, 54.78s/it]                                                       {'loss': 0.2079, 'learning_rate': 1.9300036752250563e-06, 'epoch': 3.22}
 80%|████████  | 1523/1892 [8:47:14<5:36:53, 54.78s/it] 81%|████████  | 1524/1892 [8:48:09<5:36:09, 54.81s/it]                                                       {'loss': 0.2001, 'learning_rate': 1.919905024040034e-06, 'epoch': 3.22}
 81%|████████  | 1524/1892 [8:48:09<5:36:09, 54.81s/it] 81%|████████  | 1525/1892 [8:49:04<5:35:15, 54.81s/it]                                                       {'loss': 0.2026, 'learning_rate': 1.9098300562505266e-06, 'epoch': 3.22}
 81%|████████  | 1525/1892 [8:49:04<5:35:15, 54.81s/it] 81%|████████  | 1526/1892 [8:49:58<5:34:33, 54.84s/it]                                                       {'loss': 0.219, 'learning_rate': 1.8997788013870556e-06, 'epoch': 3.22}
 81%|████████  | 1526/1892 [8:49:58<5:34:33, 54.84s/it] 81%|████████  | 1527/1892 [8:50:53<5:33:40, 54.85s/it]                                                       {'loss': 0.2026, 'learning_rate': 1.8897512889106451e-06, 'epoch': 3.23}
 81%|████████  | 1527/1892 [8:50:53<5:33:40, 54.85s/it] 81%|████████  | 1528/1892 [8:51:48<5:32:46, 54.85s/it]                                                       {'loss': 0.2165, 'learning_rate': 1.8797475482127214e-06, 'epoch': 3.23}
 81%|████████  | 1528/1892 [8:51:48<5:32:46, 54.85s/it] 81%|████████  | 1529/1892 [8:52:43<5:31:49, 54.85s/it]                                                       {'loss': 0.1884, 'learning_rate': 1.8697676086150386e-06, 'epoch': 3.23}
 81%|████████  | 1529/1892 [8:52:43<5:31:49, 54.85s/it] 81%|████████  | 1530/1892 [8:53:38<5:30:54, 54.85s/it]                                                       {'loss': 0.2, 'learning_rate': 1.8598114993695904e-06, 'epoch': 3.23}
 81%|████████  | 1530/1892 [8:53:38<5:30:54, 54.85s/it] 81%|████████  | 1531/1892 [8:54:33<5:29:58, 54.84s/it]                                                       {'loss': 0.2101, 'learning_rate': 1.8498792496585117e-06, 'epoch': 3.24}
 81%|████████  | 1531/1892 [8:54:33<5:29:58, 54.84s/it] 81%|████████  | 1532/1892 [8:55:27<5:28:53, 54.81s/it]                                                       {'loss': 0.1982, 'learning_rate': 1.8399708885940136e-06, 'epoch': 3.24}
 81%|████████  | 1532/1892 [8:55:27<5:28:53, 54.81s/it] 81%|████████  | 1533/1892 [8:56:22<5:27:48, 54.79s/it]                                                       {'loss': 0.2077, 'learning_rate': 1.830086445218282e-06, 'epoch': 3.24}
 81%|████████  | 1533/1892 [8:56:22<5:27:48, 54.79s/it] 81%|████████  | 1534/1892 [8:57:17<5:26:37, 54.74s/it]                                                       {'loss': 0.2175, 'learning_rate': 1.8202259485034002e-06, 'epoch': 3.24}
 81%|████████  | 1534/1892 [8:57:17<5:26:37, 54.74s/it] 81%|████████  | 1535/1892 [8:58:12<5:25:46, 54.75s/it]                                                       {'loss': 0.2073, 'learning_rate': 1.8103894273512656e-06, 'epoch': 3.24}
 81%|████████  | 1535/1892 [8:58:12<5:25:46, 54.75s/it] 81%|████████  | 1536/1892 [8:59:06<5:24:48, 54.74s/it]                                                       {'loss': 0.2007, 'learning_rate': 1.800576910593491e-06, 'epoch': 3.25}
 81%|████████  | 1536/1892 [8:59:06<5:24:48, 54.74s/it] 81%|████████  | 1537/1892 [9:00:01<5:24:04, 54.77s/it]                                                       {'loss': 0.2011, 'learning_rate': 1.790788426991339e-06, 'epoch': 3.25}
 81%|████████  | 1537/1892 [9:00:01<5:24:04, 54.77s/it] 81%|████████▏ | 1538/1892 [9:00:56<5:23:28, 54.83s/it]                                                       {'loss': 0.202, 'learning_rate': 1.7810240052356275e-06, 'epoch': 3.25}
 81%|████████▏ | 1538/1892 [9:00:56<5:23:28, 54.83s/it] 81%|████████▏ | 1539/1892 [9:01:51<5:22:40, 54.84s/it]                                                       {'loss': 0.1936, 'learning_rate': 1.77128367394665e-06, 'epoch': 3.25}
 81%|████████▏ | 1539/1892 [9:01:51<5:22:40, 54.84s/it] 81%|████████▏ | 1540/1892 [9:02:46<5:21:50, 54.86s/it]                                                       {'loss': 0.2161, 'learning_rate': 1.7615674616740786e-06, 'epoch': 3.25}
 81%|████████▏ | 1540/1892 [9:02:46<5:21:50, 54.86s/it] 81%|████████▏ | 1541/1892 [9:03:41<5:20:50, 54.84s/it]                                                       {'loss': 0.2063, 'learning_rate': 1.7518753968969037e-06, 'epoch': 3.26}
 81%|████████▏ | 1541/1892 [9:03:41<5:20:50, 54.84s/it] 82%|████████▏ | 1542/1892 [9:04:35<5:19:46, 54.82s/it]                                                       {'loss': 0.2295, 'learning_rate': 1.742207508023327e-06, 'epoch': 3.26}
 82%|████████▏ | 1542/1892 [9:04:35<5:19:46, 54.82s/it] 82%|████████▏ | 1543/1892 [9:05:30<5:18:31, 54.76s/it]                                                       {'loss': 0.2131, 'learning_rate': 1.732563823390695e-06, 'epoch': 3.26}
 82%|████████▏ | 1543/1892 [9:05:30<5:18:31, 54.76s/it] 82%|████████▏ | 1544/1892 [9:06:25<5:17:29, 54.74s/it]                                                       {'loss': 0.2103, 'learning_rate': 1.7229443712654093e-06, 'epoch': 3.26}
 82%|████████▏ | 1544/1892 [9:06:25<5:17:29, 54.74s/it] 82%|████████▏ | 1545/1892 [9:07:19<5:16:21, 54.70s/it]                                                       {'loss': 0.2086, 'learning_rate': 1.7133491798428392e-06, 'epoch': 3.27}
 82%|████████▏ | 1545/1892 [9:07:19<5:16:21, 54.70s/it] 82%|████████▏ | 1546/1892 [9:08:14<5:15:30, 54.71s/it]                                                       {'loss': 0.205, 'learning_rate': 1.7037782772472489e-06, 'epoch': 3.27}
 82%|████████▏ | 1546/1892 [9:08:14<5:15:30, 54.71s/it] 82%|████████▏ | 1547/1892 [9:09:09<5:14:36, 54.71s/it]                                                       {'loss': 0.2095, 'learning_rate': 1.6942316915317091e-06, 'epoch': 3.27}
 82%|████████▏ | 1547/1892 [9:09:09<5:14:36, 54.71s/it] 82%|████████▏ | 1548/1892 [9:10:04<5:13:38, 54.71s/it]                                                       {'loss': 0.2147, 'learning_rate': 1.6847094506780148e-06, 'epoch': 3.27}
 82%|████████▏ | 1548/1892 [9:10:04<5:13:38, 54.71s/it] 82%|████████▏ | 1549/1892 [9:10:58<5:12:51, 54.73s/it]                                                       {'loss': 0.2342, 'learning_rate': 1.6752115825966087e-06, 'epoch': 3.27}
 82%|████████▏ | 1549/1892 [9:10:58<5:12:51, 54.73s/it] 82%|████████▏ | 1550/1892 [9:11:53<5:11:59, 54.74s/it]                                                       {'loss': 0.2038, 'learning_rate': 1.665738115126484e-06, 'epoch': 3.28}
 82%|████████▏ | 1550/1892 [9:11:53<5:11:59, 54.74s/it] 82%|████████▏ | 1551/1892 [9:12:48<5:10:57, 54.71s/it]                                                       {'loss': 0.2185, 'learning_rate': 1.6562890760351247e-06, 'epoch': 3.28}
 82%|████████▏ | 1551/1892 [9:12:48<5:10:57, 54.71s/it] 82%|████████▏ | 1552/1892 [9:13:42<5:09:46, 54.67s/it]                                                       {'loss': 0.2142, 'learning_rate': 1.6468644930184097e-06, 'epoch': 3.28}
 82%|████████▏ | 1552/1892 [9:13:42<5:09:46, 54.67s/it] 82%|████████▏ | 1553/1892 [9:14:37<5:09:00, 54.69s/it]                                                       {'loss': 0.1934, 'learning_rate': 1.6374643937005353e-06, 'epoch': 3.28}
 82%|████████▏ | 1553/1892 [9:14:37<5:09:00, 54.69s/it] 82%|████████▏ | 1554/1892 [9:15:32<5:08:15, 54.72s/it]                                                       {'loss': 0.2055, 'learning_rate': 1.628088805633934e-06, 'epoch': 3.28}
 82%|████████▏ | 1554/1892 [9:15:32<5:08:15, 54.72s/it] 82%|████████▏ | 1555/1892 [9:16:27<5:07:26, 54.74s/it]                                                       {'loss': 0.1967, 'learning_rate': 1.6187377562991903e-06, 'epoch': 3.29}
 82%|████████▏ | 1555/1892 [9:16:27<5:07:26, 54.74s/it] 82%|████████▏ | 1556/1892 [9:17:21<5:06:42, 54.77s/it]                                                       {'loss': 0.1998, 'learning_rate': 1.6094112731049693e-06, 'epoch': 3.29}
 82%|████████▏ | 1556/1892 [9:17:21<5:06:42, 54.77s/it] 82%|████████▏ | 1557/1892 [9:18:16<5:05:36, 54.74s/it]                                                       {'loss': 0.2093, 'learning_rate': 1.6001093833879288e-06, 'epoch': 3.29}
 82%|████████▏ | 1557/1892 [9:18:16<5:05:36, 54.74s/it] 82%|████████▏ | 1558/1892 [9:19:11<5:04:48, 54.76s/it]                                                       {'loss': 0.2199, 'learning_rate': 1.5908321144126415e-06, 'epoch': 3.29}
 82%|████████▏ | 1558/1892 [9:19:11<5:04:48, 54.76s/it] 82%|████████▏ | 1559/1892 [9:20:06<5:04:09, 54.80s/it]                                                       {'loss': 0.2336, 'learning_rate': 1.5815794933715168e-06, 'epoch': 3.29}
 82%|████████▏ | 1559/1892 [9:20:06<5:04:09, 54.80s/it] 82%|████████▏ | 1560/1892 [9:21:01<5:03:16, 54.81s/it]                                                       {'loss': 0.2164, 'learning_rate': 1.5723515473847095e-06, 'epoch': 3.3}
 82%|████████▏ | 1560/1892 [9:21:01<5:03:16, 54.81s/it] 83%|████████▎ | 1561/1892 [9:21:56<5:02:29, 54.83s/it]                                                       {'loss': 0.2156, 'learning_rate': 1.5631483035000627e-06, 'epoch': 3.3}
 83%|████████▎ | 1561/1892 [9:21:56<5:02:29, 54.83s/it] 83%|████████▎ | 1562/1892 [9:22:50<5:01:26, 54.81s/it]                                                       {'loss': 0.2645, 'learning_rate': 1.5539697886930082e-06, 'epoch': 3.3}
 83%|████████▎ | 1562/1892 [9:22:50<5:01:26, 54.81s/it] 83%|████████▎ | 1563/1892 [9:23:45<5:00:25, 54.79s/it]                                                       {'loss': 0.213, 'learning_rate': 1.5448160298664982e-06, 'epoch': 3.3}
 83%|████████▎ | 1563/1892 [9:23:45<5:00:25, 54.79s/it] 83%|████████▎ | 1564/1892 [9:24:40<4:59:22, 54.76s/it]                                                       {'loss': 0.2251, 'learning_rate': 1.5356870538509195e-06, 'epoch': 3.31}
 83%|████████▎ | 1564/1892 [9:24:40<4:59:22, 54.76s/it] 83%|████████▎ | 1565/1892 [9:25:35<4:58:38, 54.80s/it]                                                       {'loss': 0.2028, 'learning_rate': 1.52658288740402e-06, 'epoch': 3.31}
 83%|████████▎ | 1565/1892 [9:25:35<4:58:38, 54.80s/it] 83%|████████▎ | 1566/1892 [9:26:29<4:57:33, 54.76s/it]                                                       {'loss': 0.2007, 'learning_rate': 1.517503557210831e-06, 'epoch': 3.31}
 83%|████████▎ | 1566/1892 [9:26:29<4:57:33, 54.76s/it] 83%|████████▎ | 1567/1892 [9:27:24<4:56:41, 54.78s/it]                                                       {'loss': 0.238, 'learning_rate': 1.5084490898835857e-06, 'epoch': 3.31}
 83%|████████▎ | 1567/1892 [9:27:24<4:56:41, 54.78s/it][2024-05-16 09:52:26,851] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 302611
[2024-05-16 09:52:26,853] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 302612
[2024-05-16 09:52:31,241] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 302613
[2024-05-16 09:52:35,641] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 302614
[2024-05-16 09:52:39,722] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 302615
[2024-05-16 09:52:43,671] [ERROR] [launch.py:321:sigkill_handler] ['/home/data_llm/anaconda3/envs/moellava/bin/python', '-u', '/home/data_llm/madehua/FoodHealthMMLLM/moellava/train/train_xformers.py', '--local_rank=4', '--do_train', '--moe_enable', 'True', '--num_experts', '4', '--top_k_experts', '2', '--capacity_factor', '1.5', '--moe_mode', 'sparse', '--use_residual', 'False', '--router_aux_loss_coef', '0.01', '--train_modules', 'fc1', 'fc2', 'wg', '--deepspeed', '../../zero2_offload.json', '--model_name_or_path', '/mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-v0426', '--version', 'phi', '--data_path', '/mnt/data_llm/json_file/101_train_prompt10.json', '--image_folder', '/media/LLM_data/food_recognition_dataset', '--image_tower', '/mnt/data_llm/model/clip-vit-large-patch14-336', '--image_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--check_point_file_name', '/mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-moe-v101_0426.json', '--output_dir', '/mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-moe-v101_0426', '--num_train_epochs', '4', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'epoch', '--save_total_limit', '5', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '512', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '64', '--lazy_preprocess', 'True', '--report_to', 'tensorboard', '--cache_dir', '/media/fast_data/huggingface/hub/'] exits with return code = -9
