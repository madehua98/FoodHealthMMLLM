nohup: 忽略输入
[2024-04-12 00:18:06,293] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 00:18:09,295] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-12 00:18:09,295] [INFO] [runner.py:555:main] cmd = /home/data_llm/anaconda3/envs/moellava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMSwgMiwgMywgNCwgNywgOCwgOV19 --master_addr=127.0.0.1 --master_port=2224 --enable_each_rank_log=None /home/data_llm/FoodHealthMMLLM/moellava/train/train_xformers.py --moe_enable True --num_experts 4 --top_k_experts 2 --capacity_factor 1.5 --moe_mode sparse --use_residual False --router_aux_loss_coef 0.01 --train_modules fc1 fc2 wg --deepspeed ../../zero2_offload.json --model_name_or_path /mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-v1/checkpoint-90000 --version phi --data_path /mnt/data_llm/json_file/101_train_prompt1.json /mnt/data_llm/json_file/172_train_prompt1.json /mnt/data_llm/json_file/2k_train_prompt1.json --image_folder /media/LLM_data/food_recognition_dataset --image_tower /media/LLM_data/model/openai/clip-vit-large-patch14-336 --image_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --check_point_file_name /mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-v1-moe-v1.json --output_dir /mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-v1-moe-v1 --num_train_epochs 1 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 3000 --save_total_limit 30 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 10 --tf32 False --model_max_length 512 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to tensorboard --cache_dir /media/fast_data/huggingface/hub/
[2024-04-12 00:18:10,555] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 00:18:13,077] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2024-04-12 00:18:13,077] [INFO] [launch.py:138:main] 0 NCCL_IB_TIMEOUT=22
[2024-04-12 00:18:13,077] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [1, 2, 3, 4, 7, 8, 9]}
[2024-04-12 00:18:13,077] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=7, node_rank=0
[2024-04-12 00:18:13,077] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6]})
[2024-04-12 00:18:13,077] [INFO] [launch.py:163:main] dist_world_size=7
[2024-04-12 00:18:13,077] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=1,2,3,4,7,8,9
[2024-04-12 00:18:16,475] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 00:18:16,571] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 00:18:16,598] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 00:18:16,610] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 00:18:16,624] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 00:18:16,700] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 00:18:16,767] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 00:18:17,456] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-12 00:18:17,456] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-04-12 00:18:17,574] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-12 00:18:17,575] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-04-12 00:18:17,586] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-12 00:18:17,586] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-04-12 00:18:17,614] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-12 00:18:17,615] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-04-12 00:18:17,658] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-12 00:18:17,658] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-04-12 00:18:17,703] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-12 00:18:17,703] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-04-12 00:18:17,831] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-12 00:18:17,831] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-04-12 00:18:17,831] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.72s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]
LLM init. firstly
 MoELLaVAPhiForCausalLM(
  (model): MoELLaVAPhiModel(
    (embed_tokens): Embedding(51200, 2560, padding_idx=50295)
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-31): 32 x PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower()
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2560, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2560, out_features=2560, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2560, out_features=51200, bias=False)
)
[2024-04-12 00:18:26,214] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.12s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.60s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.36s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  3.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.59s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.81s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.07s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.02s/it]
[2024-04-12 00:18:38,304] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-12 00:18:50,302] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-12 00:19:02,580] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-12 00:19:13,968] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-12 00:19:24,714] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-12 00:19:35,729] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-12 00:19:46,610] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-12 00:19:57,371] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-12 00:20:07,890] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-12 00:20:18,774] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-12 00:20:29,599] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-12 00:20:40,500] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-12 00:20:51,224] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-12 00:21:01,406] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-12 00:21:11,501] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Vision encoder and proj init.
 MoELLaVAPhiForCausalLM(
  (model): MoELLaVAPhiModel(
    (embed_tokens): Embedding(51200, 2560, padding_idx=50295)
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (1): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (2): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (3): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (4): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (5): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (6): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (7): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (8): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (9): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (10): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (11): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (12): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (13): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (14): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (15): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (16): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (17): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (18): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (19): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (20): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (21): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (22): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (23): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (24): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (25): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (26): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (27): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (28): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (29): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (30): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (31): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2560, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2560, out_features=2560, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2560, out_features=51200, bias=False)
)
model.layers.0.mlp.deepspeed_moe.gate.wg.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.1.mlp.fc1.weight
model.layers.1.mlp.fc1.bias
model.layers.1.mlp.fc2.weight
model.layers.1.mlp.fc2.bias
model.layers.2.mlp.deepspeed_moe.gate.wg.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.3.mlp.fc1.weight
model.layers.3.mlp.fc1.bias
model.layers.3.mlp.fc2.weight
model.layers.3.mlp.fc2.bias
model.layers.4.mlp.deepspeed_moe.gate.wg.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.5.mlp.fc1.weight
model.layers.5.mlp.fc1.bias
model.layers.5.mlp.fc2.weight
model.layers.5.mlp.fc2.bias
model.layers.6.mlp.deepspeed_moe.gate.wg.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.7.mlp.fc1.weight
model.layers.7.mlp.fc1.bias
model.layers.7.mlp.fc2.weight
model.layers.7.mlp.fc2.bias
model.layers.8.mlp.deepspeed_moe.gate.wg.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.9.mlp.fc1.weight
model.layers.9.mlp.fc1.bias
model.layers.9.mlp.fc2.weight
model.layers.9.mlp.fc2.bias
model.layers.10.mlp.deepspeed_moe.gate.wg.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.11.mlp.fc1.weight
model.layers.11.mlp.fc1.bias
model.layers.11.mlp.fc2.weight
model.layers.11.mlp.fc2.bias
model.layers.12.mlp.deepspeed_moe.gate.wg.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.13.mlp.fc1.weight
model.layers.13.mlp.fc1.bias
model.layers.13.mlp.fc2.weight
model.layers.13.mlp.fc2.bias
model.layers.14.mlp.deepspeed_moe.gate.wg.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.15.mlp.fc1.weight
model.layers.15.mlp.fc1.bias
model.layers.15.mlp.fc2.weight
model.layers.15.mlp.fc2.bias
model.layers.16.mlp.deepspeed_moe.gate.wg.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.17.mlp.fc1.weight
model.layers.17.mlp.fc1.bias
model.layers.17.mlp.fc2.weight
model.layers.17.mlp.fc2.bias
model.layers.18.mlp.deepspeed_moe.gate.wg.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.19.mlp.fc1.weight
model.layers.19.mlp.fc1.bias
model.layers.19.mlp.fc2.weight
model.layers.19.mlp.fc2.bias
model.layers.20.mlp.deepspeed_moe.gate.wg.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.21.mlp.fc1.weight
model.layers.21.mlp.fc1.bias
model.layers.21.mlp.fc2.weight
model.layers.21.mlp.fc2.bias
model.layers.22.mlp.deepspeed_moe.gate.wg.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.23.mlp.fc1.weight
model.layers.23.mlp.fc1.bias
model.layers.23.mlp.fc2.weight
model.layers.23.mlp.fc2.bias
model.layers.24.mlp.deepspeed_moe.gate.wg.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.25.mlp.fc1.weight
model.layers.25.mlp.fc1.bias
model.layers.25.mlp.fc2.weight
model.layers.25.mlp.fc2.bias
model.layers.26.mlp.deepspeed_moe.gate.wg.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.27.mlp.fc1.weight
model.layers.27.mlp.fc1.bias
model.layers.27.mlp.fc2.weight
model.layers.27.mlp.fc2.bias
model.layers.28.mlp.deepspeed_moe.gate.wg.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.29.mlp.fc1.weight
model.layers.29.mlp.fc1.bias
model.layers.29.mlp.fc2.weight
model.layers.29.mlp.fc2.bias
model.layers.30.mlp.deepspeed_moe.gate.wg.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.31.mlp.fc1.weight
model.layers.31.mlp.fc1.bias
model.layers.31.mlp.fc2.weight
model.layers.31.mlp.fc2.bias
model.mm_projector.image_spatial_proj.0.weight
model.mm_projector.image_spatial_proj.0.bias
model.mm_projector.image_spatial_proj.2.weight
model.mm_projector.image_spatial_proj.2.bias
MoELLaVAPhiForCausalLM(
  (model): MoELLaVAPhiModel(
    (embed_tokens): Embedding(51200, 2560, padding_idx=50295)
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (1): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (2): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (3): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (4): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (5): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (6): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (7): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (8): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (9): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (10): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (11): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (12): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (13): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (14): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (15): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (16): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (17): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (18): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (19): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (20): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (21): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (22): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (23): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (24): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (25): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (26): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (27): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (28): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (29): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (30): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (31): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2560, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2560, out_features=2560, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2560, out_features=51200, bias=False)
)
Formatting inputs...Skip in lazy mode
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4087140560150146 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.462022304534912 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4711410999298096 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.5402684211730957 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4969167709350586 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.5241787433624268 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.457512378692627 seconds
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
Rank: 6 partition count [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7] and sizes[(121171384, False), (29990, False), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (7489830, True), (117030, True)] 
Rank: 4 partition count [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7] and sizes[(121171384, False), (29990, False), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (7489830, True), (117030, True)] 
Rank: 0 partition count [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7] and sizes[(121171384, False), (29990, False), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (7489830, True), (117030, True)] 
Rank: 3 partition count [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7] and sizes[(121171384, False), (29990, False), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (7489830, True), (117030, True)] 
Rank: 5 partition count [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7] and sizes[(121171384, False), (29990, False), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (7489830, True), (117030, True)] 
Rank: 2 partition count [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7] and sizes[(121171384, False), (29990, False), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (7489830, True), (117030, True)] 
Rank: 1 partition count [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7] and sizes[(121171384, False), (29990, False), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (22469486, True), (7489830, True), (117030, True)] 
  0%|          | 0/9588 [00:00<?, ?it/s]/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/9588 [00:54<145:33:06, 54.66s/it]  0%|          | 2/9588 [01:39<130:54:35, 49.16s/it]  0%|          | 3/9588 [02:22<123:01:34, 46.21s/it]  0%|          | 4/9588 [03:06<120:18:28, 45.19s/it]  0%|          | 5/9588 [03:48<117:41:19, 44.21s/it]  0%|          | 6/9588 [04:32<117:21:43, 44.09s/it]  0%|          | 7/9588 [05:16<117:11:09, 44.03s/it]  0%|          | 8/9588 [05:58<115:27:32, 43.39s/it]  0%|          | 9/9588 [06:41<114:55:05, 43.19s/it]  0%|          | 10/9588 [07:24<114:36:59, 43.08s/it]                                                     {'loss': 0.304, 'learning_rate': 6.944444444444446e-07, 'epoch': 0.0}
  0%|          | 10/9588 [07:24<114:36:59, 43.08s/it]  0%|          | 11/9588 [08:07<114:47:46, 43.15s/it]  0%|          | 12/9588 [08:49<113:50:07, 42.80s/it]  0%|          | 13/9588 [09:31<113:23:16, 42.63s/it]  0%|          | 14/9588 [10:14<113:27:53, 42.66s/it]  0%|          | 15/9588 [10:57<114:06:30, 42.91s/it]  0%|          | 16/9588 [11:41<114:21:02, 43.01s/it]  0%|          | 17/9588 [12:23<113:46:55, 42.80s/it]  0%|          | 18/9588 [13:05<113:04:59, 42.54s/it]  0%|          | 19/9588 [13:47<112:26:48, 42.30s/it]  0%|          | 20/9588 [14:29<112:06:04, 42.18s/it]                                                     {'loss': 0.3043, 'learning_rate': 1.3888888888888892e-06, 'epoch': 0.0}
  0%|          | 20/9588 [14:29<112:06:04, 42.18s/it]  0%|          | 21/9588 [15:10<111:50:09, 42.08s/it]  0%|          | 22/9588 [15:52<111:42:01, 42.04s/it]  0%|          | 23/9588 [16:34<111:44:29, 42.06s/it]  0%|          | 24/9588 [17:17<111:50:05, 42.10s/it]  0%|          | 25/9588 [17:59<111:48:06, 42.09s/it]  0%|          | 26/9588 [18:41<111:37:03, 42.02s/it]  0%|          | 27/9588 [19:22<111:13:20, 41.88s/it]  0%|          | 28/9588 [20:04<111:13:25, 41.88s/it]  0%|          | 29/9588 [20:46<111:07:08, 41.85s/it]  0%|          | 30/9588 [21:27<111:00:41, 41.81s/it]                                                     {'loss': 0.2983, 'learning_rate': 2.0833333333333334e-06, 'epoch': 0.0}
  0%|          | 30/9588 [21:27<111:00:41, 41.81s/it]  0%|          | 31/9588 [22:09<110:59:23, 41.81s/it]  0%|          | 32/9588 [22:51<111:02:57, 41.84s/it]  0%|          | 33/9588 [23:33<111:01:15, 41.83s/it]  0%|          | 34/9588 [24:15<111:01:06, 41.83s/it]  0%|          | 35/9588 [24:57<111:01:53, 41.84s/it]  0%|          | 36/9588 [25:38<110:52:22, 41.79s/it]  0%|          | 37/9588 [26:20<111:00:23, 41.84s/it]  0%|          | 38/9588 [27:02<111:05:12, 41.88s/it]  0%|          | 39/9588 [27:44<111:08:43, 41.90s/it]  0%|          | 40/9588 [28:26<111:06:34, 41.89s/it]                                                     {'loss': 0.3061, 'learning_rate': 2.7777777777777783e-06, 'epoch': 0.0}
  0%|          | 40/9588 [28:26<111:06:34, 41.89s/it]  0%|          | 41/9588 [29:08<111:17:17, 41.96s/it]  0%|          | 42/9588 [29:50<111:08:23, 41.91s/it]  0%|          | 43/9588 [30:32<111:16:37, 41.97s/it]  0%|          | 44/9588 [31:14<111:08:57, 41.93s/it]  0%|          | 45/9588 [31:56<111:09:48, 41.94s/it]  0%|          | 46/9588 [32:38<111:12:56, 41.96s/it]  0%|          | 47/9588 [33:20<111:10:40, 41.95s/it]  1%|          | 48/9588 [34:02<111:19:19, 42.01s/it]  1%|          | 49/9588 [34:44<111:13:04, 41.97s/it]  1%|          | 50/9588 [35:26<111:04:50, 41.93s/it]                                                     {'loss': 0.2985, 'learning_rate': 3.4722222222222224e-06, 'epoch': 0.01}
  1%|          | 50/9588 [35:26<111:04:50, 41.93s/it]  1%|          | 51/9588 [36:08<111:07:32, 41.95s/it]  1%|          | 52/9588 [36:49<110:58:26, 41.89s/it]  1%|          | 53/9588 [37:31<110:55:14, 41.88s/it]  1%|          | 54/9588 [38:13<110:48:31, 41.84s/it]  1%|          | 55/9588 [38:55<110:42:07, 41.81s/it]  1%|          | 56/9588 [39:37<110:38:36, 41.79s/it]  1%|          | 57/9588 [40:18<110:35:36, 41.77s/it]  1%|          | 58/9588 [41:00<110:33:47, 41.77s/it]  1%|          | 59/9588 [41:42<110:45:16, 41.84s/it]  1%|          | 60/9588 [42:24<110:46:28, 41.85s/it]                                                     {'loss': 0.2918, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.01}
  1%|          | 60/9588 [42:24<110:46:28, 41.85s/it]  1%|          | 61/9588 [43:06<110:50:49, 41.89s/it]  1%|          | 62/9588 [43:48<110:45:49, 41.86s/it]  1%|          | 63/9588 [44:30<110:43:34, 41.85s/it]  1%|          | 64/9588 [45:11<110:45:02, 41.86s/it]  1%|          | 65/9588 [45:53<110:45:02, 41.87s/it]  1%|          | 66/9588 [46:35<110:39:40, 41.84s/it]  1%|          | 67/9588 [47:17<110:38:14, 41.83s/it]  1%|          | 68/9588 [47:59<110:40:06, 41.85s/it]  1%|          | 69/9588 [48:41<110:45:04, 41.89s/it]  1%|          | 70/9588 [49:23<110:49:24, 41.92s/it]                                                     {'loss': 0.2897, 'learning_rate': 4.861111111111111e-06, 'epoch': 0.01}
  1%|          | 70/9588 [49:23<110:49:24, 41.92s/it]  1%|          | 71/9588 [50:05<110:47:44, 41.91s/it]  1%|          | 72/9588 [50:46<110:36:51, 41.85s/it]  1%|          | 73/9588 [51:28<110:31:09, 41.82s/it]  1%|          | 74/9588 [52:10<110:36:52, 41.86s/it]  1%|          | 75/9588 [52:52<110:45:20, 41.91s/it]  1%|          | 76/9588 [53:34<110:43:50, 41.91s/it]  1%|          | 77/9588 [54:16<111:12:33, 42.09s/it]  1%|          | 78/9588 [54:58<111:01:58, 42.03s/it]  1%|          | 79/9588 [55:40<111:05:36, 42.06s/it]  1%|          | 80/9588 [56:22<111:01:45, 42.04s/it]                                                     {'loss': 0.3023, 'learning_rate': 5.555555555555557e-06, 'epoch': 0.01}
  1%|          | 80/9588 [56:22<111:01:45, 42.04s/it]  1%|          | 81/9588 [57:04<110:49:20, 41.96s/it]  1%|          | 82/9588 [57:46<110:45:38, 41.95s/it]  1%|          | 83/9588 [58:28<110:42:07, 41.93s/it]  1%|          | 84/9588 [59:10<110:39:17, 41.91s/it]  1%|          | 85/9588 [59:52<110:35:54, 41.90s/it]  1%|          | 86/9588 [1:00:33<110:20:33, 41.81s/it]  1%|          | 87/9588 [1:01:15<110:16:38, 41.78s/it]  1%|          | 88/9588 [1:01:57<110:17:33, 41.80s/it]  1%|          | 89/9588 [1:02:39<110:25:19, 41.85s/it]  1%|          | 90/9588 [1:03:21<110:34:47, 41.91s/it]                                                       {'loss': 0.2999, 'learning_rate': 6.25e-06, 'epoch': 0.01}
  1%|          | 90/9588 [1:03:21<110:34:47, 41.91s/it]  1%|          | 91/9588 [1:04:03<110:31:41, 41.90s/it]  1%|          | 92/9588 [1:04:44<110:15:34, 41.80s/it]  1%|          | 93/9588 [1:05:26<110:23:06, 41.85s/it]  1%|          | 94/9588 [1:06:08<110:23:43, 41.86s/it]  1%|          | 95/9588 [1:06:50<110:30:22, 41.91s/it]  1%|          | 96/9588 [1:07:32<110:37:11, 41.95s/it]  1%|          | 97/9588 [1:08:14<110:42:01, 41.99s/it]  1%|          | 98/9588 [1:08:56<110:20:25, 41.86s/it]  1%|          | 99/9588 [1:09:38<110:13:16, 41.82s/it]  1%|          | 100/9588 [1:10:20<110:20:20, 41.87s/it]                                                        {'loss': 0.3094, 'learning_rate': 6.944444444444445e-06, 'epoch': 0.01}
  1%|          | 100/9588 [1:10:20<110:20:20, 41.87s/it]  1%|          | 101/9588 [1:11:02<110:26:53, 41.91s/it]  1%|          | 102/9588 [1:11:43<110:19:33, 41.87s/it]  1%|          | 103/9588 [1:12:25<110:13:58, 41.84s/it]  1%|          | 104/9588 [1:13:07<110:06:25, 41.80s/it]  1%|          | 105/9588 [1:13:49<110:21:41, 41.90s/it]  1%|          | 106/9588 [1:14:31<110:08:35, 41.82s/it]  1%|          | 107/9588 [1:15:13<110:10:11, 41.83s/it]  1%|          | 108/9588 [1:15:55<110:15:40, 41.87s/it]  1%|          | 109/9588 [1:16:36<110:17:41, 41.89s/it]  1%|          | 110/9588 [1:17:18<110:12:05, 41.86s/it]                                                        {'loss': 0.3119, 'learning_rate': 7.638888888888888e-06, 'epoch': 0.01}
  1%|          | 110/9588 [1:17:18<110:12:05, 41.86s/it]  1%|          | 111/9588 [1:18:00<110:19:32, 41.91s/it]  1%|          | 112/9588 [1:18:42<110:07:35, 41.84s/it]  1%|          | 113/9588 [1:19:24<110:15:41, 41.89s/it]  1%|          | 114/9588 [1:20:06<110:04:45, 41.83s/it]  1%|          | 115/9588 [1:20:48<110:05:49, 41.84s/it]  1%|          | 116/9588 [1:21:29<110:02:02, 41.82s/it]  1%|          | 117/9588 [1:22:11<109:56:20, 41.79s/it]  1%|          | 118/9588 [1:22:53<109:52:03, 41.77s/it]  1%|          | 119/9588 [1:23:34<109:49:48, 41.76s/it]  1%|▏         | 120/9588 [1:24:16<109:52:28, 41.78s/it]                                                        {'loss': 0.3257, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.01}
  1%|▏         | 120/9588 [1:24:16<109:52:28, 41.78s/it]  1%|▏         | 121/9588 [1:24:58<109:53:57, 41.79s/it]  1%|▏         | 122/9588 [1:25:40<109:48:38, 41.76s/it]  1%|▏         | 123/9588 [1:26:21<109:40:55, 41.72s/it]  1%|▏         | 124/9588 [1:27:03<109:43:07, 41.74s/it]  1%|▏         | 125/9588 [1:27:45<109:38:39, 41.71s/it]  1%|▏         | 126/9588 [1:28:27<109:46:48, 41.77s/it]  1%|▏         | 127/9588 [1:29:08<109:43:30, 41.75s/it]  1%|▏         | 128/9588 [1:29:50<109:37:31, 41.72s/it]  1%|▏         | 129/9588 [1:30:32<109:36:08, 41.71s/it]  1%|▏         | 130/9588 [1:31:13<109:34:12, 41.71s/it]                                                        {'loss': 0.3184, 'learning_rate': 9.027777777777779e-06, 'epoch': 0.01}
  1%|▏         | 130/9588 [1:31:13<109:34:12, 41.71s/it]  1%|▏         | 131/9588 [1:31:55<109:33:11, 41.70s/it]  1%|▏         | 132/9588 [1:32:37<109:27:53, 41.67s/it]  1%|▏         | 133/9588 [1:33:19<109:29:27, 41.69s/it]  1%|▏         | 134/9588 [1:34:01<109:44:15, 41.79s/it]  1%|▏         | 135/9588 [1:34:42<109:41:15, 41.77s/it]  1%|▏         | 136/9588 [1:35:24<109:41:44, 41.78s/it]  1%|▏         | 137/9588 [1:36:06<109:31:54, 41.72s/it]  1%|▏         | 138/9588 [1:36:47<109:32:50, 41.73s/it]  1%|▏         | 139/9588 [1:37:30<109:52:20, 41.86s/it]  1%|▏         | 140/9588 [1:38:11<109:40:31, 41.79s/it]                                                        {'loss': 0.3453, 'learning_rate': 9.722222222222223e-06, 'epoch': 0.01}
  1%|▏         | 140/9588 [1:38:11<109:40:31, 41.79s/it]  1%|▏         | 141/9588 [1:38:53<109:34:02, 41.75s/it]  1%|▏         | 142/9588 [1:39:35<109:52:27, 41.87s/it]  1%|▏         | 143/9588 [1:40:17<109:42:59, 41.82s/it]  2%|▏         | 144/9588 [1:40:58<109:35:02, 41.77s/it]  2%|▏         | 145/9588 [1:41:40<109:43:49, 41.83s/it]  2%|▏         | 146/9588 [1:42:22<109:43:58, 41.84s/it]  2%|▏         | 147/9588 [1:43:04<109:42:57, 41.84s/it]  2%|▏         | 148/9588 [1:43:46<109:37:12, 41.80s/it]  2%|▏         | 149/9588 [1:44:27<109:29:37, 41.76s/it]  2%|▏         | 150/9588 [1:45:09<109:24:18, 41.73s/it]                                                        {'loss': 0.3316, 'learning_rate': 1.0416666666666668e-05, 'epoch': 0.02}
  2%|▏         | 150/9588 [1:45:09<109:24:18, 41.73s/it]  2%|▏         | 151/9588 [1:45:51<109:38:50, 41.83s/it]  2%|▏         | 152/9588 [1:46:33<109:57:18, 41.95s/it]  2%|▏         | 153/9588 [1:47:15<109:42:15, 41.86s/it]  2%|▏         | 154/9588 [1:47:57<110:04:44, 42.01s/it]  2%|▏         | 155/9588 [1:48:39<109:47:46, 41.90s/it]  2%|▏         | 156/9588 [1:49:21<109:42:35, 41.87s/it]  2%|▏         | 157/9588 [1:50:03<109:36:01, 41.84s/it]  2%|▏         | 158/9588 [1:50:44<109:29:22, 41.80s/it]  2%|▏         | 159/9588 [1:51:26<109:27:43, 41.79s/it]  2%|▏         | 160/9588 [1:52:08<109:28:33, 41.80s/it]                                                        {'loss': 0.3402, 'learning_rate': 1.1111111111111113e-05, 'epoch': 0.02}
  2%|▏         | 160/9588 [1:52:08<109:28:33, 41.80s/it]  2%|▏         | 161/9588 [1:52:50<109:39:58, 41.88s/it]  2%|▏         | 162/9588 [1:53:32<109:32:23, 41.84s/it]  2%|▏         | 163/9588 [1:54:14<109:34:34, 41.85s/it]  2%|▏         | 164/9588 [1:54:55<109:33:11, 41.85s/it]  2%|▏         | 165/9588 [1:55:37<109:23:30, 41.79s/it]  2%|▏         | 166/9588 [1:56:19<109:17:11, 41.76s/it]  2%|▏         | 167/9588 [1:57:00<109:14:56, 41.75s/it]  2%|▏         | 168/9588 [1:57:42<109:12:50, 41.74s/it]  2%|▏         | 169/9588 [1:58:24<109:17:03, 41.77s/it]  2%|▏         | 170/9588 [1:59:06<109:14:15, 41.76s/it]                                                        {'loss': 0.341, 'learning_rate': 1.1805555555555557e-05, 'epoch': 0.02}
  2%|▏         | 170/9588 [1:59:06<109:14:15, 41.76s/it]  2%|▏         | 171/9588 [1:59:48<109:29:16, 41.86s/it]  2%|▏         | 172/9588 [2:00:29<109:18:01, 41.79s/it]  2%|▏         | 173/9588 [2:01:11<109:08:39, 41.73s/it]  2%|▏         | 174/9588 [2:01:53<109:10:06, 41.75s/it]  2%|▏         | 175/9588 [2:02:34<109:03:47, 41.71s/it]  2%|▏         | 176/9588 [2:03:16<109:03:10, 41.71s/it]  2%|▏         | 177/9588 [2:03:58<108:59:35, 41.69s/it]  2%|▏         | 178/9588 [2:04:40<108:59:18, 41.70s/it]  2%|▏         | 179/9588 [2:05:21<108:51:36, 41.65s/it]  2%|▏         | 180/9588 [2:06:03<108:54:33, 41.67s/it]                                                        {'loss': 0.3571, 'learning_rate': 1.25e-05, 'epoch': 0.02}
  2%|▏         | 180/9588 [2:06:03<108:54:33, 41.67s/it]  2%|▏         | 181/9588 [2:06:45<109:05:44, 41.75s/it]  2%|▏         | 182/9588 [2:07:27<109:10:56, 41.79s/it]  2%|▏         | 183/9588 [2:08:09<109:24:21, 41.88s/it]  2%|▏         | 184/9588 [2:08:51<109:46:56, 42.03s/it]  2%|▏         | 185/9588 [2:09:33<109:26:16, 41.90s/it]  2%|▏         | 186/9588 [2:10:15<109:39:58, 41.99s/it]  2%|▏         | 187/9588 [2:10:57<109:36:16, 41.97s/it]  2%|▏         | 188/9588 [2:11:39<109:24:30, 41.90s/it]  2%|▏         | 189/9588 [2:12:20<109:14:52, 41.84s/it]  2%|▏         | 190/9588 [2:13:02<109:24:10, 41.91s/it]                                                        {'loss': 0.3762, 'learning_rate': 1.3194444444444446e-05, 'epoch': 0.02}
  2%|▏         | 190/9588 [2:13:02<109:24:10, 41.91s/it]  2%|▏         | 191/9588 [2:13:44<109:22:48, 41.90s/it]  2%|▏         | 192/9588 [2:14:26<109:18:30, 41.88s/it]  2%|▏         | 193/9588 [2:15:08<109:23:36, 41.92s/it]  2%|▏         | 194/9588 [2:15:50<109:13:30, 41.86s/it]  2%|▏         | 195/9588 [2:16:32<109:24:23, 41.93s/it]  2%|▏         | 196/9588 [2:17:14<109:19:23, 41.90s/it]  2%|▏         | 197/9588 [2:17:55<109:06:03, 41.82s/it]  2%|▏         | 198/9588 [2:18:37<108:56:20, 41.77s/it]  2%|▏         | 199/9588 [2:19:19<108:56:12, 41.77s/it]  2%|▏         | 200/9588 [2:20:01<109:06:39, 41.84s/it]                                                        {'loss': 0.3485, 'learning_rate': 1.388888888888889e-05, 'epoch': 0.02}
  2%|▏         | 200/9588 [2:20:01<109:06:39, 41.84s/it]  2%|▏         | 201/9588 [2:20:43<109:01:38, 41.81s/it]  2%|▏         | 202/9588 [2:21:24<109:00:18, 41.81s/it]  2%|▏         | 203/9588 [2:22:06<108:59:25, 41.81s/it]  2%|▏         | 204/9588 [2:22:48<108:56:17, 41.79s/it]  2%|▏         | 205/9588 [2:23:30<108:52:47, 41.77s/it]  2%|▏         | 206/9588 [2:24:11<108:45:37, 41.73s/it]  2%|▏         | 207/9588 [2:24:53<108:59:01, 41.82s/it]  2%|▏         | 208/9588 [2:25:35<109:10:11, 41.90s/it]  2%|▏         | 209/9588 [2:26:17<108:59:37, 41.84s/it]  2%|▏         | 210/9588 [2:26:59<109:06:09, 41.88s/it]                                                        {'loss': 0.3604, 'learning_rate': 1.4583333333333333e-05, 'epoch': 0.02}
  2%|▏         | 210/9588 [2:26:59<109:06:09, 41.88s/it]  2%|▏         | 211/9588 [2:27:41<109:04:04, 41.87s/it]  2%|▏         | 212/9588 [2:28:23<109:11:29, 41.93s/it]  2%|▏         | 213/9588 [2:29:05<109:01:36, 41.87s/it]  2%|▏         | 214/9588 [2:29:47<109:11:44, 41.94s/it]  2%|▏         | 215/9588 [2:30:29<109:12:58, 41.95s/it]  2%|▏         | 216/9588 [2:31:10<108:55:21, 41.84s/it]  2%|▏         | 217/9588 [2:31:52<108:48:50, 41.80s/it]  2%|▏         | 218/9588 [2:32:34<108:58:02, 41.87s/it]  2%|▏         | 219/9588 [2:33:16<109:00:22, 41.89s/it]  2%|▏         | 220/9588 [2:33:58<108:51:02, 41.83s/it]                                                        {'loss': 0.3718, 'learning_rate': 1.5277777777777777e-05, 'epoch': 0.02}
  2%|▏         | 220/9588 [2:33:58<108:51:02, 41.83s/it]  2%|▏         | 221/9588 [2:34:39<108:46:56, 41.81s/it]  2%|▏         | 222/9588 [2:35:21<108:44:33, 41.80s/it]  2%|▏         | 223/9588 [2:36:03<108:39:29, 41.77s/it]  2%|▏         | 224/9588 [2:36:45<108:46:17, 41.82s/it]  2%|▏         | 225/9588 [2:37:27<108:38:18, 41.77s/it]  2%|▏         | 226/9588 [2:38:09<108:54:38, 41.88s/it]  2%|▏         | 227/9588 [2:38:51<108:53:15, 41.88s/it]  2%|▏         | 228/9588 [2:39:32<108:54:03, 41.89s/it]  2%|▏         | 229/9588 [2:40:14<108:54:42, 41.89s/it]  2%|▏         | 230/9588 [2:40:56<108:48:33, 41.86s/it]                                                        {'loss': 0.3795, 'learning_rate': 1.5972222222222224e-05, 'epoch': 0.02}
  2%|▏         | 230/9588 [2:40:56<108:48:33, 41.86s/it]  2%|▏         | 231/9588 [2:41:38<109:08:03, 41.99s/it]  2%|▏         | 232/9588 [2:42:20<108:49:18, 41.87s/it]  2%|▏         | 233/9588 [2:43:02<108:57:20, 41.93s/it]  2%|▏         | 234/9588 [2:43:44<109:00:20, 41.95s/it]  2%|▏         | 235/9588 [2:44:26<108:42:56, 41.84s/it]  2%|▏         | 236/9588 [2:45:07<108:40:10, 41.83s/it]  2%|▏         | 237/9588 [2:45:49<108:46:09, 41.87s/it]  2%|▏         | 238/9588 [2:46:31<108:47:37, 41.89s/it]  2%|▏         | 239/9588 [2:47:13<108:40:24, 41.85s/it]  3%|▎         | 240/9588 [2:47:55<108:58:46, 41.97s/it]                                                        {'loss': 0.375, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.03}
  3%|▎         | 240/9588 [2:47:55<108:58:46, 41.97s/it]  3%|▎         | 241/9588 [2:48:37<108:59:29, 41.98s/it]  3%|▎         | 242/9588 [2:49:19<108:40:45, 41.86s/it]  3%|▎         | 243/9588 [2:50:01<108:54:44, 41.96s/it]  3%|▎         | 244/9588 [2:50:43<108:57:41, 41.98s/it]  3%|▎         | 245/9588 [2:51:25<108:57:24, 41.98s/it]  3%|▎         | 246/9588 [2:52:07<109:04:34, 42.03s/it]  3%|▎         | 247/9588 [2:52:49<108:54:43, 41.97s/it]  3%|▎         | 248/9588 [2:53:31<108:46:04, 41.92s/it]  3%|▎         | 249/9588 [2:54:13<108:34:46, 41.86s/it]  3%|▎         | 250/9588 [2:54:54<108:19:58, 41.76s/it]                                                        {'loss': 0.3925, 'learning_rate': 1.7361111111111114e-05, 'epoch': 0.03}
  3%|▎         | 250/9588 [2:54:54<108:19:58, 41.76s/it]  3%|▎         | 251/9588 [2:55:36<108:27:40, 41.82s/it]  3%|▎         | 252/9588 [2:56:19<108:59:08, 42.03s/it]  3%|▎         | 253/9588 [2:57:01<108:50:27, 41.97s/it]  3%|▎         | 254/9588 [2:57:42<108:36:20, 41.89s/it]  3%|▎         | 255/9588 [2:58:24<108:32:07, 41.87s/it]  3%|▎         | 256/9588 [2:59:06<108:20:25, 41.79s/it]  3%|▎         | 257/9588 [2:59:47<108:19:28, 41.79s/it]  3%|▎         | 258/9588 [3:00:29<108:17:43, 41.79s/it]  3%|▎         | 259/9588 [3:01:11<108:16:18, 41.78s/it]  3%|▎         | 260/9588 [3:01:53<108:12:05, 41.76s/it]                                                        {'loss': 0.3933, 'learning_rate': 1.8055555555555558e-05, 'epoch': 0.03}
  3%|▎         | 260/9588 [3:01:53<108:12:05, 41.76s/it]  3%|▎         | 261/9588 [3:02:35<108:24:01, 41.84s/it]  3%|▎         | 262/9588 [3:03:17<108:34:14, 41.91s/it]  3%|▎         | 263/9588 [3:03:59<109:04:41, 42.11s/it]  3%|▎         | 264/9588 [3:04:41<108:58:05, 42.07s/it]  3%|▎         | 265/9588 [3:05:23<108:47:51, 42.01s/it]  3%|▎         | 266/9588 [3:06:05<108:47:56, 42.02s/it]  3%|▎         | 267/9588 [3:06:47<108:47:19, 42.02s/it]  3%|▎         | 268/9588 [3:07:29<108:51:02, 42.05s/it]  3%|▎         | 269/9588 [3:08:11<108:33:46, 41.94s/it]  3%|▎         | 270/9588 [3:08:53<108:37:12, 41.97s/it]                                                        {'loss': 0.4104, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.03}
  3%|▎         | 270/9588 [3:08:53<108:37:12, 41.97s/it]  3%|▎         | 271/9588 [3:09:35<108:31:12, 41.93s/it]  3%|▎         | 272/9588 [3:10:17<108:18:54, 41.86s/it]  3%|▎         | 273/9588 [3:10:58<108:06:46, 41.78s/it]  3%|▎         | 274/9588 [3:11:40<108:22:19, 41.89s/it]  3%|▎         | 275/9588 [3:12:22<108:11:42, 41.82s/it]  3%|▎         | 276/9588 [3:13:04<108:26:07, 41.92s/it]  3%|▎         | 277/9588 [3:13:46<108:24:52, 41.92s/it]  3%|▎         | 278/9588 [3:14:28<108:19:03, 41.88s/it]  3%|▎         | 279/9588 [3:15:10<108:23:09, 41.92s/it]  3%|▎         | 280/9588 [3:15:52<108:41:40, 42.04s/it]                                                        {'loss': 0.4098, 'learning_rate': 1.9444444444444445e-05, 'epoch': 0.03}
  3%|▎         | 280/9588 [3:15:52<108:41:40, 42.04s/it]  3%|▎         | 281/9588 [3:16:34<108:34:37, 42.00s/it]  3%|▎         | 282/9588 [3:17:16<108:48:43, 42.09s/it]  3%|▎         | 283/9588 [3:17:58<108:46:51, 42.09s/it]  3%|▎         | 284/9588 [3:18:41<108:44:55, 42.08s/it]  3%|▎         | 285/9588 [3:19:23<108:53:26, 42.14s/it]  3%|▎         | 286/9588 [3:20:05<108:49:05, 42.11s/it]  3%|▎         | 287/9588 [3:20:47<108:49:36, 42.12s/it]  3%|▎         | 288/9588 [3:21:29<108:41:20, 42.07s/it]  3%|▎         | 289/9588 [3:22:11<108:27:06, 41.99s/it]  3%|▎         | 290/9588 [3:22:53<108:20:18, 41.95s/it]                                                        {'loss': 0.4075, 'learning_rate': 1.999999771774679e-05, 'epoch': 0.03}
  3%|▎         | 290/9588 [3:22:53<108:20:18, 41.95s/it]  3%|▎         | 291/9588 [3:23:35<108:27:14, 42.00s/it]  3%|▎         | 292/9588 [3:24:16<108:12:57, 41.91s/it]  3%|▎         | 293/9588 [3:24:58<108:10:40, 41.90s/it]  3%|▎         | 294/9588 [3:25:40<108:02:27, 41.85s/it]  3%|▎         | 295/9588 [3:26:22<108:05:12, 41.87s/it]  3%|▎         | 296/9588 [3:27:04<108:07:12, 41.89s/it]  3%|▎         | 297/9588 [3:27:46<107:57:26, 41.83s/it]  3%|▎         | 298/9588 [3:28:28<108:35:07, 42.08s/it]  3%|▎         | 299/9588 [3:29:10<108:22:47, 42.00s/it]  3%|▎         | 300/9588 [3:29:52<108:17:52, 41.98s/it]                                                        {'loss': 0.4024, 'learning_rate': 1.9999917838993664e-05, 'epoch': 0.03}
  3%|▎         | 300/9588 [3:29:52<108:17:52, 41.98s/it]  3%|▎         | 301/9588 [3:30:34<108:06:12, 41.91s/it]  3%|▎         | 302/9588 [3:31:16<108:21:50, 42.01s/it]  3%|▎         | 303/9588 [3:31:58<108:21:08, 42.01s/it]  3%|▎         | 304/9588 [3:32:40<108:26:01, 42.05s/it]  3%|▎         | 305/9588 [3:33:22<108:26:27, 42.05s/it]  3%|▎         | 306/9588 [3:34:04<108:13:47, 41.98s/it]  3%|▎         | 307/9588 [3:34:46<108:11:19, 41.97s/it]  3%|▎         | 308/9588 [3:35:28<108:13:35, 41.98s/it]  3%|▎         | 309/9588 [3:36:10<108:10:24, 41.97s/it]  3%|▎         | 310/9588 [3:36:52<108:03:24, 41.93s/it]                                                        {'loss': 0.4163, 'learning_rate': 1.9999723848621558e-05, 'epoch': 0.03}
  3%|▎         | 310/9588 [3:36:52<108:03:24, 41.93s/it]  3%|▎         | 311/9588 [3:37:34<108:15:42, 42.01s/it]  3%|▎         | 312/9588 [3:38:16<107:54:56, 41.88s/it]  3%|▎         | 313/9588 [3:38:57<107:58:35, 41.91s/it]  3%|▎         | 314/9588 [3:39:39<108:00:12, 41.93s/it]  3%|▎         | 315/9588 [3:40:21<107:51:25, 41.87s/it]  3%|▎         | 316/9588 [3:41:03<107:58:25, 41.92s/it]  3%|▎         | 317/9588 [3:41:45<107:45:46, 41.85s/it]  3%|▎         | 318/9588 [3:42:27<107:38:22, 41.80s/it]  3%|▎         | 319/9588 [3:43:08<107:34:39, 41.78s/it]  3%|▎         | 320/9588 [3:43:50<107:29:29, 41.75s/it]                                                        {'loss': 0.4281, 'learning_rate': 1.9999415748844137e-05, 'epoch': 0.03}
  3%|▎         | 320/9588 [3:43:50<107:29:29, 41.75s/it]  3%|▎         | 321/9588 [3:44:32<107:25:08, 41.73s/it]  3%|▎         | 322/9588 [3:45:13<107:25:27, 41.74s/it]  3%|▎         | 323/9588 [3:45:55<107:27:07, 41.75s/it]  3%|▎         | 324/9588 [3:46:37<107:43:58, 41.87s/it]  3%|▎         | 325/9588 [3:47:19<107:38:55, 41.84s/it]  3%|▎         | 326/9588 [3:48:01<107:48:06, 41.90s/it]  3%|▎         | 327/9588 [3:48:43<107:44:13, 41.88s/it]  3%|▎         | 328/9588 [3:49:25<107:31:14, 41.80s/it]  3%|▎         | 329/9588 [3:50:06<107:29:35, 41.79s/it]  3%|▎         | 330/9588 [3:50:48<107:24:12, 41.76s/it]                                                        {'loss': 0.4201, 'learning_rate': 1.999899354317721e-05, 'epoch': 0.03}
  3%|▎         | 330/9588 [3:50:48<107:24:12, 41.76s/it]  3%|▎         | 331/9588 [3:51:31<108:00:29, 42.00s/it]  3%|▎         | 332/9588 [3:52:12<107:48:15, 41.93s/it]  3%|▎         | 333/9588 [3:52:54<107:39:23, 41.88s/it]  3%|▎         | 334/9588 [3:53:36<107:54:42, 41.98s/it]  3%|▎         | 335/9588 [3:54:19<108:03:58, 42.04s/it]  4%|▎         | 336/9588 [3:55:01<108:01:26, 42.03s/it]  4%|▎         | 337/9588 [3:55:43<107:56:07, 42.00s/it]  4%|▎         | 338/9588 [3:56:25<108:02:49, 42.05s/it]  4%|▎         | 339/9588 [3:57:07<107:53:47, 42.00s/it]  4%|▎         | 340/9588 [3:57:49<107:52:13, 41.99s/it]                                                        {'loss': 0.4151, 'learning_rate': 1.9998457236438674e-05, 'epoch': 0.04}
  4%|▎         | 340/9588 [3:57:49<107:52:13, 41.99s/it]  4%|▎         | 341/9588 [3:58:30<107:47:04, 41.96s/it]  4%|▎         | 342/9588 [3:59:12<107:41:17, 41.93s/it]  4%|▎         | 343/9588 [3:59:54<107:42:09, 41.94s/it]  4%|▎         | 344/9588 [4:00:36<107:46:07, 41.97s/it]  4%|▎         | 345/9588 [4:01:18<107:46:39, 41.98s/it]  4%|▎         | 346/9588 [4:02:00<107:43:35, 41.96s/it]  4%|▎         | 347/9588 [4:02:42<107:31:06, 41.89s/it]  4%|▎         | 348/9588 [4:03:24<107:17:50, 41.80s/it]  4%|▎         | 349/9588 [4:04:06<107:34:38, 41.92s/it]  4%|▎         | 350/9588 [4:04:48<107:42:58, 41.98s/it]                                                        {'loss': 0.4162, 'learning_rate': 1.9997806834748455e-05, 'epoch': 0.04}
  4%|▎         | 350/9588 [4:04:48<107:42:58, 41.98s/it]  4%|▎         | 351/9588 [4:05:30<107:45:33, 42.00s/it]  4%|▎         | 352/9588 [4:06:12<107:34:32, 41.93s/it]  4%|▎         | 353/9588 [4:06:54<107:41:16, 41.98s/it]  4%|▎         | 354/9588 [4:07:36<107:56:40, 42.08s/it]  4%|▎         | 355/9588 [4:08:18<108:04:00, 42.14s/it]  4%|▎         | 356/9588 [4:09:00<107:58:16, 42.10s/it]  4%|▎         | 357/9588 [4:09:43<108:11:21, 42.19s/it]  4%|▎         | 358/9588 [4:10:25<108:10:12, 42.19s/it]  4%|▎         | 359/9588 [4:11:07<108:04:44, 42.16s/it]  4%|▍         | 360/9588 [4:11:49<107:41:59, 42.02s/it]                                                        {'loss': 0.4132, 'learning_rate': 1.9997042345528466e-05, 'epoch': 0.04}
  4%|▍         | 360/9588 [4:11:49<107:41:59, 42.02s/it]  4%|▍         | 361/9588 [4:12:31<107:43:30, 42.03s/it]  4%|▍         | 362/9588 [4:13:12<107:21:22, 41.89s/it]  4%|▍         | 363/9588 [4:13:54<107:24:00, 41.91s/it]  4%|▍         | 364/9588 [4:14:36<107:31:25, 41.97s/it]  4%|▍         | 365/9588 [4:15:18<107:19:22, 41.89s/it]  4%|▍         | 366/9588 [4:16:00<107:19:49, 41.90s/it]  4%|▍         | 367/9588 [4:16:42<107:12:09, 41.85s/it]  4%|▍         | 368/9588 [4:17:23<107:01:34, 41.79s/it]  4%|▍         | 369/9588 [4:18:05<106:56:18, 41.76s/it]  4%|▍         | 370/9588 [4:18:47<107:06:24, 41.83s/it]                                                        {'loss': 0.4051, 'learning_rate': 1.9996163777502478e-05, 'epoch': 0.04}
  4%|▍         | 370/9588 [4:18:47<107:06:24, 41.83s/it]  4%|▍         | 371/9588 [4:19:30<107:39:41, 42.05s/it]  4%|▍         | 372/9588 [4:20:12<107:55:14, 42.16s/it]  4%|▍         | 373/9588 [4:20:54<107:34:42, 42.03s/it]  4%|▍         | 374/9588 [4:21:36<107:32:07, 42.02s/it]  4%|▍         | 375/9588 [4:22:19<108:16:04, 42.31s/it]  4%|▍         | 376/9588 [4:23:00<107:38:56, 42.07s/it]  4%|▍         | 377/9588 [4:23:43<107:50:56, 42.15s/it]  4%|▍         | 378/9588 [4:24:24<107:24:08, 41.98s/it]  4%|▍         | 379/9588 [4:25:06<107:19:21, 41.95s/it]  4%|▍         | 380/9588 [4:25:48<107:08:31, 41.89s/it]                                                        {'loss': 0.4139, 'learning_rate': 1.999517114069606e-05, 'epoch': 0.04}
  4%|▍         | 380/9588 [4:25:48<107:08:31, 41.89s/it]  4%|▍         | 381/9588 [4:26:30<107:09:26, 41.90s/it]  4%|▍         | 382/9588 [4:27:12<107:05:19, 41.88s/it]  4%|▍         | 383/9588 [4:27:53<107:00:04, 41.85s/it]  4%|▍         | 384/9588 [4:28:35<106:50:58, 41.79s/it]  4%|▍         | 385/9588 [4:29:18<107:36:30, 42.09s/it]  4%|▍         | 386/9588 [4:30:00<107:52:02, 42.20s/it]  4%|▍         | 387/9588 [4:30:43<107:57:58, 42.24s/it]  4%|▍         | 388/9588 [4:31:24<107:25:12, 42.03s/it]  4%|▍         | 389/9588 [4:32:06<107:03:03, 41.89s/it]  4%|▍         | 390/9588 [4:32:47<106:47:48, 41.80s/it]                                                        {'loss': 0.393, 'learning_rate': 1.9994064446436444e-05, 'epoch': 0.04}
  4%|▍         | 390/9588 [4:32:47<106:47:48, 41.80s/it]  4%|▍         | 391/9588 [4:33:29<106:50:05, 41.82s/it]  4%|▍         | 392/9588 [4:34:11<106:44:29, 41.79s/it]  4%|▍         | 393/9588 [4:34:53<106:51:12, 41.83s/it]  4%|▍         | 394/9588 [4:35:35<106:58:05, 41.88s/it]  4%|▍         | 395/9588 [4:36:16<106:42:52, 41.79s/it]  4%|▍         | 396/9588 [4:36:58<106:50:54, 41.85s/it]  4%|▍         | 397/9588 [4:37:40<106:40:29, 41.78s/it]  4%|▍         | 398/9588 [4:38:21<106:21:21, 41.66s/it]  4%|▍         | 399/9588 [4:39:03<106:08:28, 41.58s/it]  4%|▍         | 400/9588 [4:39:44<105:59:43, 41.53s/it]                                                        {'loss': 0.4079, 'learning_rate': 1.99928437073524e-05, 'epoch': 0.04}
  4%|▍         | 400/9588 [4:39:44<105:59:43, 41.53s/it]  4%|▍         | 401/9588 [4:40:26<105:54:33, 41.50s/it]  4%|▍         | 402/9588 [4:41:07<105:59:15, 41.54s/it]  4%|▍         | 403/9588 [4:41:49<105:56:56, 41.53s/it]  4%|▍         | 404/9588 [4:42:30<105:53:15, 41.51s/it]  4%|▍         | 405/9588 [4:43:12<105:51:32, 41.50s/it]  4%|▍         | 406/9588 [4:43:53<105:46:45, 41.47s/it]  4%|▍         | 407/9588 [4:44:35<105:46:40, 41.48s/it]  4%|▍         | 408/9588 [4:45:16<105:45:23, 41.47s/it]  4%|▍         | 409/9588 [4:45:58<105:46:05, 41.48s/it]  4%|▍         | 410/9588 [4:46:39<105:40:55, 41.45s/it]                                                        {'loss': 0.4, 'learning_rate': 1.9991508937374092e-05, 'epoch': 0.04}
  4%|▍         | 410/9588 [4:46:39<105:40:55, 41.45s/it]  4%|▍         | 411/9588 [4:47:20<105:40:42, 41.46s/it]  4%|▍         | 412/9588 [4:48:02<105:40:05, 41.46s/it]  4%|▍         | 413/9588 [4:48:43<105:42:24, 41.48s/it]  4%|▍         | 414/9588 [4:49:25<105:41:33, 41.48s/it]  4%|▍         | 415/9588 [4:50:06<105:37:17, 41.45s/it]  4%|▍         | 416/9588 [4:50:48<105:32:57, 41.43s/it]  4%|▍         | 417/9588 [4:51:29<105:33:13, 41.43s/it]  4%|▍         | 418/9588 [4:52:11<105:33:27, 41.44s/it]  4%|▍         | 419/9588 [4:52:52<105:28:34, 41.41s/it]  4%|▍         | 420/9588 [4:53:34<105:46:02, 41.53s/it]                                                        {'loss': 0.4036, 'learning_rate': 1.999006015173293e-05, 'epoch': 0.04}
  4%|▍         | 420/9588 [4:53:34<105:46:02, 41.53s/it]  4%|▍         | 421/9588 [4:54:15<105:39:05, 41.49s/it]  4%|▍         | 422/9588 [4:54:57<105:37:20, 41.48s/it]  4%|▍         | 423/9588 [4:55:38<105:33:48, 41.47s/it]  4%|▍         | 424/9588 [4:56:19<105:32:48, 41.46s/it]  4%|▍         | 425/9588 [4:57:01<105:31:56, 41.46s/it]  4%|▍         | 426/9588 [4:57:42<105:29:30, 41.45s/it]  4%|▍         | 427/9588 [4:58:24<105:28:30, 41.45s/it]  4%|▍         | 428/9588 [4:59:05<105:26:41, 41.44s/it]  4%|▍         | 429/9588 [4:59:47<105:25:56, 41.44s/it]  4%|▍         | 430/9588 [5:00:28<105:22:13, 41.42s/it]                                                        {'loss': 0.4172, 'learning_rate': 1.9988497366961366e-05, 'epoch': 0.04}
  4%|▍         | 430/9588 [5:00:28<105:22:13, 41.42s/it]  4%|▍         | 431/9588 [5:01:10<105:31:57, 41.49s/it]  5%|▍         | 432/9588 [5:01:51<105:25:52, 41.45s/it]  5%|▍         | 433/9588 [5:02:32<105:20:43, 41.42s/it]  5%|▍         | 434/9588 [5:03:14<105:18:40, 41.42s/it]  5%|▍         | 435/9588 [5:03:55<105:15:52, 41.40s/it]  5%|▍         | 436/9588 [5:04:37<105:13:35, 41.39s/it]  5%|▍         | 437/9588 [5:05:18<105:14:55, 41.40s/it]  5%|▍         | 438/9588 [5:06:00<105:21:33, 41.45s/it]  5%|▍         | 439/9588 [5:06:41<105:21:34, 41.46s/it]  5%|▍         | 440/9588 [5:07:22<105:17:17, 41.43s/it]                                                        {'loss': 0.39, 'learning_rate': 1.9986820600892743e-05, 'epoch': 0.05}
  5%|▍         | 440/9588 [5:07:22<105:17:17, 41.43s/it]  5%|▍         | 441/9588 [5:08:04<105:11:44, 41.40s/it]  5%|▍         | 442/9588 [5:08:45<105:10:41, 41.40s/it]  5%|▍         | 443/9588 [5:09:26<105:09:23, 41.40s/it]  5%|▍         | 444/9588 [5:10:08<105:09:50, 41.40s/it]  5%|▍         | 445/9588 [5:10:49<105:09:48, 41.41s/it]  5%|▍         | 446/9588 [5:11:31<105:06:06, 41.39s/it]  5%|▍         | 447/9588 [5:12:12<105:15:50, 41.46s/it]  5%|▍         | 448/9588 [5:12:54<105:12:21, 41.44s/it]  5%|▍         | 449/9588 [5:13:36<105:34:08, 41.59s/it]  5%|▍         | 450/9588 [5:14:17<105:21:48, 41.51s/it]                                                        {'loss': 0.392, 'learning_rate': 1.998502987266107e-05, 'epoch': 0.05}
  5%|▍         | 450/9588 [5:14:17<105:21:48, 41.51s/it]  5%|▍         | 451/9588 [5:14:58<105:14:08, 41.46s/it]  5%|▍         | 452/9588 [5:15:40<105:08:48, 41.43s/it]  5%|▍         | 453/9588 [5:16:21<105:02:02, 41.39s/it]  5%|▍         | 454/9588 [5:17:02<104:57:50, 41.37s/it]  5%|▍         | 455/9588 [5:17:44<104:57:20, 41.37s/it]  5%|▍         | 456/9588 [5:18:25<104:58:01, 41.38s/it]  5%|▍         | 457/9588 [5:19:06<104:58:35, 41.39s/it]  5%|▍         | 458/9588 [5:19:48<104:53:59, 41.36s/it]  5%|▍         | 459/9588 [5:20:29<104:52:17, 41.36s/it]  5%|▍         | 460/9588 [5:21:11<105:08:32, 41.47s/it]                                                        {'loss': 0.3981, 'learning_rate': 1.99831252027008e-05, 'epoch': 0.05}
  5%|▍         | 460/9588 [5:21:11<105:08:32, 41.47s/it]  5%|▍         | 461/9588 [5:21:52<105:06:41, 41.46s/it]  5%|▍         | 462/9588 [5:22:34<105:02:43, 41.44s/it]  5%|▍         | 463/9588 [5:23:15<104:59:49, 41.42s/it]  5%|▍         | 464/9588 [5:23:56<104:56:50, 41.41s/it]  5%|▍         | 465/9588 [5:24:38<105:05:51, 41.47s/it]  5%|▍         | 466/9588 [5:25:19<104:59:22, 41.43s/it]  5%|▍         | 467/9588 [5:26:01<104:55:45, 41.41s/it]  5%|▍         | 468/9588 [5:26:42<104:53:33, 41.40s/it]  5%|▍         | 469/9588 [5:27:23<104:48:06, 41.37s/it]  5%|▍         | 470/9588 [5:28:05<104:44:50, 41.36s/it]                                                        {'loss': 0.3798, 'learning_rate': 1.9981106612746606e-05, 'epoch': 0.05}
  5%|▍         | 470/9588 [5:28:05<104:44:50, 41.36s/it]  5%|▍         | 471/9588 [5:28:46<104:58:07, 41.45s/it]  5%|▍         | 472/9588 [5:29:28<104:55:28, 41.44s/it]  5%|▍         | 473/9588 [5:30:09<104:52:58, 41.42s/it]  5%|▍         | 474/9588 [5:30:51<104:53:00, 41.43s/it]  5%|▍         | 475/9588 [5:31:32<104:51:40, 41.42s/it]  5%|▍         | 476/9588 [5:32:14<104:59:38, 41.48s/it]  5%|▍         | 477/9588 [5:32:55<104:50:57, 41.43s/it]  5%|▍         | 478/9588 [5:33:36<104:43:45, 41.39s/it]  5%|▍         | 479/9588 [5:34:18<104:41:46, 41.38s/it]  5%|▌         | 480/9588 [5:34:59<104:41:10, 41.38s/it]                                                        {'loss': 0.3907, 'learning_rate': 1.9978974125833136e-05, 'epoch': 0.05}
  5%|▌         | 480/9588 [5:34:59<104:41:10, 41.38s/it]  5%|▌         | 481/9588 [5:35:40<104:38:09, 41.36s/it]  5%|▌         | 482/9588 [5:36:22<104:35:51, 41.35s/it]  5%|▌         | 483/9588 [5:37:03<104:40:35, 41.39s/it]  5%|▌         | 484/9588 [5:37:44<104:37:55, 41.37s/it]  5%|▌         | 485/9588 [5:38:26<104:37:02, 41.37s/it]  5%|▌         | 486/9588 [5:39:07<104:41:06, 41.40s/it]  5%|▌         | 487/9588 [5:39:49<104:51:52, 41.48s/it]  5%|▌         | 488/9588 [5:40:30<104:46:13, 41.45s/it]  5%|▌         | 489/9588 [5:41:12<104:41:57, 41.42s/it]  5%|▌         | 490/9588 [5:41:53<104:39:41, 41.41s/it]                                                        {'loss': 0.4134, 'learning_rate': 1.997672776629475e-05, 'epoch': 0.05}
  5%|▌         | 490/9588 [5:41:53<104:39:41, 41.41s/it]  5%|▌         | 491/9588 [5:42:34<104:37:07, 41.40s/it]  5%|▌         | 492/9588 [5:43:16<104:41:40, 41.44s/it]  5%|▌         | 493/9588 [5:43:57<104:34:55, 41.40s/it]  5%|▌         | 494/9588 [5:44:39<104:31:16, 41.38s/it]  5%|▌         | 495/9588 [5:45:20<104:27:59, 41.36s/it]  5%|▌         | 496/9588 [5:46:01<104:26:30, 41.35s/it]  5%|▌         | 497/9588 [5:46:43<104:23:36, 41.34s/it]  5%|▌         | 498/9588 [5:47:24<104:32:59, 41.41s/it]  5%|▌         | 499/9588 [5:48:06<104:31:17, 41.40s/it]  5%|▌         | 500/9588 [5:48:47<104:27:52, 41.38s/it]                                                        {'loss': 0.4154, 'learning_rate': 1.9974367559765223e-05, 'epoch': 0.05}
  5%|▌         | 500/9588 [5:48:47<104:27:52, 41.38s/it]  5%|▌         | 501/9588 [5:49:28<104:28:02, 41.39s/it]  5%|▌         | 502/9588 [5:50:10<104:42:40, 41.49s/it]  5%|▌         | 503/9588 [5:50:51<104:36:25, 41.45s/it]  5%|▌         | 504/9588 [5:51:33<104:31:59, 41.43s/it]  5%|▌         | 505/9588 [5:52:14<104:30:41, 41.42s/it]  5%|▌         | 506/9588 [5:52:56<104:30:47, 41.43s/it]  5%|▌         | 507/9588 [5:53:37<104:38:01, 41.48s/it]  5%|▌         | 508/9588 [5:54:19<104:35:55, 41.47s/it]  5%|▌         | 509/9588 [5:55:00<104:32:45, 41.45s/it]  5%|▌         | 510/9588 [5:55:42<104:31:36, 41.45s/it]                                                        {'loss': 0.4061, 'learning_rate': 1.997189353317748e-05, 'epoch': 0.05}
  5%|▌         | 510/9588 [5:55:42<104:31:36, 41.45s/it]  5%|▌         | 511/9588 [5:56:23<104:29:22, 41.44s/it]  5%|▌         | 512/9588 [5:57:04<104:27:22, 41.43s/it]  5%|▌         | 513/9588 [5:57:46<104:28:15, 41.44s/it]  5%|▌         | 514/9588 [5:58:27<104:31:01, 41.47s/it]  5%|▌         | 515/9588 [5:59:09<104:24:43, 41.43s/it]  5%|▌         | 516/9588 [5:59:50<104:24:21, 41.43s/it]  5%|▌         | 517/9588 [6:00:32<104:22:53, 41.43s/it]  5%|▌         | 518/9588 [6:01:13<104:34:48, 41.51s/it]  5%|▌         | 519/9588 [6:01:55<104:27:58, 41.47s/it]  5%|▌         | 520/9588 [6:02:36<104:23:14, 41.44s/it]                                                        {'loss': 0.3994, 'learning_rate': 1.9969305714763266e-05, 'epoch': 0.05}
  5%|▌         | 520/9588 [6:02:36<104:23:14, 41.44s/it]  5%|▌         | 521/9588 [6:03:17<104:20:17, 41.43s/it]  5%|▌         | 522/9588 [6:03:59<104:16:23, 41.41s/it]  5%|▌         | 523/9588 [6:04:40<104:19:48, 41.43s/it]  5%|▌         | 524/9588 [6:05:22<104:20:17, 41.44s/it]  5%|▌         | 525/9588 [6:06:03<104:17:16, 41.43s/it]  5%|▌         | 526/9588 [6:06:44<104:15:26, 41.42s/it]  5%|▌         | 527/9588 [6:07:26<104:20:52, 41.46s/it]  6%|▌         | 528/9588 [6:08:07<104:18:16, 41.45s/it]  6%|▌         | 529/9588 [6:08:49<104:21:03, 41.47s/it]  6%|▌         | 530/9588 [6:09:30<104:15:54, 41.44s/it]                                                        {'loss': 0.4026, 'learning_rate': 1.9966604134052848e-05, 'epoch': 0.06}
  6%|▌         | 530/9588 [6:09:30<104:15:54, 41.44s/it]  6%|▌         | 531/9588 [6:10:12<104:12:25, 41.42s/it]  6%|▌         | 532/9588 [6:10:53<104:11:24, 41.42s/it]  6%|▌         | 533/9588 [6:11:35<104:11:31, 41.42s/it]  6%|▌         | 534/9588 [6:12:16<104:24:35, 41.51s/it]  6%|▌         | 535/9588 [6:12:58<104:14:27, 41.45s/it]  6%|▌         | 536/9588 [6:13:39<104:12:24, 41.44s/it]  6%|▌         | 537/9588 [6:14:20<104:04:58, 41.40s/it]  6%|▌         | 538/9588 [6:15:02<104:00:08, 41.37s/it]  6%|▌         | 539/9588 [6:15:43<103:53:34, 41.33s/it]  6%|▌         | 540/9588 [6:16:24<103:54:46, 41.34s/it]                                                        {'loss': 0.4171, 'learning_rate': 1.996378882187464e-05, 'epoch': 0.06}
  6%|▌         | 540/9588 [6:16:24<103:54:46, 41.34s/it]  6%|▌         | 541/9588 [6:17:06<103:55:56, 41.36s/it]  6%|▌         | 542/9588 [6:17:47<103:53:27, 41.35s/it]  6%|▌         | 543/9588 [6:18:28<103:51:47, 41.34s/it]  6%|▌         | 544/9588 [6:19:10<103:55:07, 41.37s/it]  6%|▌         | 545/9588 [6:19:51<104:00:34, 41.41s/it]  6%|▌         | 546/9588 [6:20:32<103:54:30, 41.37s/it]  6%|▌         | 547/9588 [6:21:14<103:51:29, 41.35s/it]  6%|▌         | 548/9588 [6:21:55<103:53:18, 41.37s/it]  6%|▌         | 549/9588 [6:22:36<103:47:45, 41.34s/it]  6%|▌         | 550/9588 [6:23:18<103:47:47, 41.34s/it]                                                        {'loss': 0.3914, 'learning_rate': 1.99608598103549e-05, 'epoch': 0.06}
  6%|▌         | 550/9588 [6:23:18<103:47:47, 41.34s/it]  6%|▌         | 551/9588 [6:23:59<103:47:17, 41.35s/it]  6%|▌         | 552/9588 [6:24:40<103:42:56, 41.32s/it]  6%|▌         | 553/9588 [6:25:22<103:50:51, 41.38s/it]  6%|▌         | 554/9588 [6:26:03<103:48:28, 41.37s/it]  6%|▌         | 555/9588 [6:26:45<103:51:25, 41.39s/it]  6%|▌         | 556/9588 [6:27:26<103:48:00, 41.37s/it]  6%|▌         | 557/9588 [6:28:07<103:47:24, 41.37s/it]  6%|▌         | 558/9588 [6:28:49<103:46:29, 41.37s/it]  6%|▌         | 559/9588 [6:29:30<103:51:15, 41.41s/it]  6%|▌         | 560/9588 [6:30:12<103:46:23, 41.38s/it]                                                        {'loss': 0.3884, 'learning_rate': 1.9957817132917325e-05, 'epoch': 0.06}
  6%|▌         | 560/9588 [6:30:12<103:46:23, 41.38s/it]  6%|▌         | 561/9588 [6:30:53<103:41:46, 41.35s/it]  6%|▌         | 562/9588 [6:31:34<103:43:00, 41.37s/it]  6%|▌         | 563/9588 [6:32:16<103:41:14, 41.36s/it]  6%|▌         | 564/9588 [6:32:57<103:49:19, 41.42s/it]  6%|▌         | 565/9588 [6:33:38<103:42:39, 41.38s/it]  6%|▌         | 566/9588 [6:34:20<103:40:57, 41.37s/it]  6%|▌         | 567/9588 [6:35:01<103:36:16, 41.35s/it]  6%|▌         | 568/9588 [6:35:42<103:32:48, 41.33s/it]  6%|▌         | 569/9588 [6:36:24<103:28:29, 41.30s/it]  6%|▌         | 570/9588 [6:37:05<103:36:39, 41.36s/it]                                                        {'loss': 0.3942, 'learning_rate': 1.9954660824282687e-05, 'epoch': 0.06}
  6%|▌         | 570/9588 [6:37:05<103:36:39, 41.36s/it]  6%|▌         | 571/9588 [6:37:46<103:34:15, 41.35s/it]  6%|▌         | 572/9588 [6:38:28<103:29:16, 41.32s/it]  6%|▌         | 573/9588 [6:39:09<103:30:37, 41.34s/it]  6%|▌         | 574/9588 [6:39:50<103:31:32, 41.35s/it]  6%|▌         | 575/9588 [6:40:32<103:33:46, 41.37s/it]  6%|▌         | 576/9588 [6:41:13<103:32:37, 41.36s/it]  6%|▌         | 577/9588 [6:41:55<103:29:10, 41.34s/it]  6%|▌         | 578/9588 [6:42:36<103:32:57, 41.37s/it]  6%|▌         | 579/9588 [6:43:17<103:30:20, 41.36s/it]  6%|▌         | 580/9588 [6:43:59<103:24:15, 41.33s/it]                                                        {'loss': 0.3902, 'learning_rate': 1.9951390920468423e-05, 'epoch': 0.06}
  6%|▌         | 580/9588 [6:43:59<103:24:15, 41.33s/it]  6%|▌         | 581/9588 [6:44:40<103:25:41, 41.34s/it]  6%|▌         | 582/9588 [6:45:21<103:24:27, 41.34s/it]  6%|▌         | 583/9588 [6:46:03<103:24:10, 41.34s/it]  6%|▌         | 584/9588 [6:46:44<103:23:30, 41.34s/it]  6%|▌         | 585/9588 [6:47:25<103:21:42, 41.33s/it]  6%|▌         | 586/9588 [6:48:07<103:26:40, 41.37s/it]  6%|▌         | 587/9588 [6:48:48<103:24:53, 41.36s/it]  6%|▌         | 588/9588 [6:49:29<103:23:48, 41.36s/it]  6%|▌         | 589/9588 [6:50:11<103:22:58, 41.36s/it]  6%|▌         | 590/9588 [6:50:52<103:24:44, 41.37s/it]                                                        {'loss': 0.3934, 'learning_rate': 1.994800745878825e-05, 'epoch': 0.06}
  6%|▌         | 590/9588 [6:50:52<103:24:44, 41.37s/it]  6%|▌         | 591/9588 [6:51:34<103:35:28, 41.45s/it]  6%|▌         | 592/9588 [6:52:15<103:30:09, 41.42s/it]  6%|▌         | 593/9588 [6:52:56<103:26:24, 41.40s/it]  6%|▌         | 594/9588 [6:53:38<103:25:08, 41.40s/it]  6%|▌         | 595/9588 [6:54:19<103:17:51, 41.35s/it]  6%|▌         | 596/9588 [6:55:01<103:27:07, 41.42s/it]  6%|▌         | 597/9588 [6:55:42<103:24:15, 41.40s/it]  6%|▌         | 598/9588 [6:56:23<103:19:07, 41.37s/it]  6%|▌         | 599/9588 [6:57:05<103:15:53, 41.36s/it]  6%|▋         | 600/9588 [6:57:46<103:13:56, 41.35s/it]                                                        {'loss': 0.3968, 'learning_rate': 1.9944510477851705e-05, 'epoch': 0.06}
  6%|▋         | 600/9588 [6:57:46<103:13:56, 41.35s/it]  6%|▋         | 601/9588 [6:58:27<103:17:20, 41.38s/it]  6%|▋         | 602/9588 [6:59:09<103:15:30, 41.37s/it]  6%|▋         | 603/9588 [6:59:50<103:08:31, 41.33s/it]  6%|▋         | 604/9588 [7:00:31<103:05:21, 41.31s/it]  6%|▋         | 605/9588 [7:01:13<103:08:09, 41.33s/it]  6%|▋         | 606/9588 [7:01:54<103:06:41, 41.33s/it]  6%|▋         | 607/9588 [7:02:36<103:14:52, 41.39s/it]  6%|▋         | 608/9588 [7:03:17<103:10:33, 41.36s/it]  6%|▋         | 609/9588 [7:03:58<103:07:53, 41.35s/it]  6%|▋         | 610/9588 [7:04:39<103:05:09, 41.34s/it]                                                        {'loss': 0.3999, 'learning_rate': 1.9940900017563737e-05, 'epoch': 0.06}
  6%|▋         | 610/9588 [7:04:39<103:05:09, 41.34s/it]  6%|▋         | 611/9588 [7:05:21<103:04:24, 41.34s/it]  6%|▋         | 612/9588 [7:06:02<103:03:53, 41.34s/it]  6%|▋         | 613/9588 [7:06:43<103:02:02, 41.33s/it]  6%|▋         | 614/9588 [7:07:25<103:12:04, 41.40s/it]  6%|▋         | 615/9588 [7:08:06<103:07:08, 41.37s/it]  6%|▋         | 616/9588 [7:08:48<103:03:37, 41.35s/it]  6%|▋         | 617/9588 [7:09:29<102:59:00, 41.33s/it]  6%|▋         | 618/9588 [7:10:10<102:56:36, 41.32s/it]  6%|▋         | 619/9588 [7:10:52<102:58:26, 41.33s/it]  6%|▋         | 620/9588 [7:11:33<103:01:55, 41.36s/it]                                                        {'loss': 0.3817, 'learning_rate': 1.9937176119124237e-05, 'epoch': 0.06}
  6%|▋         | 620/9588 [7:11:33<103:01:55, 41.36s/it]  6%|▋         | 621/9588 [7:12:14<103:01:45, 41.36s/it]  6%|▋         | 622/9588 [7:12:56<103:00:20, 41.36s/it]  6%|▋         | 623/9588 [7:13:37<102:54:52, 41.33s/it]  7%|▋         | 624/9588 [7:14:19<103:11:40, 41.44s/it]  7%|▋         | 625/9588 [7:15:00<103:10:09, 41.44s/it]  7%|▋         | 626/9588 [7:15:42<103:08:56, 41.43s/it]  7%|▋         | 627/9588 [7:16:23<103:09:29, 41.44s/it]  7%|▋         | 628/9588 [7:17:04<103:05:28, 41.42s/it]  7%|▋         | 629/9588 [7:17:46<103:00:44, 41.39s/it]  7%|▋         | 630/9588 [7:18:27<103:02:58, 41.41s/it]                                                        {'loss': 0.3917, 'learning_rate': 1.993333882502755e-05, 'epoch': 0.07}
  7%|▋         | 630/9588 [7:18:27<103:02:58, 41.41s/it]  7%|▋         | 631/9588 [7:19:08<102:57:17, 41.38s/it]  7%|▋         | 632/9588 [7:19:50<102:55:14, 41.37s/it]  7%|▋         | 633/9588 [7:20:31<102:51:15, 41.35s/it]  7%|▋         | 634/9588 [7:21:12<102:48:54, 41.34s/it]  7%|▋         | 635/9588 [7:21:54<102:51:38, 41.36s/it]  7%|▋         | 636/9588 [7:22:35<102:56:40, 41.40s/it]  7%|▋         | 637/9588 [7:23:17<102:53:25, 41.38s/it]  7%|▋         | 638/9588 [7:23:58<102:51:33, 41.37s/it]  7%|▋         | 639/9588 [7:24:39<102:51:44, 41.38s/it]  7%|▋         | 640/9588 [7:25:21<102:51:26, 41.38s/it]                                                        {'loss': 0.3967, 'learning_rate': 1.9929388179062033e-05, 'epoch': 0.07}
  7%|▋         | 640/9588 [7:25:21<102:51:26, 41.38s/it]  7%|▋         | 641/9588 [7:26:02<102:50:05, 41.38s/it]  7%|▋         | 642/9588 [7:26:43<102:48:01, 41.37s/it]  7%|▋         | 643/9588 [7:27:25<102:46:28, 41.36s/it]  7%|▋         | 644/9588 [7:28:06<102:47:59, 41.38s/it]  7%|▋         | 645/9588 [7:28:48<102:46:03, 41.37s/it]  7%|▋         | 646/9588 [7:29:29<102:42:31, 41.35s/it]  7%|▋         | 647/9588 [7:30:10<102:41:22, 41.35s/it]  7%|▋         | 648/9588 [7:30:52<102:52:23, 41.43s/it]  7%|▋         | 649/9588 [7:31:33<102:49:41, 41.41s/it]  7%|▋         | 650/9588 [7:32:15<102:46:16, 41.39s/it]                                                        {'loss': 0.384, 'learning_rate': 1.9925324226309507e-05, 'epoch': 0.07}
  7%|▋         | 650/9588 [7:32:15<102:46:16, 41.39s/it]  7%|▋         | 651/9588 [7:32:56<102:43:07, 41.38s/it]  7%|▋         | 652/9588 [7:33:37<102:46:10, 41.40s/it]  7%|▋         | 653/9588 [7:34:19<102:41:11, 41.37s/it]  7%|▋         | 654/9588 [7:35:00<102:41:17, 41.38s/it]  7%|▋         | 655/9588 [7:35:41<102:41:19, 41.38s/it]  7%|▋         | 656/9588 [7:36:23<102:44:31, 41.41s/it]  7%|▋         | 657/9588 [7:37:04<102:42:29, 41.40s/it]  7%|▋         | 658/9588 [7:37:46<102:40:00, 41.39s/it]  7%|▋         | 659/9588 [7:38:27<102:37:11, 41.37s/it]  7%|▋         | 660/9588 [7:39:08<102:38:15, 41.39s/it]                                                        {'loss': 0.4034, 'learning_rate': 1.9921147013144782e-05, 'epoch': 0.07}
  7%|▋         | 660/9588 [7:39:08<102:38:15, 41.39s/it]  7%|▋         | 661/9588 [7:39:50<102:37:13, 41.38s/it]  7%|▋         | 662/9588 [7:40:31<102:37:41, 41.39s/it]  7%|▋         | 663/9588 [7:41:13<102:37:28, 41.39s/it]  7%|▋         | 664/9588 [7:41:54<102:34:30, 41.38s/it]  7%|▋         | 665/9588 [7:42:35<102:30:48, 41.36s/it]  7%|▋         | 666/9588 [7:43:17<102:32:02, 41.37s/it]  7%|▋         | 667/9588 [7:43:58<102:31:25, 41.37s/it]  7%|▋         | 668/9588 [7:44:39<102:29:54, 41.37s/it]  7%|▋         | 669/9588 [7:45:21<102:30:46, 41.38s/it]  7%|▋         | 670/9588 [7:46:02<102:26:46, 41.36s/it]                                                        {'loss': 0.4044, 'learning_rate': 1.99168565872351e-05, 'epoch': 0.07}
  7%|▋         | 670/9588 [7:46:02<102:26:46, 41.36s/it]  7%|▋         | 671/9588 [7:46:43<102:25:04, 41.35s/it]  7%|▋         | 672/9588 [7:47:25<102:23:15, 41.34s/it]  7%|▋         | 673/9588 [7:48:06<102:20:39, 41.33s/it]  7%|▋         | 674/9588 [7:48:47<102:23:16, 41.35s/it]  7%|▋         | 675/9588 [7:49:29<102:24:57, 41.37s/it]  7%|▋         | 676/9588 [7:50:10<102:27:12, 41.39s/it]  7%|▋         | 677/9588 [7:50:52<102:23:23, 41.37s/it]  7%|▋         | 678/9588 [7:51:33<102:19:05, 41.34s/it]  7%|▋         | 679/9588 [7:52:14<102:20:38, 41.36s/it]  7%|▋         | 680/9588 [7:52:56<102:17:56, 41.34s/it]                                                        {'loss': 0.3858, 'learning_rate': 1.991245299753961e-05, 'epoch': 0.07}
  7%|▋         | 680/9588 [7:52:56<102:17:56, 41.34s/it]  7%|▋         | 681/9588 [7:53:37<102:21:22, 41.37s/it]  7%|▋         | 682/9588 [7:54:18<102:20:10, 41.37s/it]  7%|▋         | 683/9588 [7:55:00<102:22:31, 41.39s/it]  7%|▋         | 684/9588 [7:55:41<102:19:03, 41.37s/it]  7%|▋         | 685/9588 [7:56:23<102:16:39, 41.36s/it]  7%|▋         | 686/9588 [7:57:04<102:17:14, 41.37s/it]  7%|▋         | 687/9588 [7:57:45<102:13:12, 41.34s/it]  7%|▋         | 688/9588 [7:58:27<102:14:57, 41.36s/it]  7%|▋         | 689/9588 [7:59:08<102:12:21, 41.35s/it]  7%|▋         | 690/9588 [7:59:49<102:13:26, 41.36s/it]                                                        {'loss': 0.3945, 'learning_rate': 1.9907936294308802e-05, 'epoch': 0.07}
  7%|▋         | 690/9588 [7:59:49<102:13:26, 41.36s/it]  7%|▋         | 691/9588 [8:00:31<102:21:00, 41.41s/it]  7%|▋         | 692/9588 [8:01:12<102:14:56, 41.38s/it]  7%|▋         | 693/9588 [8:01:54<102:15:07, 41.38s/it]  7%|▋         | 694/9588 [8:02:35<102:12:01, 41.37s/it]  7%|▋         | 695/9588 [8:03:16<102:10:46, 41.36s/it]  7%|▋         | 696/9588 [8:03:58<102:11:22, 41.37s/it]  7%|▋         | 697/9588 [8:04:39<102:09:14, 41.36s/it]  7%|▋         | 698/9588 [8:05:20<102:08:27, 41.36s/it]  7%|▋         | 699/9588 [8:06:02<102:03:58, 41.34s/it]  7%|▋         | 700/9588 [8:06:43<101:59:30, 41.31s/it]                                                        {'loss': 0.3821, 'learning_rate': 1.9903306529083927e-05, 'epoch': 0.07}
  7%|▋         | 700/9588 [8:06:43<101:59:30, 41.31s/it]  7%|▋         | 701/9588 [8:07:24<102:03:42, 41.34s/it]  7%|▋         | 702/9588 [8:08:06<102:00:51, 41.33s/it]  7%|▋         | 703/9588 [8:08:47<102:03:08, 41.35s/it]  7%|▋         | 704/9588 [8:09:28<102:04:49, 41.37s/it]  7%|▋         | 705/9588 [8:10:10<102:04:32, 41.37s/it]  7%|▋         | 706/9588 [8:10:51<102:04:51, 41.37s/it]  7%|▋         | 707/9588 [8:11:32<102:04:44, 41.38s/it]  7%|▋         | 708/9588 [8:12:14<102:02:34, 41.37s/it]  7%|▋         | 709/9588 [8:12:55<102:03:38, 41.38s/it]  7%|▋         | 710/9588 [8:13:37<102:04:53, 41.39s/it]                                                        {'loss': 0.386, 'learning_rate': 1.9898563754696427e-05, 'epoch': 0.07}
  7%|▋         | 710/9588 [8:13:37<102:04:53, 41.39s/it]  7%|▋         | 711/9588 [8:14:18<102:03:03, 41.39s/it]  7%|▋         | 712/9588 [8:14:59<101:58:47, 41.36s/it]  7%|▋         | 713/9588 [8:15:41<101:55:01, 41.34s/it]  7%|▋         | 714/9588 [8:16:22<101:54:31, 41.34s/it]  7%|▋         | 715/9588 [8:17:03<101:53:33, 41.34s/it]  7%|▋         | 716/9588 [8:17:45<101:50:12, 41.32s/it]  7%|▋         | 717/9588 [8:18:26<101:56:17, 41.37s/it]  7%|▋         | 718/9588 [8:19:07<101:52:03, 41.34s/it]  7%|▋         | 719/9588 [8:19:49<101:50:40, 41.34s/it]  8%|▊         | 720/9588 [8:20:30<101:52:48, 41.36s/it]                                                        {'loss': 0.3758, 'learning_rate': 1.9893708025267305e-05, 'epoch': 0.08}
  8%|▊         | 720/9588 [8:20:30<101:52:48, 41.36s/it]  8%|▊         | 721/9588 [8:21:11<101:50:46, 41.35s/it]  8%|▊         | 722/9588 [8:21:53<101:52:33, 41.37s/it]  8%|▊         | 723/9588 [8:22:34<101:50:19, 41.36s/it]  8%|▊         | 724/9588 [8:23:16<101:51:08, 41.37s/it]  8%|▊         | 725/9588 [8:23:57<101:46:54, 41.34s/it]  8%|▊         | 726/9588 [8:24:38<101:47:28, 41.35s/it]  8%|▊         | 727/9588 [8:25:20<101:50:25, 41.38s/it]  8%|▊         | 728/9588 [8:26:01<101:49:41, 41.37s/it]  8%|▊         | 729/9588 [8:26:42<101:45:16, 41.35s/it]  8%|▊         | 730/9588 [8:27:24<101:45:25, 41.36s/it]                                                        {'loss': 0.377, 'learning_rate': 1.9888739396206534e-05, 'epoch': 0.08}
  8%|▊         | 730/9588 [8:27:24<101:45:25, 41.36s/it]  8%|▊         | 731/9588 [8:28:05<101:46:36, 41.37s/it]  8%|▊         | 732/9588 [8:28:47<101:52:20, 41.41s/it]  8%|▊         | 733/9588 [8:29:28<101:47:06, 41.38s/it]  8%|▊         | 734/9588 [8:30:09<101:44:28, 41.37s/it]  8%|▊         | 735/9588 [8:30:51<101:44:24, 41.37s/it]  8%|▊         | 736/9588 [8:31:32<101:42:56, 41.37s/it]  8%|▊         | 737/9588 [8:32:14<101:58:05, 41.47s/it]  8%|▊         | 738/9588 [8:32:55<101:49:21, 41.42s/it]  8%|▊         | 739/9588 [8:33:36<101:45:07, 41.40s/it]  8%|▊         | 740/9588 [8:34:18<101:50:53, 41.44s/it]                                                        {'loss': 0.3819, 'learning_rate': 1.988365792421241e-05, 'epoch': 0.08}
  8%|▊         | 740/9588 [8:34:18<101:50:53, 41.44s/it]  8%|▊         | 741/9588 [8:34:59<101:46:19, 41.41s/it]  8%|▊         | 742/9588 [8:35:41<101:44:55, 41.41s/it]  8%|▊         | 743/9588 [8:36:22<101:43:11, 41.40s/it]  8%|▊         | 744/9588 [8:37:03<101:40:25, 41.39s/it]  8%|▊         | 745/9588 [8:37:45<101:38:51, 41.38s/it]  8%|▊         | 746/9588 [8:38:26<101:36:32, 41.37s/it]  8%|▊         | 747/9588 [8:39:07<101:35:05, 41.36s/it]  8%|▊         | 748/9588 [8:39:49<101:32:14, 41.35s/it]  8%|▊         | 749/9588 [8:40:30<101:33:48, 41.37s/it]  8%|▊         | 750/9588 [8:41:11<101:32:55, 41.36s/it]                                                        {'loss': 0.3879, 'learning_rate': 1.9878463667270914e-05, 'epoch': 0.08}
  8%|▊         | 750/9588 [8:41:11<101:32:55, 41.36s/it]  8%|▊         | 751/9588 [8:41:53<101:31:08, 41.36s/it]  8%|▊         | 752/9588 [8:42:34<101:31:32, 41.36s/it]  8%|▊         | 753/9588 [8:43:16<101:30:32, 41.36s/it]  8%|▊         | 754/9588 [8:43:57<101:31:16, 41.37s/it]  8%|▊         | 755/9588 [8:44:38<101:29:56, 41.37s/it]  8%|▊         | 756/9588 [8:45:20<101:29:11, 41.37s/it]  8%|▊         | 757/9588 [8:46:01<101:31:32, 41.39s/it]  8%|▊         | 758/9588 [8:46:42<101:27:58, 41.37s/it]  8%|▊         | 759/9588 [8:47:24<101:27:19, 41.37s/it]  8%|▊         | 760/9588 [8:48:05<101:25:55, 41.36s/it]                                                        {'loss': 0.3773, 'learning_rate': 1.987315668465503e-05, 'epoch': 0.08}
  8%|▊         | 760/9588 [8:48:05<101:25:55, 41.36s/it]  8%|▊         | 761/9588 [8:48:47<101:27:00, 41.38s/it]  8%|▊         | 762/9588 [8:49:28<101:25:07, 41.37s/it]  8%|▊         | 763/9588 [8:50:09<101:26:10, 41.38s/it]  8%|▊         | 764/9588 [8:50:51<101:23:00, 41.36s/it]  8%|▊         | 765/9588 [8:51:32<101:22:54, 41.37s/it]  8%|▊         | 766/9588 [8:52:13<101:20:20, 41.35s/it]  8%|▊         | 767/9588 [8:52:55<101:20:13, 41.36s/it]  8%|▊         | 768/9588 [8:53:36<101:18:28, 41.35s/it]  8%|▊         | 769/9588 [8:54:17<101:19:38, 41.36s/it]  8%|▊         | 770/9588 [8:54:59<101:18:36, 41.36s/it]                                                        {'loss': 0.4021, 'learning_rate': 1.9867737036924105e-05, 'epoch': 0.08}
  8%|▊         | 770/9588 [8:54:59<101:18:36, 41.36s/it]  8%|▊         | 771/9588 [8:55:40<101:20:15, 41.38s/it]  8%|▊         | 772/9588 [8:56:22<101:21:00, 41.39s/it]  8%|▊         | 773/9588 [8:57:03<101:17:57, 41.37s/it]  8%|▊         | 774/9588 [8:57:44<101:15:12, 41.36s/it]  8%|▊         | 775/9588 [8:58:26<101:16:35, 41.37s/it]  8%|▊         | 776/9588 [8:59:07<101:12:45, 41.35s/it]  8%|▊         | 777/9588 [8:59:48<101:12:47, 41.35s/it]  8%|▊         | 778/9588 [9:00:30<101:09:56, 41.34s/it]  8%|▊         | 779/9588 [9:01:11<101:08:55, 41.34s/it]  8%|▊         | 780/9588 [9:01:52<101:09:00, 41.34s/it]                                                        {'loss': 0.3929, 'learning_rate': 1.9862204785923117e-05, 'epoch': 0.08}
  8%|▊         | 780/9588 [9:01:52<101:09:00, 41.34s/it]  8%|▊         | 781/9588 [9:02:34<101:07:32, 41.34s/it]  8%|▊         | 782/9588 [9:03:15<101:06:22, 41.33s/it]  8%|▊         | 783/9588 [9:03:56<101:06:21, 41.34s/it]  8%|▊         | 784/9588 [9:04:38<101:06:33, 41.34s/it]  8%|▊         | 785/9588 [9:05:19<101:04:06, 41.33s/it]  8%|▊         | 786/9588 [9:06:01<101:11:50, 41.39s/it]  8%|▊         | 787/9588 [9:06:42<101:11:50, 41.39s/it]  8%|▊         | 788/9588 [9:07:23<101:16:45, 41.43s/it]  8%|▊         | 789/9588 [9:08:05<101:24:22, 41.49s/it]  8%|▊         | 790/9588 [9:08:47<101:36:43, 41.58s/it]                                                        {'loss': 0.3971, 'learning_rate': 1.9856559994782e-05, 'epoch': 0.08}
  8%|▊         | 790/9588 [9:08:47<101:36:43, 41.58s/it]  8%|▊         | 791/9588 [9:09:28<101:39:09, 41.60s/it]  8%|▊         | 792/9588 [9:10:10<101:44:23, 41.64s/it]  8%|▊         | 793/9588 [9:10:52<101:37:23, 41.60s/it]  8%|▊         | 794/9588 [9:11:33<101:37:39, 41.60s/it]  8%|▊         | 795/9588 [9:12:15<101:31:38, 41.57s/it]  8%|▊         | 796/9588 [9:12:56<101:24:58, 41.53s/it]  8%|▊         | 797/9588 [9:13:38<101:23:06, 41.52s/it]  8%|▊         | 798/9588 [9:14:20<101:38:09, 41.63s/it]  8%|▊         | 799/9588 [9:15:01<101:29:01, 41.57s/it]  8%|▊         | 800/9588 [9:15:42<101:22:31, 41.53s/it]                                                        {'loss': 0.3921, 'learning_rate': 1.9850802727914902e-05, 'epoch': 0.08}
  8%|▊         | 800/9588 [9:15:42<101:22:31, 41.53s/it]  8%|▊         | 801/9588 [9:16:24<101:15:36, 41.49s/it]  8%|▊         | 802/9588 [9:17:05<101:15:42, 41.49s/it]  8%|▊         | 803/9588 [9:17:47<101:10:34, 41.46s/it]  8%|▊         | 804/9588 [9:18:28<101:08:20, 41.45s/it]  8%|▊         | 805/9588 [9:19:10<101:07:27, 41.45s/it]  8%|▊         | 806/9588 [9:19:51<101:16:46, 41.52s/it]  8%|▊         | 807/9588 [9:20:33<101:14:14, 41.50s/it]  8%|▊         | 808/9588 [9:21:14<101:09:45, 41.48s/it]  8%|▊         | 809/9588 [9:21:56<101:03:36, 41.44s/it]  8%|▊         | 810/9588 [9:22:37<101:07:39, 41.47s/it]                                                        {'loss': 0.3856, 'learning_rate': 1.9844933051019475e-05, 'epoch': 0.08}
  8%|▊         | 810/9588 [9:22:37<101:07:39, 41.47s/it]  8%|▊         | 811/9588 [9:23:19<101:07:23, 41.48s/it]  8%|▊         | 812/9588 [9:24:00<101:02:19, 41.45s/it]  8%|▊         | 813/9588 [9:24:42<101:41:10, 41.72s/it]  8%|▊         | 814/9588 [9:25:24<101:32:55, 41.67s/it]  9%|▊         | 815/9588 [9:26:05<101:24:04, 41.61s/it]  9%|▊         | 816/9588 [9:26:47<101:13:22, 41.54s/it]  9%|▊         | 817/9588 [9:27:28<101:09:18, 41.52s/it]  9%|▊         | 818/9588 [9:28:10<101:09:31, 41.52s/it]  9%|▊         | 819/9588 [9:28:51<101:07:10, 41.51s/it]  9%|▊         | 820/9588 [9:29:33<101:06:02, 41.51s/it]                                                        {'loss': 0.3765, 'learning_rate': 1.98389510310761e-05, 'epoch': 0.09}
  9%|▊         | 820/9588 [9:29:33<101:06:02, 41.51s/it]  9%|▊         | 821/9588 [9:30:14<101:04:50, 41.51s/it]  9%|▊         | 822/9588 [9:30:56<101:02:32, 41.50s/it]  9%|▊         | 823/9588 [9:31:37<101:00:29, 41.49s/it]  9%|▊         | 824/9588 [9:32:19<100:58:12, 41.48s/it]  9%|▊         | 825/9588 [9:33:01<101:20:01, 41.63s/it]  9%|▊         | 826/9588 [9:33:42<101:16:14, 41.61s/it]  9%|▊         | 827/9588 [9:34:24<101:06:35, 41.55s/it]  9%|▊         | 828/9588 [9:35:05<101:05:51, 41.55s/it]  9%|▊         | 829/9588 [9:35:47<101:03:55, 41.54s/it]  9%|▊         | 830/9588 [9:36:28<101:07:20, 41.57s/it]                                                        {'loss': 0.3869, 'learning_rate': 1.9832856736347134e-05, 'epoch': 0.09}
  9%|▊         | 830/9588 [9:36:28<101:07:20, 41.57s/it]  9%|▊         | 831/9588 [9:37:10<101:04:19, 41.55s/it]  9%|▊         | 832/9588 [9:37:51<101:07:33, 41.58s/it]  9%|▊         | 833/9588 [9:38:33<101:01:20, 41.54s/it]  9%|▊         | 834/9588 [9:39:14<101:03:01, 41.56s/it]  9%|▊         | 835/9588 [9:39:56<101:00:08, 41.54s/it]  9%|▊         | 836/9588 [9:40:38<100:58:49, 41.54s/it]  9%|▊         | 837/9588 [9:41:19<100:57:27, 41.53s/it]  9%|▊         | 838/9588 [9:42:01<100:54:26, 41.52s/it]  9%|▉         | 839/9588 [9:42:42<100:51:45, 41.50s/it]  9%|▉         | 840/9588 [9:43:24<101:31:54, 41.78s/it]                                                        {'loss': 0.3881, 'learning_rate': 1.9826650236376136e-05, 'epoch': 0.09}
  9%|▉         | 840/9588 [9:43:24<101:31:54, 41.78s/it]  9%|▉         | 841/9588 [9:44:06<101:23:24, 41.73s/it]  9%|▉         | 842/9588 [9:44:48<101:36:10, 41.82s/it]  9%|▉         | 843/9588 [9:45:30<101:31:18, 41.79s/it]  9%|▉         | 844/9588 [9:46:13<102:19:30, 42.13s/it]  9%|▉         | 845/9588 [9:46:55<102:14:22, 42.10s/it]  9%|▉         | 846/9588 [9:47:38<102:55:15, 42.38s/it]  9%|▉         | 847/9588 [9:48:20<102:27:17, 42.20s/it]  9%|▉         | 848/9588 [9:49:04<104:05:56, 42.88s/it]  9%|▉         | 849/9588 [9:49:46<103:10:10, 42.50s/it]  9%|▉         | 850/9588 [9:50:27<102:32:33, 42.25s/it]                                                        {'loss': 0.3864, 'learning_rate': 1.9820331601987063e-05, 'epoch': 0.09}
  9%|▉         | 850/9588 [9:50:27<102:32:33, 42.25s/it]  9%|▉         | 851/9588 [9:51:09<102:07:40, 42.08s/it]  9%|▉         | 852/9588 [9:51:51<101:44:13, 41.92s/it]  9%|▉         | 853/9588 [9:52:33<101:57:13, 42.02s/it]  9%|▉         | 854/9588 [9:53:14<101:33:13, 41.86s/it]  9%|▉         | 855/9588 [9:53:56<101:20:21, 41.78s/it]  9%|▉         | 856/9588 [9:54:38<101:56:30, 42.03s/it]  9%|▉         | 857/9588 [9:55:20<101:39:05, 41.91s/it]  9%|▉         | 858/9588 [9:56:02<101:36:14, 41.90s/it]  9%|▉         | 859/9588 [9:56:44<101:21:13, 41.80s/it]  9%|▉         | 860/9588 [9:57:25<101:11:03, 41.74s/it]                                                        {'loss': 0.3825, 'learning_rate': 1.9813900905283464e-05, 'epoch': 0.09}
  9%|▉         | 860/9588 [9:57:25<101:11:03, 41.74s/it]  9%|▉         | 861/9588 [9:58:07<101:02:55, 41.68s/it]  9%|▉         | 862/9588 [9:58:49<101:24:11, 41.83s/it]  9%|▉         | 863/9588 [9:59:30<101:13:21, 41.77s/it]  9%|▉         | 864/9588 [10:00:13<101:26:47, 41.86s/it]  9%|▉         | 865/9588 [10:00:54<101:17:09, 41.80s/it]  9%|▉         | 866/9588 [10:01:36<101:12:21, 41.77s/it]  9%|▉         | 867/9588 [10:02:17<101:02:34, 41.71s/it]  9%|▉         | 868/9588 [10:02:59<101:02:15, 41.71s/it]  9%|▉         | 869/9588 [10:03:41<100:56:05, 41.68s/it]  9%|▉         | 870/9588 [10:04:23<101:21:10, 41.85s/it]                                                         {'loss': 0.3806, 'learning_rate': 1.980735821964767e-05, 'epoch': 0.09}
  9%|▉         | 870/9588 [10:04:23<101:21:10, 41.85s/it]  9%|▉         | 871/9588 [10:05:05<101:11:46, 41.79s/it]  9%|▉         | 872/9588 [10:05:47<101:25:11, 41.89s/it]  9%|▉         | 873/9588 [10:06:28<101:13:02, 41.81s/it]  9%|▉         | 874/9588 [10:07:10<101:05:28, 41.76s/it]  9%|▉         | 875/9588 [10:07:52<100:59:00, 41.72s/it]  9%|▉         | 876/9588 [10:08:33<100:53:29, 41.69s/it]  9%|▉         | 877/9588 [10:09:15<100:52:46, 41.69s/it]  9%|▉         | 878/9588 [10:09:57<101:15:59, 41.86s/it]  9%|▉         | 879/9588 [10:10:39<101:08:22, 41.81s/it]  9%|▉         | 880/9588 [10:11:21<101:04:44, 41.79s/it]                                                         {'loss': 0.3716, 'learning_rate': 1.980070361973993e-05, 'epoch': 0.09}
  9%|▉         | 880/9588 [10:11:21<101:04:44, 41.79s/it]  9%|▉         | 881/9588 [10:12:03<101:20:12, 41.90s/it]  9%|▉         | 882/9588 [10:12:46<101:52:55, 42.13s/it]  9%|▉         | 883/9588 [10:13:28<102:14:43, 42.28s/it]  9%|▉         | 884/9588 [10:14:11<102:23:02, 42.35s/it]  9%|▉         | 885/9588 [10:14:53<102:35:36, 42.44s/it]  9%|▉         | 886/9588 [10:15:36<102:47:47, 42.53s/it]  9%|▉         | 887/9588 [10:16:19<102:59:46, 42.61s/it]  9%|▉         | 888/9588 [10:17:02<103:03:28, 42.64s/it]  9%|▉         | 889/9588 [10:17:44<103:12:20, 42.71s/it]  9%|▉         | 890/9588 [10:18:27<103:00:14, 42.63s/it]                                                         {'loss': 0.3898, 'learning_rate': 1.979393718149759e-05, 'epoch': 0.09}
  9%|▉         | 890/9588 [10:18:27<103:00:14, 42.63s/it]  9%|▉         | 891/9588 [10:19:09<102:49:00, 42.56s/it]  9%|▉         | 892/9588 [10:19:52<102:49:27, 42.57s/it]  9%|▉         | 893/9588 [10:20:35<103:07:13, 42.70s/it]  9%|▉         | 894/9588 [10:21:18<103:19:33, 42.79s/it]  9%|▉         | 895/9588 [10:22:00<103:02:30, 42.67s/it]  9%|▉         | 896/9588 [10:22:43<103:14:35, 42.76s/it]  9%|▉         | 897/9588 [10:23:26<103:24:16, 42.83s/it]  9%|▉         | 898/9588 [10:24:09<103:17:56, 42.79s/it]  9%|▉         | 899/9588 [10:24:52<103:23:46, 42.84s/it]  9%|▉         | 900/9588 [10:25:35<103:23:03, 42.84s/it]                                                         {'loss': 0.373, 'learning_rate': 1.978705898213421e-05, 'epoch': 0.09}
  9%|▉         | 900/9588 [10:25:35<103:23:03, 42.84s/it]  9%|▉         | 901/9588 [10:26:18<103:22:27, 42.84s/it]  9%|▉         | 902/9588 [10:27:01<103:35:01, 42.93s/it]  9%|▉         | 903/9588 [10:27:44<103:31:38, 42.91s/it]  9%|▉         | 904/9588 [10:28:25<102:33:50, 42.52s/it]  9%|▉         | 905/9588 [10:29:07<102:04:37, 42.32s/it]  9%|▉         | 906/9588 [10:29:50<102:33:12, 42.52s/it]  9%|▉         | 907/9588 [10:30:32<102:25:04, 42.47s/it]  9%|▉         | 908/9588 [10:31:15<102:13:47, 42.40s/it]  9%|▉         | 909/9588 [10:31:57<102:25:17, 42.48s/it]  9%|▉         | 910/9588 [10:32:40<102:22:55, 42.47s/it]                                                         {'loss': 0.3904, 'learning_rate': 1.9780069100138674e-05, 'epoch': 0.09}
  9%|▉         | 910/9588 [10:32:40<102:22:55, 42.47s/it] 10%|▉         | 911/9588 [10:33:23<102:39:37, 42.59s/it] 10%|▉         | 912/9588 [10:34:05<102:43:36, 42.63s/it] 10%|▉         | 913/9588 [10:34:48<102:46:58, 42.65s/it] 10%|▉         | 914/9588 [10:35:30<102:23:44, 42.50s/it] 10%|▉         | 915/9588 [10:36:12<102:09:05, 42.40s/it] 10%|▉         | 916/9588 [10:36:55<102:33:09, 42.57s/it] 10%|▉         | 917/9588 [10:37:38<102:49:08, 42.69s/it] 10%|▉         | 918/9588 [10:38:21<102:57:25, 42.75s/it] 10%|▉         | 919/9588 [10:39:04<103:15:21, 42.88s/it] 10%|▉         | 920/9588 [10:39:47<103:06:19, 42.82s/it]                                                         {'loss': 0.401, 'learning_rate': 1.977296761527432e-05, 'epoch': 0.1}
 10%|▉         | 920/9588 [10:39:47<103:06:19, 42.82s/it] 10%|▉         | 921/9588 [10:40:29<102:47:44, 42.70s/it] 10%|▉         | 922/9588 [10:41:12<102:58:35, 42.78s/it] 10%|▉         | 923/9588 [10:41:55<103:02:21, 42.81s/it] 10%|▉         | 924/9588 [10:42:38<103:07:03, 42.85s/it] 10%|▉         | 925/9588 [10:43:21<103:10:13, 42.87s/it] 10%|▉         | 926/9588 [10:44:04<102:54:54, 42.77s/it] 10%|▉         | 927/9588 [10:44:46<102:53:01, 42.76s/it] 10%|▉         | 928/9588 [10:45:29<102:46:23, 42.72s/it] 10%|▉         | 929/9588 [10:46:12<102:54:41, 42.79s/it] 10%|▉         | 930/9588 [10:46:55<102:56:49, 42.81s/it]                                                         {'loss': 0.3969, 'learning_rate': 1.9765754608578e-05, 'epoch': 0.1}
 10%|▉         | 930/9588 [10:46:55<102:56:49, 42.81s/it] 10%|▉         | 931/9588 [10:47:38<102:56:34, 42.81s/it] 10%|▉         | 932/9588 [10:48:21<103:01:20, 42.85s/it] 10%|▉         | 933/9588 [10:49:04<103:04:51, 42.88s/it] 10%|▉         | 934/9588 [10:49:46<103:03:39, 42.87s/it] 10%|▉         | 935/9588 [10:50:29<102:44:40, 42.75s/it] 10%|▉         | 936/9588 [10:51:12<102:43:34, 42.74s/it] 10%|▉         | 937/9588 [10:51:54<102:25:11, 42.62s/it] 10%|▉         | 938/9588 [10:52:37<102:29:43, 42.66s/it] 10%|▉         | 939/9588 [10:53:19<102:12:35, 42.54s/it] 10%|▉         | 940/9588 [10:54:01<101:49:22, 42.39s/it]                                                         {'loss': 0.3901, 'learning_rate': 1.975843016235918e-05, 'epoch': 0.1}
 10%|▉         | 940/9588 [10:54:01<101:49:22, 42.39s/it] 10%|▉         | 941/9588 [10:54:44<102:06:53, 42.51s/it] 10%|▉         | 942/9588 [10:55:26<101:48:10, 42.39s/it] 10%|▉         | 943/9588 [10:56:08<101:37:00, 42.32s/it] 10%|▉         | 944/9588 [10:56:50<101:40:00, 42.34s/it] 10%|▉         | 945/9588 [10:57:34<102:12:15, 42.57s/it] 10%|▉         | 946/9588 [10:58:16<102:20:16, 42.63s/it] 10%|▉         | 947/9588 [10:58:58<101:53:22, 42.45s/it] 10%|▉         | 948/9588 [10:59:41<102:12:43, 42.59s/it] 10%|▉         | 949/9588 [11:00:24<102:08:03, 42.56s/it] 10%|▉         | 950/9588 [11:01:06<102:04:02, 42.54s/it]                                                         {'loss': 0.389, 'learning_rate': 1.975099436019899e-05, 'epoch': 0.1}
 10%|▉         | 950/9588 [11:01:06<102:04:02, 42.54s/it] 10%|▉         | 951/9588 [11:01:49<102:02:03, 42.53s/it] 10%|▉         | 952/9588 [11:02:31<102:02:04, 42.53s/it] 10%|▉         | 953/9588 [11:03:14<102:09:38, 42.59s/it] 10%|▉         | 954/9588 [11:03:57<102:27:22, 42.72s/it] 10%|▉         | 955/9588 [11:04:39<102:11:14, 42.61s/it] 10%|▉         | 956/9588 [11:05:21<101:38:59, 42.39s/it] 10%|▉         | 957/9588 [11:06:03<101:26:17, 42.31s/it] 10%|▉         | 958/9588 [11:06:45<101:04:43, 42.16s/it] 10%|█         | 959/9588 [11:07:27<100:44:23, 42.03s/it] 10%|█         | 960/9588 [11:08:08<100:18:27, 41.85s/it]                                                         {'loss': 0.3878, 'learning_rate': 1.9743447286949274e-05, 'epoch': 0.1}
 10%|█         | 960/9588 [11:08:08<100:18:27, 41.85s/it] 10%|█         | 961/9588 [11:08:50<100:03:15, 41.75s/it] 10%|█         | 962/9588 [11:09:31<99:54:20, 41.69s/it]  10%|█         | 963/9588 [11:10:13<99:44:49, 41.63s/it] 10%|█         | 964/9588 [11:10:54<99:40:22, 41.61s/it] 10%|█         | 965/9588 [11:11:36<99:35:52, 41.58s/it] 10%|█         | 966/9588 [11:12:18<99:50:55, 41.69s/it] 10%|█         | 967/9588 [11:13:00<100:18:20, 41.89s/it] 10%|█         | 968/9588 [11:13:42<99:59:09, 41.76s/it]  10%|█         | 969/9588 [11:14:23<99:45:50, 41.67s/it] 10%|█         | 970/9588 [11:15:05<99:41:46, 41.65s/it]                                                        {'loss': 0.4044, 'learning_rate': 1.9735789028731603e-05, 'epoch': 0.1}
 10%|█         | 970/9588 [11:15:05<99:41:46, 41.65s/it] 10%|█         | 971/9588 [11:15:46<99:35:30, 41.61s/it] 10%|█         | 972/9588 [11:16:28<99:33:08, 41.60s/it] 10%|█         | 973/9588 [11:17:09<99:27:59, 41.56s/it] 10%|█         | 974/9588 [11:17:51<99:24:26, 41.54s/it] 10%|█         | 975/9588 [11:18:33<99:26:56, 41.57s/it] 10%|█         | 976/9588 [11:19:15<99:50:17, 41.73s/it] 10%|█         | 977/9588 [11:19:57<99:58:58, 41.80s/it] 10%|█         | 978/9588 [11:20:38<99:43:32, 41.70s/it] 10%|█         | 979/9588 [11:21:20<99:34:44, 41.64s/it] 10%|█         | 980/9588 [11:22:01<99:29:35, 41.61s/it]                                                        {'loss': 0.395, 'learning_rate': 1.972801967293633e-05, 'epoch': 0.1}
 10%|█         | 980/9588 [11:22:01<99:29:35, 41.61s/it] 10%|█         | 981/9588 [11:22:43<99:27:46, 41.60s/it] 10%|█         | 982/9588 [11:23:24<99:23:30, 41.58s/it] 10%|█         | 983/9588 [11:24:06<99:18:40, 41.55s/it] 10%|█         | 984/9588 [11:24:47<99:20:44, 41.57s/it] 10%|█         | 985/9588 [11:25:30<99:58:55, 41.84s/it] 10%|█         | 986/9588 [11:26:11<99:50:08, 41.78s/it] 10%|█         | 987/9588 [11:26:53<99:38:08, 41.70s/it] 10%|█         | 988/9588 [11:27:34<99:30:52, 41.66s/it] 10%|█         | 989/9588 [11:28:16<99:25:58, 41.63s/it] 10%|█         | 990/9588 [11:28:58<99:24:31, 41.62s/it]                                                        {'loss': 0.3816, 'learning_rate': 1.9720139308221552e-05, 'epoch': 0.1}
 10%|█         | 990/9588 [11:28:58<99:24:31, 41.62s/it] 10%|█         | 991/9588 [11:29:39<99:21:18, 41.61s/it] 10%|█         | 992/9588 [11:30:21<99:15:59, 41.57s/it] 10%|█         | 993/9588 [11:31:03<99:32:55, 41.70s/it] 10%|█         | 994/9588 [11:31:45<99:44:56, 41.78s/it] 10%|█         | 995/9588 [11:32:26<99:29:59, 41.69s/it] 10%|█         | 996/9588 [11:33:08<99:22:57, 41.64s/it] 10%|█         | 997/9588 [11:33:49<99:12:23, 41.57s/it] 10%|█         | 998/9588 [11:34:31<99:05:49, 41.53s/it] 10%|█         | 999/9588 [11:35:12<99:01:50, 41.51s/it] 10%|█         | 1000/9588 [11:35:53<98:59:11, 41.49s/it]                                                         {'loss': 0.401, 'learning_rate': 1.971214802451213e-05, 'epoch': 0.1}
 10%|█         | 1000/9588 [11:35:53<98:59:11, 41.49s/it] 10%|█         | 1001/9588 [11:36:35<99:11:59, 41.59s/it] 10%|█         | 1002/9588 [11:37:17<99:35:30, 41.76s/it] 10%|█         | 1003/9588 [11:37:59<99:22:41, 41.67s/it] 10%|█         | 1004/9588 [11:38:40<99:17:51, 41.64s/it] 10%|█         | 1005/9588 [11:39:22<99:17:03, 41.64s/it] 10%|█         | 1006/9588 [11:40:04<99:10:49, 41.60s/it] 11%|█         | 1007/9588 [11:40:45<99:04:58, 41.57s/it] 11%|█         | 1008/9588 [11:41:27<99:04:09, 41.57s/it] 11%|█         | 1009/9588 [11:42:08<99:05:01, 41.58s/it] 11%|█         | 1010/9588 [11:42:50<99:17:11, 41.67s/it]                                                         {'loss': 0.3967, 'learning_rate': 1.9704045912998646e-05, 'epoch': 0.11}
 11%|█         | 1010/9588 [11:42:50<99:17:11, 41.67s/it] 11%|█         | 1011/9588 [11:43:32<99:10:51, 41.63s/it] 11%|█         | 1012/9588 [11:44:13<99:03:45, 41.58s/it] 11%|█         | 1013/9588 [11:44:55<98:58:49, 41.55s/it] 11%|█         | 1014/9588 [11:45:36<99:02:14, 41.58s/it] 11%|█         | 1015/9588 [11:46:18<99:23:24, 41.74s/it] 11%|█         | 1016/9588 [11:47:00<99:37:45, 41.84s/it] 11%|█         | 1017/9588 [11:47:42<99:38:51, 41.85s/it] 11%|█         | 1018/9588 [11:48:25<99:54:07, 41.97s/it] 11%|█         | 1019/9588 [11:49:06<99:32:46, 41.82s/it] 11%|█         | 1020/9588 [11:49:48<99:22:24, 41.75s/it]                                                         {'loss': 0.3837, 'learning_rate': 1.9695833066136365e-05, 'epoch': 0.11}
 11%|█         | 1020/9588 [11:49:48<99:22:24, 41.75s/it] 11%|█         | 1021/9588 [11:50:29<99:13:28, 41.70s/it] 11%|█         | 1022/9588 [11:51:11<99:04:36, 41.64s/it] 11%|█         | 1023/9588 [11:51:52<98:59:38, 41.61s/it] 11%|█         | 1024/9588 [11:52:34<98:51:04, 41.55s/it] 11%|█         | 1025/9588 [11:53:15<98:47:48, 41.54s/it] 11%|█         | 1026/9588 [11:53:57<99:11:18, 41.70s/it] 11%|█         | 1027/9588 [11:54:39<99:21:05, 41.78s/it] 11%|█         | 1028/9588 [11:55:21<99:10:33, 41.71s/it] 11%|█         | 1029/9588 [11:56:02<98:58:54, 41.63s/it] 11%|█         | 1030/9588 [11:56:44<98:50:27, 41.58s/it]                                                         {'loss': 0.3956, 'learning_rate': 1.968750957764418e-05, 'epoch': 0.11}
 11%|█         | 1030/9588 [11:56:44<98:50:27, 41.58s/it] 11%|█         | 1031/9588 [11:57:25<98:50:12, 41.58s/it] 11%|█         | 1032/9588 [11:58:07<98:46:59, 41.56s/it] 11%|█         | 1033/9588 [11:58:48<98:40:59, 41.53s/it] 11%|█         | 1034/9588 [11:59:30<98:39:30, 41.52s/it] 11%|█         | 1035/9588 [12:00:12<98:55:07, 41.64s/it] 11%|█         | 1036/9588 [12:00:54<99:14:34, 41.78s/it] 11%|█         | 1037/9588 [12:01:35<99:04:46, 41.71s/it] 11%|█         | 1038/9588 [12:02:17<98:53:44, 41.64s/it] 11%|█         | 1039/9588 [12:02:58<98:46:07, 41.59s/it] 11%|█         | 1040/9588 [12:03:40<98:43:02, 41.57s/it]                                                         {'loss': 0.3785, 'learning_rate': 1.9679075542503546e-05, 'epoch': 0.11}
 11%|█         | 1040/9588 [12:03:40<98:43:02, 41.57s/it] 11%|█         | 1041/9588 [12:04:21<98:39:52, 41.56s/it] 11%|█         | 1042/9588 [12:05:03<98:39:10, 41.56s/it] 11%|█         | 1043/9588 [12:05:44<98:35:40, 41.54s/it] 11%|█         | 1044/9588 [12:06:26<98:48:16, 41.63s/it] 11%|█         | 1045/9588 [12:07:08<98:51:37, 41.66s/it] 11%|█         | 1046/9588 [12:07:50<99:09:27, 41.79s/it] 11%|█         | 1047/9588 [12:08:32<98:58:02, 41.71s/it] 11%|█         | 1048/9588 [12:09:13<98:46:02, 41.63s/it] 11%|█         | 1049/9588 [12:09:55<98:40:48, 41.60s/it] 11%|█         | 1050/9588 [12:10:36<98:42:08, 41.62s/it]                                                         {'loss': 0.3878, 'learning_rate': 1.9670531056957398e-05, 'epoch': 0.11}
 11%|█         | 1050/9588 [12:10:36<98:42:08, 41.62s/it] 11%|█         | 1051/9588 [12:11:18<98:40:39, 41.61s/it] 11%|█         | 1052/9588 [12:11:59<98:37:20, 41.59s/it] 11%|█         | 1053/9588 [12:12:41<98:46:52, 41.67s/it] 11%|█         | 1054/9588 [12:13:23<98:58:26, 41.75s/it] 11%|█         | 1055/9588 [12:14:05<98:52:03, 41.71s/it] 11%|█         | 1056/9588 [12:14:46<98:47:01, 41.68s/it] 11%|█         | 1057/9588 [12:15:28<98:38:31, 41.63s/it] 11%|█         | 1058/9588 [12:16:09<98:33:20, 41.59s/it] 11%|█         | 1059/9588 [12:16:51<98:25:48, 41.55s/it] 11%|█         | 1060/9588 [12:17:32<98:22:08, 41.53s/it]                                                         {'loss': 0.3926, 'learning_rate': 1.9661876218509045e-05, 'epoch': 0.11}
 11%|█         | 1060/9588 [12:17:32<98:22:08, 41.53s/it] 11%|█         | 1061/9588 [12:18:14<98:20:59, 41.52s/it] 11%|█         | 1062/9588 [12:18:56<98:29:19, 41.59s/it] 11%|█         | 1063/9588 [12:19:37<98:43:54, 41.69s/it] 11%|█         | 1064/9588 [12:20:19<98:52:25, 41.76s/it] 11%|█         | 1065/9588 [12:21:01<98:47:52, 41.73s/it] 11%|█         | 1066/9588 [12:21:43<98:39:29, 41.68s/it] 11%|█         | 1067/9588 [12:22:24<98:31:48, 41.63s/it] 11%|█         | 1068/9588 [12:23:06<98:45:28, 41.73s/it] 11%|█         | 1069/9588 [12:23:50<99:56:29, 42.23s/it] 11%|█         | 1070/9588 [12:24:32<99:56:24, 42.24s/it]                                                         {'loss': 0.3768, 'learning_rate': 1.9653111125921053e-05, 'epoch': 0.11}
 11%|█         | 1070/9588 [12:24:32<99:56:24, 42.24s/it] 11%|█         | 1071/9588 [12:25:13<99:27:10, 42.04s/it] 11%|█         | 1072/9588 [12:25:55<99:15:03, 41.96s/it] 11%|█         | 1073/9588 [12:26:37<99:00:00, 41.86s/it] 11%|█         | 1074/9588 [12:27:18<98:40:08, 41.72s/it] 11%|█         | 1075/9588 [12:28:00<98:52:03, 41.81s/it] 11%|█         | 1076/9588 [12:28:42<98:33:12, 41.68s/it] 11%|█         | 1077/9588 [12:29:23<98:19:43, 41.59s/it] 11%|█         | 1078/9588 [12:30:05<98:30:31, 41.67s/it] 11%|█▏        | 1079/9588 [12:30:46<98:18:13, 41.59s/it] 11%|█▏        | 1080/9588 [12:31:28<98:21:51, 41.62s/it]                                                         {'loss': 0.3928, 'learning_rate': 1.9644235879214144e-05, 'epoch': 0.11}
 11%|█▏        | 1080/9588 [12:31:28<98:21:51, 41.62s/it] 11%|█▏        | 1081/9588 [12:32:10<98:24:53, 41.65s/it] 11%|█▏        | 1082/9588 [12:32:51<98:08:15, 41.53s/it] 11%|█▏        | 1083/9588 [12:33:33<98:33:31, 41.72s/it] 11%|█▏        | 1084/9588 [12:34:15<98:27:00, 41.68s/it] 11%|█▏        | 1085/9588 [12:34:56<98:12:36, 41.58s/it] 11%|█▏        | 1086/9588 [12:35:38<98:34:34, 41.74s/it] 11%|█▏        | 1087/9588 [12:36:19<98:21:00, 41.65s/it] 11%|█▏        | 1088/9588 [12:37:01<98:09:08, 41.57s/it] 11%|█▏        | 1089/9588 [12:37:43<98:18:29, 41.64s/it] 11%|█▏        | 1090/9588 [12:38:24<98:03:31, 41.54s/it]                                                         {'loss': 0.3761, 'learning_rate': 1.9635250579666027e-05, 'epoch': 0.11}
 11%|█▏        | 1090/9588 [12:38:24<98:03:31, 41.54s/it] 11%|█▏        | 1091/9588 [12:39:05<97:54:07, 41.48s/it] 11%|█▏        | 1092/9588 [12:39:47<98:11:10, 41.60s/it] 11%|█▏        | 1093/9588 [12:40:28<97:55:33, 41.50s/it] 11%|█▏        | 1094/9588 [12:41:10<97:49:46, 41.46s/it] 11%|█▏        | 1095/9588 [12:41:52<98:03:06, 41.56s/it] 11%|█▏        | 1096/9588 [12:42:33<98:13:15, 41.64s/it] 11%|█▏        | 1097/9588 [12:43:15<98:28:02, 41.75s/it] 11%|█▏        | 1098/9588 [12:43:57<98:11:38, 41.64s/it] 11%|█▏        | 1099/9588 [12:44:39<98:17:35, 41.68s/it] 11%|█▏        | 1100/9588 [12:45:20<97:57:09, 41.54s/it]                                                         {'loss': 0.3829, 'learning_rate': 1.9626155329810244e-05, 'epoch': 0.11}
 11%|█▏        | 1100/9588 [12:45:20<97:57:09, 41.54s/it] 11%|█▏        | 1101/9588 [12:46:02<98:12:41, 41.66s/it] 11%|█▏        | 1102/9588 [12:46:43<97:57:33, 41.56s/it] 12%|█▏        | 1103/9588 [12:47:25<97:57:26, 41.56s/it] 12%|█▏        | 1104/9588 [12:48:06<97:43:26, 41.47s/it] 12%|█▏        | 1105/9588 [12:48:48<97:53:51, 41.55s/it] 12%|█▏        | 1106/9588 [12:49:29<98:00:44, 41.60s/it] 12%|█▏        | 1107/9588 [12:50:11<98:00:43, 41.60s/it] 12%|█▏        | 1108/9588 [12:50:53<98:19:04, 41.74s/it] 12%|█▏        | 1109/9588 [12:51:34<98:00:06, 41.61s/it] 12%|█▏        | 1110/9588 [12:52:16<98:00:51, 41.62s/it]                                                         {'loss': 0.3935, 'learning_rate': 1.961695023343502e-05, 'epoch': 0.12}
 12%|█▏        | 1110/9588 [12:52:16<98:00:51, 41.62s/it] 12%|█▏        | 1111/9588 [12:52:57<97:49:32, 41.54s/it] 12%|█▏        | 1112/9588 [12:53:39<98:05:52, 41.67s/it] 12%|█▏        | 1113/9588 [12:54:20<97:45:30, 41.53s/it] 12%|█▏        | 1114/9588 [12:55:02<97:50:17, 41.56s/it] 12%|█▏        | 1115/9588 [12:55:43<97:32:46, 41.45s/it] 12%|█▏        | 1116/9588 [12:56:25<97:49:30, 41.57s/it] 12%|█▏        | 1117/9588 [12:57:06<97:33:49, 41.46s/it] 12%|█▏        | 1118/9588 [12:57:48<97:45:12, 41.55s/it] 12%|█▏        | 1119/9588 [12:58:29<97:27:25, 41.43s/it] 12%|█▏        | 1120/9588 [12:59:11<97:38:24, 41.51s/it]                                                         {'loss': 0.3804, 'learning_rate': 1.9607635395582067e-05, 'epoch': 0.12}
 12%|█▏        | 1120/9588 [12:59:11<97:38:24, 41.51s/it] 12%|█▏        | 1121/9588 [12:59:52<97:26:13, 41.43s/it] 12%|█▏        | 1122/9588 [13:00:34<97:30:43, 41.47s/it] 12%|█▏        | 1123/9588 [13:01:15<97:19:04, 41.39s/it] 12%|█▏        | 1124/9588 [13:01:57<97:26:19, 41.44s/it] 12%|█▏        | 1125/9588 [13:02:38<97:16:25, 41.38s/it] 12%|█▏        | 1126/9588 [13:03:20<97:35:10, 41.52s/it] 12%|█▏        | 1127/9588 [13:04:01<97:23:00, 41.43s/it] 12%|█▏        | 1128/9588 [13:04:42<97:27:37, 41.47s/it] 12%|█▏        | 1129/9588 [13:05:24<97:18:55, 41.42s/it] 12%|█▏        | 1130/9588 [13:06:05<97:32:05, 41.51s/it]                                                         {'loss': 0.3899, 'learning_rate': 1.959821092254538e-05, 'epoch': 0.12}
 12%|█▏        | 1130/9588 [13:06:05<97:32:05, 41.51s/it] 12%|█▏        | 1131/9588 [13:06:47<97:20:34, 41.44s/it] 12%|█▏        | 1132/9588 [13:07:28<97:32:28, 41.53s/it] 12%|█▏        | 1133/9588 [13:08:10<97:21:05, 41.45s/it] 12%|█▏        | 1134/9588 [13:08:52<97:39:57, 41.59s/it] 12%|█▏        | 1135/9588 [13:09:33<97:23:02, 41.47s/it] 12%|█▏        | 1136/9588 [13:10:14<97:29:14, 41.52s/it] 12%|█▏        | 1137/9588 [13:10:56<97:15:59, 41.43s/it] 12%|█▏        | 1138/9588 [13:11:37<97:23:59, 41.50s/it] 12%|█▏        | 1139/9588 [13:12:19<97:17:25, 41.45s/it] 12%|█▏        | 1140/9588 [13:13:00<97:29:18, 41.54s/it]                                                         {'loss': 0.3706, 'learning_rate': 1.9588676921870028e-05, 'epoch': 0.12}
 12%|█▏        | 1140/9588 [13:13:00<97:29:18, 41.54s/it] 12%|█▏        | 1141/9588 [13:13:42<97:18:25, 41.47s/it] 12%|█▏        | 1142/9588 [13:14:24<97:30:32, 41.56s/it] 12%|█▏        | 1143/9588 [13:15:05<97:17:32, 41.47s/it] 12%|█▏        | 1144/9588 [13:15:46<97:22:20, 41.51s/it] 12%|█▏        | 1145/9588 [13:16:28<97:14:03, 41.46s/it] 12%|█▏        | 1146/9588 [13:17:10<97:32:32, 41.60s/it] 12%|█▏        | 1147/9588 [13:17:51<97:13:16, 41.46s/it] 12%|█▏        | 1148/9588 [13:18:33<97:24:11, 41.55s/it] 12%|█▏        | 1149/9588 [13:19:14<97:14:31, 41.48s/it] 12%|█▏        | 1150/9588 [13:19:56<97:24:31, 41.56s/it]                                                         {'loss': 0.3856, 'learning_rate': 1.957903350235093e-05, 'epoch': 0.12}
 12%|█▏        | 1150/9588 [13:19:56<97:24:31, 41.56s/it] 12%|█▏        | 1151/9588 [13:20:37<97:13:01, 41.48s/it] 12%|█▏        | 1152/9588 [13:21:19<97:34:46, 41.64s/it] 12%|█▏        | 1153/9588 [13:22:00<97:28:15, 41.60s/it] 12%|█▏        | 1154/9588 [13:22:42<97:47:29, 41.74s/it] 12%|█▏        | 1155/9588 [13:23:24<97:28:35, 41.61s/it] 12%|█▏        | 1156/9588 [13:24:06<97:43:32, 41.72s/it] 12%|█▏        | 1157/9588 [13:24:47<97:21:07, 41.57s/it] 12%|█▏        | 1158/9588 [13:25:29<97:22:05, 41.58s/it] 12%|█▏        | 1159/9588 [13:26:10<97:10:56, 41.51s/it] 12%|█▏        | 1160/9588 [13:26:52<97:23:58, 41.60s/it]                                                         {'loss': 0.3749, 'learning_rate': 1.956928077403161e-05, 'epoch': 0.12}
 12%|█▏        | 1160/9588 [13:26:52<97:23:58, 41.60s/it] 12%|█▏        | 1161/9588 [13:27:33<97:09:04, 41.50s/it] 12%|█▏        | 1162/9588 [13:28:15<97:16:15, 41.56s/it] 12%|█▏        | 1163/9588 [13:28:56<97:06:23, 41.49s/it] 12%|█▏        | 1164/9588 [13:29:38<97:20:26, 41.60s/it] 12%|█▏        | 1165/9588 [13:30:19<97:06:40, 41.51s/it] 12%|█▏        | 1166/9588 [13:31:01<97:12:27, 41.55s/it] 12%|█▏        | 1167/9588 [13:31:42<97:00:23, 41.47s/it] 12%|█▏        | 1168/9588 [13:32:24<97:10:14, 41.55s/it] 12%|█▏        | 1169/9588 [13:33:05<96:58:45, 41.47s/it] 12%|█▏        | 1170/9588 [13:33:47<97:10:21, 41.56s/it]                                                         {'loss': 0.3795, 'learning_rate': 1.9559418848202947e-05, 'epoch': 0.12}
 12%|█▏        | 1170/9588 [13:33:47<97:10:21, 41.56s/it] 12%|█▏        | 1171/9588 [13:34:28<96:58:09, 41.47s/it] 12%|█▏        | 1172/9588 [13:35:10<97:05:13, 41.53s/it] 12%|█▏        | 1173/9588 [13:35:51<96:57:20, 41.48s/it] 12%|█▏        | 1174/9588 [13:36:33<97:15:30, 41.61s/it] 12%|█▏        | 1175/9588 [13:37:14<97:00:31, 41.51s/it] 12%|█▏        | 1176/9588 [13:37:56<97:13:41, 41.61s/it] 12%|█▏        | 1177/9588 [13:38:38<97:00:26, 41.52s/it] 12%|█▏        | 1178/9588 [13:39:19<97:10:58, 41.60s/it] 12%|█▏        | 1179/9588 [13:40:01<96:55:41, 41.50s/it] 12%|█▏        | 1180/9588 [13:40:42<97:00:12, 41.53s/it]                                                         {'loss': 0.3856, 'learning_rate': 1.95494478374019e-05, 'epoch': 0.12}
 12%|█▏        | 1180/9588 [13:40:42<97:00:12, 41.53s/it] 12%|█▏        | 1181/9588 [13:41:24<96:59:11, 41.53s/it] 12%|█▏        | 1182/9588 [13:42:06<97:17:23, 41.67s/it] 12%|█▏        | 1183/9588 [13:42:47<96:57:31, 41.53s/it] 12%|█▏        | 1184/9588 [13:43:29<97:09:30, 41.62s/it] 12%|█▏        | 1185/9588 [13:44:10<96:55:45, 41.53s/it] 12%|█▏        | 1186/9588 [13:44:52<97:05:37, 41.60s/it] 12%|█▏        | 1187/9588 [13:45:33<96:54:33, 41.53s/it] 12%|█▏        | 1188/9588 [13:46:15<97:06:12, 41.62s/it] 12%|█▏        | 1189/9588 [13:46:56<96:53:29, 41.53s/it] 12%|█▏        | 1190/9588 [13:47:38<97:11:00, 41.66s/it]                                                         {'loss': 0.3801, 'learning_rate': 1.953936785541022e-05, 'epoch': 0.12}
 12%|█▏        | 1190/9588 [13:47:38<97:11:00, 41.66s/it] 12%|█▏        | 1191/9588 [13:48:20<97:15:41, 41.70s/it] 12%|█▏        | 1192/9588 [13:49:02<97:15:54, 41.70s/it] 12%|█▏        | 1193/9588 [13:49:43<96:58:36, 41.59s/it] 12%|█▏        | 1194/9588 [13:50:25<97:03:28, 41.63s/it] 12%|█▏        | 1195/9588 [13:51:06<96:47:06, 41.51s/it] 12%|█▏        | 1196/9588 [13:51:48<96:56:30, 41.59s/it] 12%|█▏        | 1197/9588 [13:52:29<96:43:22, 41.50s/it] 12%|█▏        | 1198/9588 [13:53:11<96:54:35, 41.58s/it] 13%|█▎        | 1199/9588 [13:53:52<96:42:37, 41.50s/it] 13%|█▎        | 1200/9588 [13:54:34<97:05:36, 41.67s/it]                                                         {'loss': 0.379, 'learning_rate': 1.952917901725316e-05, 'epoch': 0.13}
 13%|█▎        | 1200/9588 [13:54:34<97:05:36, 41.67s/it] 13%|█▎        | 1201/9588 [13:55:16<96:48:07, 41.55s/it] 13%|█▎        | 1202/9588 [13:55:57<97:00:29, 41.64s/it] 13%|█▎        | 1203/9588 [13:56:39<96:39:43, 41.50s/it] 13%|█▎        | 1204/9588 [13:57:21<97:07:26, 41.70s/it] 13%|█▎        | 1205/9588 [13:58:02<96:47:19, 41.56s/it] 13%|█▎        | 1206/9588 [13:58:44<96:55:07, 41.63s/it] 13%|█▎        | 1207/9588 [13:59:25<96:42:06, 41.54s/it] 13%|█▎        | 1208/9588 [14:00:07<96:56:01, 41.64s/it] 13%|█▎        | 1209/9588 [14:00:48<96:41:07, 41.54s/it] 13%|█▎        | 1210/9588 [14:01:30<96:45:29, 41.58s/it]                                                         {'loss': 0.4053, 'learning_rate': 1.951888143919815e-05, 'epoch': 0.13}
 13%|█▎        | 1210/9588 [14:01:30<96:45:29, 41.58s/it] 13%|█▎        | 1211/9588 [14:02:11<96:36:46, 41.52s/it] 13%|█▎        | 1212/9588 [14:02:53<96:50:03, 41.62s/it] 13%|█▎        | 1213/9588 [14:03:34<96:34:51, 41.52s/it] 13%|█▎        | 1214/9588 [14:04:16<96:37:49, 41.54s/it] 13%|█▎        | 1215/9588 [14:04:57<96:31:11, 41.50s/it] 13%|█▎        | 1216/9588 [14:05:39<96:43:22, 41.59s/it] 13%|█▎        | 1217/9588 [14:06:21<96:33:00, 41.52s/it] 13%|█▎        | 1218/9588 [14:07:02<96:38:30, 41.57s/it] 13%|█▎        | 1219/9588 [14:07:44<96:24:34, 41.47s/it] 13%|█▎        | 1220/9588 [14:08:26<96:51:28, 41.67s/it]                                                         {'loss': 0.3644, 'learning_rate': 1.950847523875349e-05, 'epoch': 0.13}
 13%|█▎        | 1220/9588 [14:08:26<96:51:28, 41.67s/it] 13%|█▎        | 1221/9588 [14:09:07<96:40:24, 41.59s/it] 13%|█▎        | 1222/9588 [14:09:49<96:58:45, 41.73s/it] 13%|█▎        | 1223/9588 [14:10:30<96:40:13, 41.60s/it] 13%|█▎        | 1224/9588 [14:11:12<96:45:30, 41.65s/it] 13%|█▎        | 1225/9588 [14:11:53<96:27:41, 41.52s/it] 13%|█▎        | 1226/9588 [14:12:35<96:45:23, 41.66s/it] 13%|█▎        | 1227/9588 [14:13:17<96:24:22, 41.51s/it] 13%|█▎        | 1228/9588 [14:13:59<96:40:53, 41.63s/it] 13%|█▎        | 1229/9588 [14:14:40<96:25:03, 41.52s/it] 13%|█▎        | 1230/9588 [14:15:22<96:52:13, 41.72s/it]                                                         {'loss': 0.3829, 'learning_rate': 1.9497960534667e-05, 'epoch': 0.13}
 13%|█▎        | 1230/9588 [14:15:22<96:52:13, 41.72s/it] 13%|█▎        | 1231/9588 [14:16:03<96:30:39, 41.57s/it] 13%|█▎        | 1232/9588 [14:16:45<96:46:26, 41.69s/it] 13%|█▎        | 1233/9588 [14:17:27<96:30:43, 41.59s/it] 13%|█▎        | 1234/9588 [14:18:08<96:29:52, 41.58s/it] 13%|█▎        | 1235/9588 [14:18:49<96:17:01, 41.50s/it] 13%|█▎        | 1236/9588 [14:19:31<96:17:22, 41.50s/it] 13%|█▎        | 1237/9588 [14:20:13<96:33:28, 41.62s/it] 13%|█▎        | 1238/9588 [14:20:54<96:33:02, 41.63s/it] 13%|█▎        | 1239/9588 [14:21:36<96:49:02, 41.75s/it] 13%|█▎        | 1240/9588 [14:22:18<96:44:05, 41.72s/it]                                                         {'loss': 0.368, 'learning_rate': 1.948733744692465e-05, 'epoch': 0.13}
 13%|█▎        | 1240/9588 [14:22:18<96:44:05, 41.72s/it] 13%|█▎        | 1241/9588 [14:23:00<96:53:46, 41.79s/it] 13%|█▎        | 1242/9588 [14:23:41<96:31:44, 41.64s/it] 13%|█▎        | 1243/9588 [14:24:23<96:44:24, 41.73s/it] 13%|█▎        | 1244/9588 [14:25:05<96:27:58, 41.62s/it] 13%|█▎        | 1245/9588 [14:25:46<96:29:11, 41.63s/it] 13%|█▎        | 1246/9588 [14:26:28<96:12:57, 41.52s/it] 13%|█▎        | 1247/9588 [14:27:10<96:28:43, 41.64s/it] 13%|█▎        | 1248/9588 [14:27:51<96:17:35, 41.57s/it] 13%|█▎        | 1249/9588 [14:28:33<96:30:35, 41.66s/it] 13%|█▎        | 1250/9588 [14:29:14<96:13:30, 41.55s/it]                                                         {'loss': 0.37, 'learning_rate': 1.947660609674921e-05, 'epoch': 0.13}
 13%|█▎        | 1250/9588 [14:29:14<96:13:30, 41.55s/it] 13%|█▎        | 1251/9588 [14:29:56<96:29:16, 41.66s/it] 13%|█▎        | 1252/9588 [14:30:37<96:13:21, 41.55s/it] 13%|█▎        | 1253/9588 [14:31:19<96:23:43, 41.63s/it] 13%|█▎        | 1254/9588 [14:32:00<96:06:38, 41.52s/it] 13%|█▎        | 1255/9588 [14:32:42<96:30:49, 41.70s/it] 13%|█▎        | 1256/9588 [14:33:24<96:11:50, 41.56s/it] 13%|█▎        | 1257/9588 [14:34:05<96:15:20, 41.59s/it] 13%|█▎        | 1258/9588 [14:34:47<96:02:01, 41.50s/it] 13%|█▎        | 1259/9588 [14:35:29<96:17:19, 41.62s/it] 13%|█▎        | 1260/9588 [14:36:10<96:19:13, 41.64s/it]                                                         {'loss': 0.3713, 'learning_rate': 1.946576660659886e-05, 'epoch': 0.13}
 13%|█▎        | 1260/9588 [14:36:10<96:19:13, 41.64s/it] 13%|█▎        | 1261/9588 [14:36:52<96:18:48, 41.64s/it] 13%|█▎        | 1262/9588 [14:37:34<96:25:10, 41.69s/it] 13%|█▎        | 1263/9588 [14:38:16<96:28:37, 41.72s/it] 13%|█▎        | 1264/9588 [14:38:58<96:39:38, 41.80s/it] 13%|█▎        | 1265/9588 [14:39:39<96:35:53, 41.78s/it] 13%|█▎        | 1266/9588 [14:40:21<96:29:41, 41.74s/it] 13%|█▎        | 1267/9588 [14:41:03<96:39:44, 41.82s/it] 13%|█▎        | 1268/9588 [14:41:45<96:35:31, 41.79s/it] 13%|█▎        | 1269/9588 [14:42:27<96:56:47, 41.95s/it] 13%|█▎        | 1270/9588 [14:43:09<96:49:20, 41.90s/it]                                                         {'loss': 0.385, 'learning_rate': 1.9454819100165797e-05, 'epoch': 0.13}
 13%|█▎        | 1270/9588 [14:43:09<96:49:20, 41.90s/it] 13%|█▎        | 1271/9588 [14:43:51<96:42:58, 41.86s/it] 13%|█▎        | 1272/9588 [14:44:32<96:31:39, 41.79s/it] 13%|█▎        | 1273/9588 [14:45:14<96:31:48, 41.79s/it] 13%|█▎        | 1274/9588 [14:45:55<96:16:02, 41.68s/it] 13%|█▎        | 1275/9588 [14:46:38<96:36:26, 41.84s/it] 13%|█▎        | 1276/9588 [14:47:19<96:14:55, 41.69s/it] 13%|█▎        | 1277/9588 [14:48:01<96:17:41, 41.71s/it] 13%|█▎        | 1278/9588 [14:48:42<96:07:41, 41.64s/it] 13%|█▎        | 1279/9588 [14:49:24<96:23:40, 41.76s/it] 13%|█▎        | 1280/9588 [14:50:06<96:04:51, 41.63s/it]                                                         {'loss': 0.3713, 'learning_rate': 1.944376370237481e-05, 'epoch': 0.13}
 13%|█▎        | 1280/9588 [14:50:06<96:04:51, 41.63s/it] 13%|█▎        | 1281/9588 [14:50:47<96:07:57, 41.66s/it] 13%|█▎        | 1282/9588 [14:51:28<95:50:20, 41.54s/it] 13%|█▎        | 1283/9588 [14:52:10<96:03:33, 41.64s/it] 13%|█▎        | 1284/9588 [14:52:52<95:52:12, 41.56s/it] 13%|█▎        | 1285/9588 [14:53:34<96:01:45, 41.64s/it] 13%|█▎        | 1286/9588 [14:54:15<95:47:16, 41.54s/it] 13%|█▎        | 1287/9588 [14:54:57<96:06:26, 41.68s/it] 13%|█▎        | 1288/9588 [14:55:38<95:53:00, 41.59s/it] 13%|█▎        | 1289/9588 [14:56:20<95:57:37, 41.63s/it] 13%|█▎        | 1290/9588 [14:57:01<95:43:24, 41.53s/it]                                                         {'loss': 0.3619, 'learning_rate': 1.9432600539381878e-05, 'epoch': 0.13}
 13%|█▎        | 1290/9588 [14:57:01<95:43:24, 41.53s/it] 13%|█▎        | 1291/9588 [14:57:43<95:53:26, 41.61s/it] 13%|█▎        | 1292/9588 [14:58:24<95:39:37, 41.51s/it] 13%|█▎        | 1293/9588 [14:59:06<95:51:10, 41.60s/it] 13%|█▎        | 1294/9588 [14:59:47<95:36:06, 41.50s/it] 14%|█▎        | 1295/9588 [15:00:29<95:42:34, 41.55s/it] 14%|█▎        | 1296/9588 [15:01:10<95:32:18, 41.48s/it] 14%|█▎        | 1297/9588 [15:01:53<95:58:39, 41.67s/it] 14%|█▎        | 1298/9588 [15:02:34<95:42:16, 41.56s/it] 14%|█▎        | 1299/9588 [15:03:15<95:45:24, 41.59s/it] 14%|█▎        | 1300/9588 [15:03:57<95:30:05, 41.48s/it]                                                         {'loss': 0.3776, 'learning_rate': 1.9421329738572702e-05, 'epoch': 0.14}
 14%|█▎        | 1300/9588 [15:03:57<95:30:05, 41.48s/it] 14%|█▎        | 1301/9588 [15:04:39<96:00:18, 41.71s/it] 14%|█▎        | 1302/9588 [15:05:20<95:41:47, 41.58s/it] 14%|█▎        | 1303/9588 [15:06:02<95:52:19, 41.66s/it] 14%|█▎        | 1304/9588 [15:06:43<95:33:47, 41.53s/it] 14%|█▎        | 1305/9588 [15:07:25<95:44:08, 41.61s/it] 14%|█▎        | 1306/9588 [15:08:06<95:27:16, 41.49s/it] 14%|█▎        | 1307/9588 [15:08:48<95:33:11, 41.54s/it] 14%|█▎        | 1308/9588 [15:09:29<95:22:44, 41.47s/it] 14%|█▎        | 1309/9588 [15:10:12<95:56:11, 41.72s/it] 14%|█▎        | 1310/9588 [15:10:53<95:35:02, 41.57s/it]                                                         {'loss': 0.3742, 'learning_rate': 1.940995142856127e-05, 'epoch': 0.14}
 14%|█▎        | 1310/9588 [15:10:53<95:35:02, 41.57s/it] 14%|█▎        | 1311/9588 [15:11:35<95:54:46, 41.72s/it] 14%|█▎        | 1312/9588 [15:12:16<95:37:45, 41.60s/it] 14%|█▎        | 1313/9588 [15:12:58<95:49:52, 41.69s/it] 14%|█▎        | 1314/9588 [15:13:39<95:34:25, 41.58s/it] 14%|█▎        | 1315/9588 [15:14:21<95:38:23, 41.62s/it] 14%|█▎        | 1316/9588 [15:15:02<95:20:30, 41.49s/it] 14%|█▎        | 1317/9588 [15:15:44<95:32:58, 41.59s/it] 14%|█▎        | 1318/9588 [15:16:25<95:22:38, 41.52s/it] 14%|█▍        | 1319/9588 [15:17:07<95:30:05, 41.58s/it] 14%|█▍        | 1320/9588 [15:17:48<95:17:26, 41.49s/it]                                                         {'loss': 0.3841, 'learning_rate': 1.9398465739188392e-05, 'epoch': 0.14}
 14%|█▍        | 1320/9588 [15:17:48<95:17:26, 41.49s/it] 14%|█▍        | 1321/9588 [15:18:30<95:27:25, 41.57s/it] 14%|█▍        | 1322/9588 [15:19:11<95:13:21, 41.47s/it] 14%|█▍        | 1323/9588 [15:19:53<95:24:30, 41.56s/it] 14%|█▍        | 1324/9588 [15:20:35<95:13:36, 41.48s/it] 14%|█▍        | 1325/9588 [15:21:16<95:16:51, 41.51s/it] 14%|█▍        | 1326/9588 [15:21:57<95:07:13, 41.45s/it] 14%|█▍        | 1327/9588 [15:22:39<95:18:44, 41.54s/it] 14%|█▍        | 1328/9588 [15:23:20<95:06:23, 41.45s/it] 14%|█▍        | 1329/9588 [15:24:02<95:24:28, 41.59s/it] 14%|█▍        | 1330/9588 [15:24:44<95:11:11, 41.50s/it]                                                         {'loss': 0.3755, 'learning_rate': 1.9386872801520202e-05, 'epoch': 0.14}
 14%|█▍        | 1330/9588 [15:24:44<95:11:11, 41.50s/it] 14%|█▍        | 1331/9588 [15:25:26<95:37:22, 41.69s/it] 14%|█▍        | 1332/9588 [15:26:07<95:29:58, 41.64s/it] 14%|█▍        | 1333/9588 [15:26:49<95:29:41, 41.65s/it] 14%|█▍        | 1334/9588 [15:27:30<95:11:53, 41.52s/it] 14%|█▍        | 1335/9588 [15:28:12<95:25:16, 41.62s/it] 14%|█▍        | 1336/9588 [15:28:53<95:07:18, 41.50s/it] 14%|█▍        | 1337/9588 [15:29:35<95:23:00, 41.62s/it] 14%|█▍        | 1338/9588 [15:30:16<95:06:23, 41.50s/it] 14%|█▍        | 1339/9588 [15:30:58<95:29:23, 41.67s/it] 14%|█▍        | 1340/9588 [15:31:40<95:10:41, 41.54s/it]                                                         {'loss': 0.3834, 'learning_rate': 1.9375172747846677e-05, 'epoch': 0.14}
 14%|█▍        | 1340/9588 [15:31:40<95:10:41, 41.54s/it] 14%|█▍        | 1341/9588 [15:32:22<95:35:59, 41.73s/it] 14%|█▍        | 1342/9588 [15:33:03<95:18:37, 41.61s/it] 14%|█▍        | 1343/9588 [15:33:45<95:16:44, 41.60s/it] 14%|█▍        | 1344/9588 [15:34:26<95:17:03, 41.61s/it] 14%|█▍        | 1345/9588 [15:35:08<95:35:55, 41.75s/it] 14%|█▍        | 1346/9588 [15:35:50<95:39:05, 41.78s/it] 14%|█▍        | 1347/9588 [15:36:32<95:44:39, 41.82s/it] 14%|█▍        | 1348/9588 [15:37:14<95:53:12, 41.89s/it] 14%|█▍        | 1349/9588 [15:37:56<95:51:49, 41.89s/it] 14%|█▍        | 1350/9588 [15:38:38<95:59:56, 41.95s/it]                                                         {'loss': 0.3856, 'learning_rate': 1.9363365711680114e-05, 'epoch': 0.14}
 14%|█▍        | 1350/9588 [15:38:38<95:59:56, 41.95s/it] 14%|█▍        | 1351/9588 [15:39:20<95:46:53, 41.86s/it] 14%|█▍        | 1352/9588 [15:40:02<95:49:16, 41.88s/it] 14%|█▍        | 1353/9588 [15:40:44<95:41:36, 41.83s/it] 14%|█▍        | 1354/9588 [15:41:26<96:03:25, 42.00s/it] 14%|█▍        | 1355/9588 [15:42:07<95:36:53, 41.81s/it] 14%|█▍        | 1356/9588 [15:42:49<95:50:40, 41.91s/it] 14%|█▍        | 1357/9588 [15:43:31<95:22:43, 41.72s/it] 14%|█▍        | 1358/9588 [15:44:12<95:21:27, 41.71s/it] 14%|█▍        | 1359/9588 [15:44:54<95:01:59, 41.57s/it] 14%|█▍        | 1360/9588 [15:45:35<95:00:23, 41.57s/it]                                                         {'loss': 0.3751, 'learning_rate': 1.9351451827753632e-05, 'epoch': 0.14}
 14%|█▍        | 1360/9588 [15:45:35<95:00:23, 41.57s/it] 14%|█▍        | 1361/9588 [15:46:17<94:52:19, 41.51s/it] 14%|█▍        | 1362/9588 [15:46:58<94:58:53, 41.57s/it] 14%|█▍        | 1363/9588 [15:47:40<95:13:41, 41.68s/it] 14%|█▍        | 1364/9588 [15:48:22<95:13:06, 41.68s/it] 14%|█▍        | 1365/9588 [15:49:04<95:14:35, 41.70s/it] 14%|█▍        | 1366/9588 [15:49:45<94:59:15, 41.59s/it] 14%|█▍        | 1367/9588 [15:50:26<94:53:44, 41.56s/it] 14%|█▍        | 1368/9588 [15:51:08<94:46:48, 41.51s/it] 14%|█▍        | 1369/9588 [15:51:49<94:43:31, 41.49s/it] 14%|█▍        | 1370/9588 [15:52:31<94:43:15, 41.49s/it]                                                         {'loss': 0.3864, 'learning_rate': 1.93394312320196e-05, 'epoch': 0.14}
 14%|█▍        | 1370/9588 [15:52:31<94:43:15, 41.49s/it] 14%|█▍        | 1371/9588 [15:53:12<94:33:34, 41.43s/it] 14%|█▍        | 1372/9588 [15:53:53<94:29:48, 41.41s/it] 14%|█▍        | 1373/9588 [15:54:35<94:31:36, 41.42s/it] 14%|█▍        | 1374/9588 [15:55:16<94:24:08, 41.37s/it] 14%|█▍        | 1375/9588 [15:55:58<94:25:54, 41.39s/it] 14%|█▍        | 1376/9588 [15:56:39<94:22:48, 41.37s/it] 14%|█▍        | 1377/9588 [15:57:20<94:22:56, 41.38s/it] 14%|█▍        | 1378/9588 [15:58:02<94:26:49, 41.41s/it] 14%|█▍        | 1379/9588 [15:58:43<94:23:36, 41.40s/it] 14%|█▍        | 1380/9588 [15:59:25<94:20:29, 41.38s/it]                                                         {'loss': 0.3795, 'learning_rate': 1.9327304061648116e-05, 'epoch': 0.14}
 14%|█▍        | 1380/9588 [15:59:25<94:20:29, 41.38s/it] 14%|█▍        | 1381/9588 [16:00:06<94:17:54, 41.36s/it] 14%|█▍        | 1382/9588 [16:00:47<94:15:47, 41.35s/it] 14%|█▍        | 1383/9588 [16:01:29<94:52:40, 41.63s/it] 14%|█▍        | 1384/9588 [16:02:11<94:48:42, 41.60s/it] 14%|█▍        | 1385/9588 [16:02:52<94:35:17, 41.51s/it] 14%|█▍        | 1386/9588 [16:03:34<94:28:51, 41.47s/it] 14%|█▍        | 1387/9588 [16:04:15<94:20:06, 41.41s/it] 14%|█▍        | 1388/9588 [16:04:56<94:25:27, 41.45s/it] 14%|█▍        | 1389/9588 [16:05:38<94:23:11, 41.44s/it] 14%|█▍        | 1390/9588 [16:06:19<94:16:05, 41.40s/it]                                                         {'loss': 0.3662, 'learning_rate': 1.931507045502541e-05, 'epoch': 0.14}
 14%|█▍        | 1390/9588 [16:06:19<94:16:05, 41.40s/it] 15%|█▍        | 1391/9588 [16:07:01<94:16:10, 41.40s/it] 15%|█▍        | 1392/9588 [16:07:42<94:27:18, 41.49s/it] 15%|█▍        | 1393/9588 [16:08:24<94:18:16, 41.43s/it] 15%|█▍        | 1394/9588 [16:09:05<94:17:52, 41.43s/it] 15%|█▍        | 1395/9588 [16:09:46<94:08:43, 41.37s/it] 15%|█▍        | 1396/9588 [16:10:28<94:07:07, 41.36s/it] 15%|█▍        | 1397/9588 [16:11:09<94:14:03, 41.42s/it] 15%|█▍        | 1398/9588 [16:11:50<94:06:51, 41.37s/it] 15%|█▍        | 1399/9588 [16:12:32<94:05:44, 41.37s/it] 15%|█▍        | 1400/9588 [16:13:13<94:06:59, 41.38s/it]                                                         {'loss': 0.3764, 'learning_rate': 1.93027305517523e-05, 'epoch': 0.15}
 15%|█▍        | 1400/9588 [16:13:13<94:06:59, 41.38s/it] 15%|█▍        | 1401/9588 [16:13:55<94:11:34, 41.42s/it] 15%|█▍        | 1402/9588 [16:14:36<94:26:18, 41.53s/it] 15%|█▍        | 1403/9588 [16:15:18<94:28:22, 41.55s/it] 15%|█▍        | 1404/9588 [16:16:00<94:26:57, 41.55s/it] 15%|█▍        | 1405/9588 [16:16:41<94:31:23, 41.58s/it] 15%|█▍        | 1406/9588 [16:17:23<94:34:32, 41.61s/it] 15%|█▍        | 1407/9588 [16:18:04<94:19:15, 41.51s/it] 15%|█▍        | 1408/9588 [16:18:45<94:07:35, 41.42s/it] 15%|█▍        | 1409/9588 [16:19:27<93:58:57, 41.37s/it] 15%|█▍        | 1410/9588 [16:20:08<93:53:47, 41.33s/it]                                                         {'loss': 0.3736, 'learning_rate': 1.9290284492642582e-05, 'epoch': 0.15}
 15%|█▍        | 1410/9588 [16:20:08<93:53:47, 41.33s/it] 15%|█▍        | 1411/9588 [16:20:49<93:48:57, 41.30s/it] 15%|█▍        | 1412/9588 [16:21:30<93:45:23, 41.28s/it] 15%|█▍        | 1413/9588 [16:22:12<93:42:04, 41.26s/it] 15%|█▍        | 1414/9588 [16:22:53<93:47:27, 41.31s/it] 15%|█▍        | 1415/9588 [16:23:34<93:44:12, 41.29s/it] 15%|█▍        | 1416/9588 [16:24:16<93:43:31, 41.29s/it] 15%|█▍        | 1417/9588 [16:24:57<93:39:29, 41.26s/it] 15%|█▍        | 1418/9588 [16:25:38<93:38:38, 41.26s/it] 15%|█▍        | 1419/9588 [16:26:19<93:35:17, 41.24s/it] 15%|█▍        | 1420/9588 [16:27:00<93:35:04, 41.25s/it]                                                         {'loss': 0.3689, 'learning_rate': 1.927773241972141e-05, 'epoch': 0.15}
 15%|█▍        | 1420/9588 [16:27:00<93:35:04, 41.25s/it] 15%|█▍        | 1421/9588 [16:27:42<93:37:44, 41.27s/it] 15%|█▍        | 1422/9588 [16:28:23<93:37:44, 41.28s/it] 15%|█▍        | 1423/9588 [16:29:04<93:37:13, 41.28s/it] 15%|█▍        | 1424/9588 [16:29:46<93:33:18, 41.25s/it] 15%|█▍        | 1425/9588 [16:30:27<93:31:59, 41.25s/it] 15%|█▍        | 1426/9588 [16:31:08<93:36:40, 41.29s/it] 15%|█▍        | 1427/9588 [16:31:49<93:31:33, 41.26s/it] 15%|█▍        | 1428/9588 [16:32:31<93:30:46, 41.26s/it] 15%|█▍        | 1429/9588 [16:33:12<93:29:08, 41.25s/it] 15%|█▍        | 1430/9588 [16:33:53<93:29:18, 41.26s/it]                                                         {'loss': 0.3489, 'learning_rate': 1.9265074476223704e-05, 'epoch': 0.15}
 15%|█▍        | 1430/9588 [16:33:53<93:29:18, 41.26s/it] 15%|█▍        | 1431/9588 [16:34:34<93:28:14, 41.25s/it] 15%|█▍        | 1432/9588 [16:35:16<93:34:22, 41.30s/it][2024-04-12 16:58:06,482] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3617842
[2024-04-12 16:58:09,515] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3617843
[2024-04-12 16:58:12,927] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3617844
[2024-04-12 16:58:16,286] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3617845
[2024-04-12 16:58:19,325] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3617846
[2024-04-12 16:58:22,564] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3617847
[2024-04-12 16:58:26,674] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3617848
[2024-04-12 16:58:26,674] [ERROR] [launch.py:321:sigkill_handler] ['/home/data_llm/anaconda3/envs/moellava/bin/python', '-u', '/home/data_llm/FoodHealthMMLLM/moellava/train/train_xformers.py', '--local_rank=6', '--moe_enable', 'True', '--num_experts', '4', '--top_k_experts', '2', '--capacity_factor', '1.5', '--moe_mode', 'sparse', '--use_residual', 'False', '--router_aux_loss_coef', '0.01', '--train_modules', 'fc1', 'fc2', 'wg', '--deepspeed', '../../zero2_offload.json', '--model_name_or_path', '/mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-v1/checkpoint-90000', '--version', 'phi', '--data_path', '/mnt/data_llm/json_file/101_train_prompt1.json', '/mnt/data_llm/json_file/172_train_prompt1.json', '/mnt/data_llm/json_file/2k_train_prompt1.json', '--image_folder', '/media/LLM_data/food_recognition_dataset', '--image_tower', '/media/LLM_data/model/openai/clip-vit-large-patch14-336', '--image_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--check_point_file_name', '/mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-v1-moe-v1.json', '--output_dir', '/mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-v1-moe-v1', '--num_train_epochs', '1', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '3000', '--save_total_limit', '30', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--tf32', 'False', '--model_max_length', '512', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'tensorboard', '--cache_dir', '/media/fast_data/huggingface/hub/'] exits with return code = -15
