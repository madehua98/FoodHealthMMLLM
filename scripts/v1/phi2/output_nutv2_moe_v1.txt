nohup: å¿½ç•¥è¾“å…¥
0,1,2,3,7
[2024-04-22 23:13:33,157] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 23:13:35,803] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-22 23:13:35,803] [INFO] [runner.py:555:main] cmd = /home/data_llm/anaconda3/envs/moellava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgN119 --master_addr=127.0.0.1 --master_port=2225 --enable_each_rank_log=None /home/data_llm/FoodHealthMMLLM/moellava/train/train_xformers.py --moe_enable True --num_experts 4 --top_k_experts 2 --capacity_factor 1.5 --moe_mode sparse --use_residual False --router_aux_loss_coef 0.01 --train_modules fc1 fc2 wg --deepspeed ../../zero2_offload.json --model_name_or_path /mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-nutv2 --version phi --data_path /mnt/data_llm/json_file/101_train_prompt1.json /mnt/data_llm/json_file/172_train_prompt1.json --image_folder /media/LLM_data/food_recognition_dataset --image_tower openai/clip-vit-large-patch14-336 --image_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --check_point_file_name /mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-nutv2-moe-v1.json --output_dir /mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-nutv2-moe-v1 --num_train_epochs 1 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy epoch --save_total_limit 30 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 10 --tf32 False --model_max_length 512 --gradient_checkpointing True --dataloader_num_workers 16 --lazy_preprocess True --report_to tensorboard --cache_dir /media/fast_data/huggingface/hub/
[2024-04-22 23:13:37,162] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 23:13:41,310] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2024-04-22 23:13:41,310] [INFO] [launch.py:138:main] 0 NCCL_IB_TIMEOUT=22
[2024-04-22 23:13:41,310] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 7]}
[2024-04-22 23:13:41,310] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=5, node_rank=0
[2024-04-22 23:13:41,310] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4]})
[2024-04-22 23:13:41,310] [INFO] [launch.py:163:main] dist_world_size=5
[2024-04-22 23:13:41,310] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,7
[2024-04-22 23:13:44,772] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 23:13:44,774] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 23:13:44,808] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 23:13:44,944] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 23:13:44,998] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 23:13:45,779] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-22 23:13:45,779] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-04-22 23:13:45,825] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-22 23:13:45,825] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-04-22 23:13:45,845] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-22 23:13:45,845] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-04-22 23:13:45,999] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-22 23:13:45,999] [INFO] [comm.py:594:init_distributed] cdb=None
[2024-04-22 23:13:45,999] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-22 23:13:46,070] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-04-22 23:13:46,070] [INFO] [comm.py:594:init_distributed] cdb=None
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_phi to instantiate a model of type moe_llava_phi. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.95s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.25s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.93s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.85s/it]
LLM init. firstly
 MoELLaVAPhiForCausalLM(
  (model): MoELLaVAPhiModel(
    (embed_tokens): Embedding(51200, 2560, padding_idx=50295)
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-31): 32 x PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower()
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2560, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2560, out_features=2560, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2560, out_features=51200, bias=False)
)
[2024-04-22 23:13:51,285] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:05<00:05,  5.36s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:05<00:05,  5.40s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:05<00:05,  5.43s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.98s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  2.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.06s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  2.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.07s/it]
[2024-04-22 23:13:58,437] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 23:14:07,596] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 23:14:15,045] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 23:14:24,721] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 23:14:32,283] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 23:14:40,411] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 23:14:49,632] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 23:14:56,976] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 23:15:06,275] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 23:15:13,674] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 23:15:20,784] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 23:15:30,131] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 23:15:37,979] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 23:15:47,527] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2024-04-22 23:15:54,946] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.6391475200653076 seconds
Vision encoder and proj init.
 MoELLaVAPhiForCausalLM(
  (model): MoELLaVAPhiModel(
    (embed_tokens): Embedding(51200, 2560, padding_idx=50295)
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (1): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (2): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (3): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (4): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (5): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (6): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (7): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (8): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (9): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (10): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (11): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (12): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (13): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (14): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (15): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (16): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (17): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (18): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (19): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (20): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (21): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (22): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (23): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (24): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (25): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (26): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (27): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (28): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (29): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (30): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (31): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2560, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2560, out_features=2560, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2560, out_features=51200, bias=False)
)
model.layers.0.mlp.deepspeed_moe.gate.wg.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.1.mlp.fc1.weight
model.layers.1.mlp.fc1.bias
model.layers.1.mlp.fc2.weight
model.layers.1.mlp.fc2.bias
model.layers.2.mlp.deepspeed_moe.gate.wg.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.3.mlp.fc1.weight
model.layers.3.mlp.fc1.bias
model.layers.3.mlp.fc2.weight
model.layers.3.mlp.fc2.bias
model.layers.4.mlp.deepspeed_moe.gate.wg.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.5.mlp.fc1.weight
model.layers.5.mlp.fc1.bias
model.layers.5.mlp.fc2.weight
model.layers.5.mlp.fc2.bias
model.layers.6.mlp.deepspeed_moe.gate.wg.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.7.mlp.fc1.weight
model.layers.7.mlp.fc1.bias
model.layers.7.mlp.fc2.weight
model.layers.7.mlp.fc2.bias
model.layers.8.mlp.deepspeed_moe.gate.wg.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.9.mlp.fc1.weight
model.layers.9.mlp.fc1.bias
model.layers.9.mlp.fc2.weight
model.layers.9.mlp.fc2.bias
model.layers.10.mlp.deepspeed_moe.gate.wg.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.11.mlp.fc1.weight
model.layers.11.mlp.fc1.bias
model.layers.11.mlp.fc2.weight
model.layers.11.mlp.fc2.bias
model.layers.12.mlp.deepspeed_moe.gate.wg.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.13.mlp.fc1.weight
model.layers.13.mlp.fc1.bias
model.layers.13.mlp.fc2.weight
model.layers.13.mlp.fc2.bias
model.layers.14.mlp.deepspeed_moe.gate.wg.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.15.mlp.fc1.weight
model.layers.15.mlp.fc1.bias
model.layers.15.mlp.fc2.weight
model.layers.15.mlp.fc2.bias
model.layers.16.mlp.deepspeed_moe.gate.wg.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.17.mlp.fc1.weight
model.layers.17.mlp.fc1.bias
model.layers.17.mlp.fc2.weight
model.layers.17.mlp.fc2.bias
model.layers.18.mlp.deepspeed_moe.gate.wg.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.19.mlp.fc1.weight
model.layers.19.mlp.fc1.bias
model.layers.19.mlp.fc2.weight
model.layers.19.mlp.fc2.bias
model.layers.20.mlp.deepspeed_moe.gate.wg.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.21.mlp.fc1.weight
model.layers.21.mlp.fc1.bias
model.layers.21.mlp.fc2.weight
model.layers.21.mlp.fc2.bias
model.layers.22.mlp.deepspeed_moe.gate.wg.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.23.mlp.fc1.weight
model.layers.23.mlp.fc1.bias
model.layers.23.mlp.fc2.weight
model.layers.23.mlp.fc2.bias
model.layers.24.mlp.deepspeed_moe.gate.wg.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.24.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.25.mlp.fc1.weight
model.layers.25.mlp.fc1.bias
model.layers.25.mlp.fc2.weight
model.layers.25.mlp.fc2.bias
model.layers.26.mlp.deepspeed_moe.gate.wg.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.26.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.27.mlp.fc1.weight
model.layers.27.mlp.fc1.bias
model.layers.27.mlp.fc2.weight
model.layers.27.mlp.fc2.bias
model.layers.28.mlp.deepspeed_moe.gate.wg.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.28.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.29.mlp.fc1.weight
model.layers.29.mlp.fc1.bias
model.layers.29.mlp.fc2.weight
model.layers.29.mlp.fc2.bias
model.layers.30.mlp.deepspeed_moe.gate.wg.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.0.fc2.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.1.fc2.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.2.fc2.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc1.bias
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.weight
model.layers.30.mlp.deepspeed_moe.experts.deepspeed_experts.3.fc2.bias
model.layers.31.mlp.fc1.weight
model.layers.31.mlp.fc1.bias
model.layers.31.mlp.fc2.weight
model.layers.31.mlp.fc2.bias
model.mm_projector.image_spatial_proj.0.weight
model.mm_projector.image_spatial_proj.0.bias
model.mm_projector.image_spatial_proj.2.weight
model.mm_projector.image_spatial_proj.2.bias
MoELLaVAPhiForCausalLM(
  (model): MoELLaVAPhiModel(
    (embed_tokens): Embedding(51200, 2560, padding_idx=50295)
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (1): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (2): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (3): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (4): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (5): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (6): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (7): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (8): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (9): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (10): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (11): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (12): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (13): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (14): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (15): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (16): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (17): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (18): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (19): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (20): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (21): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (22): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (23): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (24): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (25): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (26): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (27): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (28): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (29): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (30): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): MoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2560, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x PhiMLP(
                  (activation_fn): NewGELUActivation()
                  (fc1): Linear(in_features=2560, out_features=10240, bias=True)
                  (fc2): Linear(in_features=10240, out_features=2560, bias=True)
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (31): PhiDecoderLayer(
        (self_attn): PhiAttention(
          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
          (dense): Linear(in_features=2560, out_features=2560, bias=True)
          (rotary_emb): PhiRotaryEmbedding()
        )
        (mlp): PhiMLP(
          (activation_fn): NewGELUActivation()
          (fc1): Linear(in_features=2560, out_features=10240, bias=True)
          (fc2): Linear(in_features=10240, out_features=2560, bias=True)
        )
        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2560, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2560, out_features=2560, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2560, out_features=51200, bias=False)
)
Formatting inputs...Skip in lazy mode
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.560112953186035 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4500765800476074 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4722676277160645 seconds
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/data_llm/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Emitting ninja build file /home/data_llm/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.5018601417541504 seconds
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
Rank: 1 partition count [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] and sizes[(169639936, False), (41984, False), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (10485760, True), (163840, True)] 
Rank: 3 partition count [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] and sizes[(169639936, False), (41984, False), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (10485760, True), (163840, True)] 
Rank: 4 partition count [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] and sizes[(169639936, False), (41984, False), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (10485760, True), (163840, True)] 
Rank: 0 partition count [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] and sizes[(169639936, False), (41984, False), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (10485760, True), (163840, True)] 
Rank: 2 partition count [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5] and sizes[(169639936, False), (41984, False), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (31457280, True), (10485760, True), (163840, True)] 
  0%|          | 0/1773 [00:00<?, ?it/s]/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/1773 [00:45<22:26:38, 45.60s/it]  0%|          | 2/1773 [01:18<18:39:19, 37.92s/it]  0%|          | 3/1773 [01:50<17:27:00, 35.49s/it]  0%|          | 4/1773 [02:23<16:53:38, 34.38s/it]  0%|          | 5/1773 [02:55<16:31:54, 33.66s/it]  0%|          | 6/1773 [03:28<16:20:07, 33.28s/it]  0%|          | 7/1773 [04:00<16:10:16, 32.97s/it]  0%|          | 8/1773 [04:33<16:06:17, 32.85s/it]  1%|          | 9/1773 [05:04<15:55:11, 32.49s/it]  1%|          | 10/1773 [05:37<15:55:31, 32.52s/it]                                                    {'loss': 2.991, 'learning_rate': 3.7037037037037037e-06, 'epoch': 0.01}
  1%|          | 10/1773 [05:37<15:55:31, 32.52s/it]  1%|          | 11/1773 [06:09<15:50:12, 32.36s/it]  1%|          | 12/1773 [06:42<15:52:16, 32.45s/it]  1%|          | 13/1773 [07:14<15:47:34, 32.30s/it]  1%|          | 14/1773 [07:47<15:55:39, 32.60s/it]  1%|          | 15/1773 [08:19<15:52:13, 32.50s/it]  1%|          | 16/1773 [08:53<16:00:40, 32.81s/it]  1%|          | 17/1773 [09:25<15:55:56, 32.66s/it]  1%|          | 18/1773 [09:58<15:55:03, 32.65s/it]  1%|          | 19/1773 [10:30<15:50:37, 32.52s/it]  1%|          | 20/1773 [11:02<15:48:18, 32.46s/it]                                                    {'loss': 2.271, 'learning_rate': 7.4074074074074075e-06, 'epoch': 0.01}
  1%|          | 20/1773 [11:02<15:48:18, 32.46s/it]  1%|          | 21/1773 [11:34<15:45:22, 32.38s/it]  1%|          | 22/1773 [12:06<15:41:43, 32.27s/it]  1%|â–         | 23/1773 [12:39<15:43:58, 32.37s/it]  1%|â–         | 24/1773 [13:11<15:38:53, 32.21s/it]  1%|â–         | 25/1773 [13:43<15:41:51, 32.33s/it]  1%|â–         | 26/1773 [14:15<15:37:36, 32.20s/it]  2%|â–         | 27/1773 [14:48<15:41:04, 32.34s/it]  2%|â–         | 28/1773 [15:20<15:38:51, 32.28s/it]  2%|â–         | 29/1773 [15:54<15:47:35, 32.60s/it]  2%|â–         | 30/1773 [16:26<15:44:42, 32.52s/it]                                                    {'loss': 1.7561, 'learning_rate': 1.1111111111111113e-05, 'epoch': 0.02}
  2%|â–         | 30/1773 [16:26<15:44:42, 32.52s/it]  2%|â–         | 31/1773 [16:59<15:51:38, 32.78s/it]  2%|â–         | 32/1773 [17:31<15:43:47, 32.53s/it]  2%|â–         | 33/1773 [18:04<15:49:30, 32.74s/it]  2%|â–         | 34/1773 [18:37<15:44:55, 32.60s/it]  2%|â–         | 35/1773 [19:10<15:47:03, 32.69s/it]  2%|â–         | 36/1773 [19:42<15:41:20, 32.52s/it]  2%|â–         | 37/1773 [20:14<15:37:54, 32.42s/it]  2%|â–         | 38/1773 [20:46<15:33:25, 32.28s/it]  2%|â–         | 39/1773 [21:18<15:30:41, 32.20s/it]  2%|â–         | 40/1773 [21:50<15:31:53, 32.26s/it]                                                    {'loss': 1.317, 'learning_rate': 1.4814814814814815e-05, 'epoch': 0.02}
  2%|â–         | 40/1773 [21:50<15:31:53, 32.26s/it]  2%|â–         | 41/1773 [22:22<15:27:01, 32.11s/it]  2%|â–         | 42/1773 [22:55<15:30:04, 32.24s/it]  2%|â–         | 43/1773 [23:26<15:26:17, 32.13s/it]  2%|â–         | 44/1773 [23:59<15:29:42, 32.26s/it]  3%|â–Ž         | 45/1773 [24:31<15:25:28, 32.13s/it]  3%|â–Ž         | 46/1773 [25:05<15:39:56, 32.66s/it]  3%|â–Ž         | 47/1773 [25:37<15:33:34, 32.45s/it]  3%|â–Ž         | 48/1773 [26:10<15:38:33, 32.65s/it]  3%|â–Ž         | 49/1773 [26:42<15:32:21, 32.45s/it]  3%|â–Ž         | 50/1773 [27:14<15:32:43, 32.48s/it]                                                    {'loss': 1.0077, 'learning_rate': 1.851851851851852e-05, 'epoch': 0.03}
  3%|â–Ž         | 50/1773 [27:14<15:32:43, 32.48s/it]  3%|â–Ž         | 51/1773 [27:46<15:25:48, 32.26s/it]  3%|â–Ž         | 52/1773 [28:18<15:23:44, 32.20s/it]  3%|â–Ž         | 53/1773 [28:50<15:20:08, 32.10s/it]  3%|â–Ž         | 54/1773 [29:22<15:16:18, 31.98s/it]  3%|â–Ž         | 55/1773 [29:54<15:16:37, 32.01s/it]  3%|â–Ž         | 56/1773 [30:25<15:11:57, 31.87s/it]  3%|â–Ž         | 57/1773 [30:58<15:14:39, 31.98s/it]  3%|â–Ž         | 58/1773 [31:29<15:11:28, 31.89s/it]  3%|â–Ž         | 59/1773 [32:02<15:15:20, 32.04s/it]  3%|â–Ž         | 60/1773 [32:33<15:10:54, 31.91s/it]                                                    {'loss': 0.8296, 'learning_rate': 1.999939880391268e-05, 'epoch': 0.03}
  3%|â–Ž         | 60/1773 [32:33<15:10:54, 31.91s/it]  3%|â–Ž         | 61/1773 [33:06<15:21:01, 32.28s/it]  3%|â–Ž         | 62/1773 [33:38<15:16:21, 32.13s/it]  4%|â–Ž         | 63/1773 [34:11<15:21:43, 32.34s/it]  4%|â–Ž         | 64/1773 [34:43<15:16:10, 32.17s/it]  4%|â–Ž         | 65/1773 [35:15<15:13:57, 32.11s/it]  4%|â–Ž         | 66/1773 [35:47<15:10:41, 32.01s/it]  4%|â–         | 67/1773 [36:18<15:08:55, 31.97s/it]  4%|â–         | 68/1773 [36:50<15:09:17, 32.00s/it]  4%|â–         | 69/1773 [37:22<15:05:45, 31.89s/it]  4%|â–         | 70/1773 [37:54<15:06:19, 31.93s/it]                                                    {'loss': 0.7455, 'learning_rate': 1.999572508960157e-05, 'epoch': 0.04}
  4%|â–         | 70/1773 [37:54<15:06:19, 31.93s/it]  4%|â–         | 71/1773 [38:26<15:02:22, 31.81s/it]  4%|â–         | 72/1773 [38:58<15:05:42, 31.95s/it]  4%|â–         | 73/1773 [39:30<15:02:58, 31.87s/it]  4%|â–         | 74/1773 [40:03<15:14:01, 32.28s/it]  4%|â–         | 75/1773 [40:35<15:08:48, 32.11s/it]  4%|â–         | 76/1773 [41:07<15:13:22, 32.29s/it]  4%|â–         | 77/1773 [41:39<15:09:24, 32.17s/it]  4%|â–         | 78/1773 [42:12<15:11:28, 32.26s/it]  4%|â–         | 79/1773 [42:43<15:07:03, 32.13s/it]  5%|â–         | 80/1773 [43:15<15:02:50, 32.00s/it]                                                    {'loss': 0.6408, 'learning_rate': 1.9988712884308967e-05, 'epoch': 0.05}
  5%|â–         | 80/1773 [43:15<15:02:50, 32.00s/it]  5%|â–         | 81/1773 [43:47<15:00:52, 31.95s/it]  5%|â–         | 82/1773 [44:19<14:57:02, 31.83s/it]  5%|â–         | 83/1773 [44:51<14:58:05, 31.89s/it]  5%|â–         | 84/1773 [45:22<14:53:50, 31.75s/it]  5%|â–         | 85/1773 [45:54<14:55:52, 31.84s/it]  5%|â–         | 86/1773 [46:26<14:54:22, 31.81s/it]  5%|â–         | 87/1773 [46:58<14:59:45, 32.02s/it]  5%|â–         | 88/1773 [47:30<14:55:50, 31.90s/it]  5%|â–Œ         | 89/1773 [48:03<15:05:51, 32.28s/it]  5%|â–Œ         | 90/1773 [48:35<14:59:26, 32.07s/it]                                                    {'loss': 0.5782, 'learning_rate': 1.9978364530054465e-05, 'epoch': 0.05}
  5%|â–Œ         | 90/1773 [48:35<14:59:26, 32.07s/it]  5%|â–Œ         | 91/1773 [49:08<15:06:34, 32.34s/it]  5%|â–Œ         | 92/1773 [49:39<14:59:36, 32.11s/it]  5%|â–Œ         | 93/1773 [50:11<14:56:41, 32.02s/it]  5%|â–Œ         | 94/1773 [50:43<14:53:59, 31.95s/it]  5%|â–Œ         | 95/1773 [51:14<14:51:10, 31.87s/it]  5%|â–Œ         | 96/1773 [51:47<14:52:53, 31.95s/it]  5%|â–Œ         | 97/1773 [52:18<14:49:36, 31.85s/it]  6%|â–Œ         | 98/1773 [52:50<14:50:35, 31.90s/it]  6%|â–Œ         | 99/1773 [53:22<14:48:17, 31.84s/it]  6%|â–Œ         | 100/1773 [53:55<14:56:59, 32.17s/it]                                                     {'loss': 0.5222, 'learning_rate': 1.996468348310433e-05, 'epoch': 0.06}
  6%|â–Œ         | 100/1773 [53:55<14:56:59, 32.17s/it]  6%|â–Œ         | 101/1773 [54:27<14:53:29, 32.06s/it]  6%|â–Œ         | 102/1773 [55:00<15:03:49, 32.45s/it]  6%|â–Œ         | 103/1773 [55:32<14:57:29, 32.25s/it]  6%|â–Œ         | 104/1773 [56:05<15:03:27, 32.48s/it]  6%|â–Œ         | 105/1773 [56:37<14:56:45, 32.26s/it]  6%|â–Œ         | 106/1773 [57:09<15:00:32, 32.41s/it]  6%|â–Œ         | 107/1773 [57:41<14:55:31, 32.25s/it]  6%|â–Œ         | 108/1773 [58:13<14:52:39, 32.17s/it]  6%|â–Œ         | 109/1773 [58:45<14:49:59, 32.09s/it]  6%|â–Œ         | 110/1773 [59:17<14:46:45, 31.99s/it]                                                     {'loss': 0.4923, 'learning_rate': 1.9947674312817105e-05, 'epoch': 0.06}
  6%|â–Œ         | 110/1773 [59:17<14:46:45, 31.99s/it]  6%|â–‹         | 111/1773 [59:49<14:49:35, 32.12s/it]  6%|â–‹         | 112/1773 [1:00:21<14:45:45, 32.00s/it]  6%|â–‹         | 113/1773 [1:00:53<14:47:48, 32.09s/it]  6%|â–‹         | 114/1773 [1:01:25<14:46:08, 32.05s/it]  6%|â–‹         | 115/1773 [1:01:58<14:52:12, 32.29s/it]  7%|â–‹         | 116/1773 [1:02:30<14:47:52, 32.15s/it]  7%|â–‹         | 117/1773 [1:03:04<15:02:27, 32.70s/it]  7%|â–‹         | 118/1773 [1:03:36<14:57:13, 32.53s/it]  7%|â–‹         | 119/1773 [1:04:09<15:04:01, 32.79s/it]  7%|â–‹         | 120/1773 [1:04:41<14:56:29, 32.54s/it]                                                       {'loss': 0.4754, 'learning_rate': 1.99273427001175e-05, 'epoch': 0.07}
  7%|â–‹         | 120/1773 [1:04:41<14:56:29, 32.54s/it]  7%|â–‹         | 121/1773 [1:05:14<14:56:22, 32.56s/it]  7%|â–‹         | 122/1773 [1:05:46<14:51:00, 32.38s/it]  7%|â–‹         | 123/1773 [1:06:18<14:47:46, 32.28s/it]  7%|â–‹         | 124/1773 [1:06:50<14:46:04, 32.24s/it]  7%|â–‹         | 125/1773 [1:07:22<14:41:52, 32.11s/it]  7%|â–‹         | 126/1773 [1:07:55<14:47:25, 32.33s/it]  7%|â–‹         | 127/1773 [1:08:27<14:42:09, 32.16s/it]  7%|â–‹         | 128/1773 [1:08:59<14:43:30, 32.23s/it]  7%|â–‹         | 129/1773 [1:09:31<14:39:55, 32.11s/it]  7%|â–‹         | 130/1773 [1:10:04<14:49:59, 32.50s/it]                                                       {'loss': 0.4679, 'learning_rate': 1.9903695435598992e-05, 'epoch': 0.07}
  7%|â–‹         | 130/1773 [1:10:04<14:49:59, 32.50s/it]  7%|â–‹         | 131/1773 [1:10:36<14:45:54, 32.37s/it]  7%|â–‹         | 132/1773 [1:11:10<14:55:22, 32.74s/it]  8%|â–Š         | 133/1773 [1:11:42<14:49:06, 32.53s/it]  8%|â–Š         | 134/1773 [1:12:15<14:55:13, 32.77s/it]  8%|â–Š         | 135/1773 [1:12:47<14:48:34, 32.55s/it]  8%|â–Š         | 136/1773 [1:13:20<14:48:32, 32.57s/it]  8%|â–Š         | 137/1773 [1:13:52<14:41:58, 32.35s/it]  8%|â–Š         | 138/1773 [1:14:24<14:37:04, 32.19s/it]  8%|â–Š         | 139/1773 [1:14:56<14:35:15, 32.14s/it]  8%|â–Š         | 140/1773 [1:15:27<14:31:30, 32.02s/it]                                                       {'loss': 0.4657, 'learning_rate': 1.9876740417255835e-05, 'epoch': 0.08}
  8%|â–Š         | 140/1773 [1:15:27<14:31:30, 32.02s/it]  8%|â–Š         | 141/1773 [1:16:00<14:36:19, 32.22s/it]  8%|â–Š         | 142/1773 [1:16:32<14:32:22, 32.09s/it]  8%|â–Š         | 143/1773 [1:17:04<14:33:46, 32.16s/it]  8%|â–Š         | 144/1773 [1:17:36<14:28:39, 31.99s/it]  8%|â–Š         | 145/1773 [1:18:09<14:38:22, 32.37s/it]  8%|â–Š         | 146/1773 [1:18:41<14:35:34, 32.29s/it]  8%|â–Š         | 147/1773 [1:19:15<14:45:43, 32.68s/it]  8%|â–Š         | 148/1773 [1:19:47<14:40:06, 32.50s/it]  8%|â–Š         | 149/1773 [1:20:20<14:42:10, 32.59s/it]  8%|â–Š         | 150/1773 [1:20:52<14:36:58, 32.42s/it]                                                       {'loss': 0.4581, 'learning_rate': 1.9846486647845197e-05, 'epoch': 0.08}
  8%|â–Š         | 150/1773 [1:20:52<14:36:58, 32.42s/it]  9%|â–Š         | 151/1773 [1:21:24<14:32:54, 32.29s/it]  9%|â–Š         | 152/1773 [1:21:56<14:32:18, 32.29s/it]  9%|â–Š         | 153/1773 [1:22:28<14:29:26, 32.20s/it]  9%|â–Š         | 154/1773 [1:23:01<14:33:16, 32.36s/it]  9%|â–Š         | 155/1773 [1:23:32<14:28:16, 32.20s/it]  9%|â–‰         | 156/1773 [1:24:05<14:30:03, 32.28s/it]  9%|â–‰         | 157/1773 [1:24:37<14:27:55, 32.22s/it]  9%|â–‰         | 158/1773 [1:25:10<14:35:41, 32.53s/it]  9%|â–‰         | 159/1773 [1:25:42<14:30:23, 32.36s/it]  9%|â–‰         | 160/1773 [1:26:16<14:43:03, 32.85s/it]                                                       {'loss': 0.4178, 'learning_rate': 1.9812944231880292e-05, 'epoch': 0.09}
  9%|â–‰         | 160/1773 [1:26:16<14:43:03, 32.85s/it]  9%|â–‰         | 161/1773 [1:26:48<14:33:39, 32.52s/it]  9%|â–‰         | 162/1773 [1:27:21<14:39:03, 32.74s/it]  9%|â–‰         | 163/1773 [1:27:53<14:31:27, 32.48s/it]  9%|â–‰         | 164/1773 [1:28:26<14:31:35, 32.50s/it]  9%|â–‰         | 165/1773 [1:28:57<14:25:48, 32.31s/it]  9%|â–‰         | 166/1773 [1:29:29<14:21:05, 32.15s/it]  9%|â–‰         | 167/1773 [1:30:01<14:19:20, 32.10s/it]  9%|â–‰         | 168/1773 [1:30:33<14:16:39, 32.02s/it] 10%|â–‰         | 169/1773 [1:31:05<14:19:18, 32.14s/it] 10%|â–‰         | 170/1773 [1:31:37<14:15:16, 32.01s/it]                                                       {'loss': 0.4173, 'learning_rate': 1.9776124372255582e-05, 'epoch': 0.1}
 10%|â–‰         | 170/1773 [1:31:37<14:15:16, 32.01s/it] 10%|â–‰         | 171/1773 [1:32:10<14:19:14, 32.18s/it] 10%|â–‰         | 172/1773 [1:32:41<14:14:03, 32.01s/it] 10%|â–‰         | 173/1773 [1:33:15<14:24:13, 32.41s/it] 10%|â–‰         | 174/1773 [1:33:47<14:19:41, 32.26s/it] 10%|â–‰         | 175/1773 [1:34:20<14:26:18, 32.53s/it] 10%|â–‰         | 176/1773 [1:34:52<14:20:30, 32.33s/it] 10%|â–‰         | 177/1773 [1:35:24<14:20:44, 32.36s/it] 10%|â–ˆ         | 178/1773 [1:35:56<14:16:51, 32.23s/it] 10%|â–ˆ         | 179/1773 [1:36:28<14:14:21, 32.16s/it] 10%|â–ˆ         | 180/1773 [1:37:00<14:11:55, 32.09s/it]                                                       {'loss': 0.4119, 'learning_rate': 1.9736039366505087e-05, 'epoch': 0.1}
 10%|â–ˆ         | 180/1773 [1:37:00<14:11:55, 32.09s/it] 10%|â–ˆ         | 181/1773 [1:37:31<14:07:01, 31.92s/it] 10%|â–ˆ         | 182/1773 [1:38:04<14:08:43, 32.01s/it] 10%|â–ˆ         | 183/1773 [1:38:35<14:05:20, 31.90s/it] 10%|â–ˆ         | 184/1773 [1:39:08<14:08:58, 32.06s/it] 10%|â–ˆ         | 185/1773 [1:39:40<14:06:35, 31.99s/it] 10%|â–ˆ         | 186/1773 [1:40:13<14:19:11, 32.48s/it] 11%|â–ˆ         | 187/1773 [1:40:45<14:13:28, 32.29s/it] 11%|â–ˆ         | 188/1773 [1:41:18<14:19:58, 32.55s/it] 11%|â–ˆ         | 189/1773 [1:41:50<14:14:23, 32.36s/it] 11%|â–ˆ         | 190/1773 [1:42:22<14:14:06, 32.37s/it]                                                       {'loss': 0.3991, 'learning_rate': 1.9692702602695097e-05, 'epoch': 0.11}
 11%|â–ˆ         | 190/1773 [1:42:23<14:14:06, 32.37s/it] 11%|â–ˆ         | 191/1773 [1:42:54<14:09:13, 32.21s/it] 11%|â–ˆ         | 192/1773 [1:43:26<14:05:14, 32.08s/it] 11%|â–ˆ         | 193/1773 [1:43:58<14:05:37, 32.11s/it] 11%|â–ˆ         | 194/1773 [1:44:30<14:01:54, 31.99s/it] 11%|â–ˆ         | 195/1773 [1:45:02<14:04:25, 32.11s/it] 11%|â–ˆ         | 196/1773 [1:45:34<13:59:45, 31.95s/it] 11%|â–ˆ         | 197/1773 [1:46:06<14:01:49, 32.05s/it] 11%|â–ˆ         | 198/1773 [1:46:38<13:57:38, 31.91s/it] 11%|â–ˆ         | 199/1773 [1:47:11<14:08:20, 32.34s/it] 11%|â–ˆâ–        | 200/1773 [1:47:43<14:05:11, 32.24s/it]                                                       {'loss': 0.3868, 'learning_rate': 1.9646128554952685e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 200/1773 [1:47:43<14:05:11, 32.24s/it] 11%|â–ˆâ–        | 201/1773 [1:48:17<14:15:03, 32.64s/it] 11%|â–ˆâ–        | 202/1773 [1:48:49<14:08:36, 32.41s/it] 11%|â–ˆâ–        | 203/1773 [1:49:21<14:11:16, 32.53s/it] 12%|â–ˆâ–        | 204/1773 [1:49:54<14:07:10, 32.40s/it] 12%|â–ˆâ–        | 205/1773 [1:50:25<14:02:55, 32.25s/it] 12%|â–ˆâ–        | 206/1773 [1:50:57<14:00:29, 32.18s/it] 12%|â–ˆâ–        | 207/1773 [1:51:29<13:57:27, 32.09s/it] 12%|â–ˆâ–        | 208/1773 [1:52:02<13:57:48, 32.12s/it] 12%|â–ˆâ–        | 209/1773 [1:52:33<13:52:38, 31.94s/it] 12%|â–ˆâ–        | 210/1773 [1:53:05<13:53:47, 32.01s/it]                                                       {'loss': 0.4048, 'learning_rate': 1.9596332778631467e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 210/1773 [1:53:05<13:53:47, 32.01s/it] 12%|â–ˆâ–        | 211/1773 [1:53:37<13:51:10, 31.93s/it] 12%|â–ˆâ–        | 212/1773 [1:54:10<13:58:04, 32.21s/it] 12%|â–ˆâ–        | 213/1773 [1:54:42<13:54:36, 32.10s/it] 12%|â–ˆâ–        | 214/1773 [1:55:15<14:06:25, 32.58s/it] 12%|â–ˆâ–        | 215/1773 [1:55:47<14:00:36, 32.37s/it] 12%|â–ˆâ–        | 216/1773 [1:56:20<14:06:43, 32.63s/it] 12%|â–ˆâ–        | 217/1773 [1:56:52<14:01:02, 32.43s/it] 12%|â–ˆâ–        | 218/1773 [1:57:24<13:56:59, 32.30s/it] 12%|â–ˆâ–        | 219/1773 [1:57:56<13:53:25, 32.18s/it] 12%|â–ˆâ–        | 220/1773 [1:58:28<13:51:23, 32.12s/it]                                                       {'loss': 0.4022, 'learning_rate': 1.9543331905116224e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 220/1773 [1:58:28<13:51:23, 32.12s/it] 12%|â–ˆâ–        | 221/1773 [1:59:00<13:51:09, 32.13s/it] 13%|â–ˆâ–Ž        | 222/1773 [1:59:32<13:46:54, 31.99s/it] 13%|â–ˆâ–Ž        | 223/1773 [2:00:04<13:48:16, 32.06s/it] 13%|â–ˆâ–Ž        | 224/1773 [2:00:36<13:45:05, 31.96s/it] 13%|â–ˆâ–Ž        | 225/1773 [2:01:09<13:51:02, 32.21s/it] 13%|â–ˆâ–Ž        | 226/1773 [2:01:41<13:46:49, 32.07s/it] 13%|â–ˆâ–Ž        | 227/1773 [2:02:14<13:59:30, 32.58s/it] 13%|â–ˆâ–Ž        | 228/1773 [2:02:46<13:54:24, 32.40s/it] 13%|â–ˆâ–Ž        | 229/1773 [2:03:20<14:00:00, 32.64s/it] 13%|â–ˆâ–Ž        | 230/1773 [2:03:51<13:53:03, 32.39s/it]                                                       {'loss': 0.3934, 'learning_rate': 1.948714363626817e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 230/1773 [2:03:51<13:53:03, 32.39s/it] 13%|â–ˆâ–Ž        | 231/1773 [2:04:24<13:53:13, 32.42s/it] 13%|â–ˆâ–Ž        | 232/1773 [2:04:56<13:48:26, 32.26s/it] 13%|â–ˆâ–Ž        | 233/1773 [2:05:28<13:44:28, 32.12s/it] 13%|â–ˆâ–Ž        | 234/1773 [2:06:00<13:43:52, 32.12s/it] 13%|â–ˆâ–Ž        | 235/1773 [2:06:31<13:39:59, 31.99s/it] 13%|â–ˆâ–Ž        | 236/1773 [2:07:04<13:42:41, 32.12s/it] 13%|â–ˆâ–Ž        | 237/1773 [2:07:35<13:38:08, 31.96s/it] 13%|â–ˆâ–Ž        | 238/1773 [2:08:08<13:41:18, 32.10s/it] 13%|â–ˆâ–Ž        | 239/1773 [2:08:40<13:38:05, 32.00s/it] 14%|â–ˆâ–Ž        | 240/1773 [2:09:13<13:48:57, 32.44s/it]                                                       {'loss': 0.3835, 'learning_rate': 1.9427786738512667e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 240/1773 [2:09:13<13:48:57, 32.44s/it] 14%|â–ˆâ–Ž        | 241/1773 [2:09:45<13:46:03, 32.35s/it] 14%|â–ˆâ–Ž        | 242/1773 [2:10:19<13:54:01, 32.69s/it] 14%|â–ˆâ–Ž        | 243/1773 [2:10:51<13:48:51, 32.50s/it] 14%|â–ˆâ–        | 244/1773 [2:11:23<13:50:24, 32.59s/it] 14%|â–ˆâ–        | 245/1773 [2:11:55<13:44:14, 32.37s/it] 14%|â–ˆâ–        | 246/1773 [2:12:27<13:40:42, 32.25s/it] 14%|â–ˆâ–        | 247/1773 [2:12:59<13:38:46, 32.19s/it] 14%|â–ˆâ–        | 248/1773 [2:13:31<13:34:54, 32.06s/it] 14%|â–ˆâ–        | 249/1773 [2:14:04<13:38:39, 32.23s/it] 14%|â–ˆâ–        | 250/1773 [2:14:35<13:33:56, 32.07s/it]                                                       {'loss': 0.3802, 'learning_rate': 1.9365281036571383e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 250/1773 [2:14:35<13:33:56, 32.07s/it] 14%|â–ˆâ–        | 251/1773 [2:15:08<13:34:59, 32.13s/it] 14%|â–ˆâ–        | 252/1773 [2:15:40<13:33:18, 32.08s/it] 14%|â–ˆâ–        | 253/1773 [2:16:13<13:39:48, 32.36s/it] 14%|â–ˆâ–        | 254/1773 [2:16:45<13:36:00, 32.23s/it] 14%|â–ˆâ–        | 255/1773 [2:17:18<13:45:27, 32.63s/it] 14%|â–ˆâ–        | 256/1773 [2:17:50<13:40:14, 32.44s/it] 14%|â–ˆâ–        | 257/1773 [2:18:23<13:43:01, 32.57s/it] 15%|â–ˆâ–        | 258/1773 [2:18:55<13:38:51, 32.43s/it] 15%|â–ˆâ–        | 259/1773 [2:19:27<13:34:11, 32.27s/it] 15%|â–ˆâ–        | 260/1773 [2:19:59<13:30:39, 32.15s/it]                                                       {'loss': 0.3655, 'learning_rate': 1.9299647406841017e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 260/1773 [2:19:59<13:30:39, 32.15s/it] 15%|â–ˆâ–        | 261/1773 [2:20:31<13:27:04, 32.03s/it] 15%|â–ˆâ–        | 262/1773 [2:21:03<13:28:59, 32.12s/it] 15%|â–ˆâ–        | 263/1773 [2:21:35<13:25:11, 31.99s/it] 15%|â–ˆâ–        | 264/1773 [2:22:07<13:27:32, 32.11s/it] 15%|â–ˆâ–        | 265/1773 [2:22:39<13:24:46, 32.02s/it] 15%|â–ˆâ–Œ        | 266/1773 [2:23:12<13:34:20, 32.42s/it] 15%|â–ˆâ–Œ        | 267/1773 [2:23:44<13:31:09, 32.32s/it] 15%|â–ˆâ–Œ        | 268/1773 [2:24:18<13:37:05, 32.58s/it] 15%|â–ˆâ–Œ        | 269/1773 [2:24:49<13:31:33, 32.38s/it] 15%|â–ˆâ–Œ        | 270/1773 [2:25:22<13:33:55, 32.49s/it]                                                       {'loss': 0.3541, 'learning_rate': 1.9230907770420737e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 270/1773 [2:25:22<13:33:55, 32.49s/it] 15%|â–ˆâ–Œ        | 271/1773 [2:25:54<13:27:51, 32.27s/it] 15%|â–ˆâ–Œ        | 272/1773 [2:26:26<13:24:20, 32.15s/it] 15%|â–ˆâ–Œ        | 273/1773 [2:26:58<13:22:35, 32.10s/it] 15%|â–ˆâ–Œ        | 274/1773 [2:27:29<13:18:36, 31.97s/it] 16%|â–ˆâ–Œ        | 275/1773 [2:28:02<13:22:41, 32.15s/it] 16%|â–ˆâ–Œ        | 276/1773 [2:28:34<13:19:33, 32.05s/it] 16%|â–ˆâ–Œ        | 277/1773 [2:29:06<13:22:10, 32.17s/it] 16%|â–ˆâ–Œ        | 278/1773 [2:29:38<13:18:42, 32.06s/it] 16%|â–ˆâ–Œ        | 279/1773 [2:30:12<13:30:14, 32.54s/it] 16%|â–ˆâ–Œ        | 280/1773 [2:30:44<13:24:35, 32.33s/it]                                                       {'loss': 0.3956, 'learning_rate': 1.9159085085790716e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 280/1773 [2:30:44<13:24:35, 32.33s/it] 16%|â–ˆâ–Œ        | 281/1773 [2:31:17<13:31:13, 32.62s/it] 16%|â–ˆâ–Œ        | 282/1773 [2:31:49<13:25:56, 32.43s/it] 16%|â–ˆâ–Œ        | 283/1773 [2:32:22<13:27:24, 32.51s/it] 16%|â–ˆâ–Œ        | 284/1773 [2:32:53<13:21:59, 32.32s/it] 16%|â–ˆâ–Œ        | 285/1773 [2:33:25<13:19:07, 32.22s/it] 16%|â–ˆâ–Œ        | 286/1773 [2:33:57<13:16:04, 32.12s/it] 16%|â–ˆâ–Œ        | 287/1773 [2:34:29<13:13:19, 32.03s/it] 16%|â–ˆâ–Œ        | 288/1773 [2:35:02<13:15:00, 32.12s/it] 16%|â–ˆâ–‹        | 289/1773 [2:35:33<13:10:51, 31.98s/it] 16%|â–ˆâ–‹        | 290/1773 [2:36:06<13:13:19, 32.10s/it]                                                       {'loss': 0.3766, 'learning_rate': 1.9084203341144177e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 290/1773 [2:36:06<13:13:19, 32.10s/it] 16%|â–ˆâ–‹        | 291/1773 [2:36:37<13:10:54, 32.02s/it] 16%|â–ˆâ–‹        | 292/1773 [2:37:11<13:18:54, 32.37s/it] 17%|â–ˆâ–‹        | 293/1773 [2:37:43<13:15:25, 32.25s/it] 17%|â–ˆâ–‹        | 294/1773 [2:38:16<13:24:25, 32.63s/it] 17%|â–ˆâ–‹        | 295/1773 [2:38:48<13:18:20, 32.41s/it] 17%|â–ˆâ–‹        | 296/1773 [2:39:21<13:21:49, 32.57s/it] 17%|â–ˆâ–‹        | 297/1773 [2:39:53<13:16:59, 32.40s/it] 17%|â–ˆâ–‹        | 298/1773 [2:40:25<13:15:50, 32.37s/it] 17%|â–ˆâ–‹        | 299/1773 [2:40:57<13:10:46, 32.19s/it] 17%|â–ˆâ–‹        | 300/1773 [2:41:29<13:08:24, 32.11s/it]                                                       {'loss': 0.3424, 'learning_rate': 1.9006287546375546e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 300/1773 [2:41:29<13:08:24, 32.11s/it] 17%|â–ˆâ–‹        | 301/1773 [2:42:01<13:05:51, 32.03s/it] 17%|â–ˆâ–‹        | 302/1773 [2:42:32<13:03:22, 31.95s/it] 17%|â–ˆâ–‹        | 303/1773 [2:43:05<13:05:23, 32.06s/it] 17%|â–ˆâ–‹        | 304/1773 [2:43:36<13:00:50, 31.89s/it] 17%|â–ˆâ–‹        | 305/1773 [2:44:09<13:02:50, 32.00s/it] 17%|â–ˆâ–‹        | 306/1773 [2:44:40<12:59:58, 31.90s/it] 17%|â–ˆâ–‹        | 307/1773 [2:45:13<13:08:17, 32.26s/it] 17%|â–ˆâ–‹        | 308/1773 [2:45:45<13:04:54, 32.15s/it] 17%|â–ˆâ–‹        | 309/1773 [2:46:18<13:12:49, 32.49s/it] 17%|â–ˆâ–‹        | 310/1773 [2:46:50<13:08:35, 32.34s/it]                                                       {'loss': 0.3715, 'learning_rate': 1.8925363724727338e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 310/1773 [2:46:50<13:08:35, 32.34s/it] 18%|â–ˆâ–Š        | 311/1773 [2:47:23<13:12:17, 32.52s/it] 18%|â–ˆâ–Š        | 312/1773 [2:47:55<13:07:00, 32.32s/it] 18%|â–ˆâ–Š        | 313/1773 [2:48:28<13:06:58, 32.34s/it] 18%|â–ˆâ–Š        | 314/1773 [2:48:59<13:02:07, 32.16s/it] 18%|â–ˆâ–Š        | 315/1773 [2:49:31<12:58:57, 32.06s/it] 18%|â–ˆâ–Š        | 316/1773 [2:50:03<12:59:27, 32.10s/it] 18%|â–ˆâ–Š        | 317/1773 [2:50:35<12:55:37, 31.96s/it] 18%|â–ˆâ–Š        | 318/1773 [2:51:07<12:57:53, 32.08s/it] 18%|â–ˆâ–Š        | 319/1773 [2:51:39<12:54:06, 31.94s/it] 18%|â–ˆâ–Š        | 320/1773 [2:52:12<12:59:09, 32.17s/it]                                                       {'loss': 0.3734, 'learning_rate': 1.884145890409862e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 320/1773 [2:52:12<12:59:09, 32.17s/it] 18%|â–ˆâ–Š        | 321/1773 [2:52:43<12:55:02, 32.03s/it] 18%|â–ˆâ–Š        | 322/1773 [2:53:17<13:03:14, 32.39s/it] 18%|â–ˆâ–Š        | 323/1773 [2:53:49<12:59:02, 32.24s/it] 18%|â–ˆâ–Š        | 324/1773 [2:54:21<13:02:44, 32.41s/it] 18%|â–ˆâ–Š        | 325/1773 [2:54:53<12:58:08, 32.24s/it] 18%|â–ˆâ–Š        | 326/1773 [2:55:25<12:56:53, 32.21s/it] 18%|â–ˆâ–Š        | 327/1773 [2:55:57<12:55:19, 32.17s/it] 18%|â–ˆâ–Š        | 328/1773 [2:56:29<12:52:19, 32.07s/it] 19%|â–ˆâ–Š        | 329/1773 [2:57:01<12:52:13, 32.09s/it] 19%|â–ˆâ–Š        | 330/1773 [2:57:33<12:48:27, 31.95s/it]                                                       {'loss': 0.3502, 'learning_rate': 1.8754601108017895e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 330/1773 [2:57:33<12:48:27, 31.95s/it] 19%|â–ˆâ–Š        | 331/1773 [2:58:05<12:49:35, 32.02s/it] 19%|â–ˆâ–Š        | 332/1773 [2:58:37<12:47:26, 31.95s/it] 19%|â–ˆâ–‰        | 333/1773 [2:59:10<12:51:43, 32.16s/it] 19%|â–ˆâ–‰        | 334/1773 [2:59:41<12:48:13, 32.03s/it] 19%|â–ˆâ–‰        | 335/1773 [3:00:15<12:58:57, 32.50s/it] 19%|â–ˆâ–‰        | 336/1773 [3:00:47<12:54:33, 32.34s/it] 19%|â–ˆâ–‰        | 337/1773 [3:01:20<13:01:22, 32.65s/it] 19%|â–ˆâ–‰        | 338/1773 [3:01:52<12:55:13, 32.41s/it] 19%|â–ˆâ–‰        | 339/1773 [3:02:25<12:59:27, 32.61s/it] 19%|â–ˆâ–‰        | 340/1773 [3:02:57<12:53:14, 32.38s/it]                                                       {'loss': 0.3387, 'learning_rate': 1.8664819346283477e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 340/1773 [3:02:57<12:53:14, 32.38s/it] 19%|â–ˆâ–‰        | 341/1773 [3:03:29<12:52:58, 32.39s/it] 19%|â–ˆâ–‰        | 342/1773 [3:04:01<12:47:35, 32.18s/it] 19%|â–ˆâ–‰        | 343/1773 [3:04:33<12:43:26, 32.03s/it] 19%|â–ˆâ–‰        | 344/1773 [3:05:05<12:42:32, 32.02s/it] 19%|â–ˆâ–‰        | 345/1773 [3:05:36<12:39:10, 31.90s/it] 20%|â–ˆâ–‰        | 346/1773 [3:06:09<12:42:11, 32.05s/it] 20%|â–ˆâ–‰        | 347/1773 [3:06:41<12:39:45, 31.97s/it] 20%|â–ˆâ–‰        | 348/1773 [3:07:13<12:42:53, 32.12s/it] 20%|â–ˆâ–‰        | 349/1773 [3:07:45<12:39:10, 31.99s/it] 20%|â–ˆâ–‰        | 350/1773 [3:08:18<12:50:23, 32.48s/it]                                                       {'loss': 0.3334, 'learning_rate': 1.8572143605274463e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 350/1773 [3:08:18<12:50:23, 32.48s/it] 20%|â–ˆâ–‰        | 351/1773 [3:08:50<12:45:30, 32.30s/it] 20%|â–ˆâ–‰        | 352/1773 [3:09:23<12:49:57, 32.51s/it] 20%|â–ˆâ–‰        | 353/1773 [3:09:55<12:45:36, 32.35s/it] 20%|â–ˆâ–‰        | 354/1773 [3:10:28<12:44:33, 32.33s/it] 20%|â–ˆâ–ˆ        | 355/1773 [3:10:59<12:40:58, 32.20s/it] 20%|â–ˆâ–ˆ        | 356/1773 [3:11:31<12:37:50, 32.09s/it] 20%|â–ˆâ–ˆ        | 357/1773 [3:12:03<12:36:28, 32.05s/it] 20%|â–ˆâ–ˆ        | 358/1773 [3:12:35<12:32:28, 31.91s/it] 20%|â–ˆâ–ˆ        | 359/1773 [3:13:07<12:35:35, 32.06s/it] 20%|â–ˆâ–ˆ        | 360/1773 [3:13:39<12:31:46, 31.92s/it]                                                       {'loss': 0.3658, 'learning_rate': 1.8476604837935515e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 360/1773 [3:13:39<12:31:46, 31.92s/it] 20%|â–ˆâ–ˆ        | 361/1773 [3:14:12<12:36:15, 32.14s/it] 20%|â–ˆâ–ˆ        | 362/1773 [3:14:43<12:32:28, 32.00s/it] 20%|â–ˆâ–ˆ        | 363/1773 [3:15:17<12:43:01, 32.47s/it] 21%|â–ˆâ–ˆ        | 364/1773 [3:15:49<12:38:11, 32.29s/it] 21%|â–ˆâ–ˆ        | 365/1773 [3:16:21<12:41:39, 32.46s/it] 21%|â–ˆâ–ˆ        | 366/1773 [3:16:53<12:37:30, 32.30s/it] 21%|â–ˆâ–ˆ        | 367/1773 [3:17:25<12:33:46, 32.17s/it] 21%|â–ˆâ–ˆ        | 368/1773 [3:17:57<12:31:39, 32.10s/it] 21%|â–ˆâ–ˆ        | 369/1773 [3:18:29<12:28:33, 31.99s/it] 21%|â–ˆâ–ˆ        | 370/1773 [3:19:01<12:30:09, 32.08s/it]                                                       {'loss': 0.3193, 'learning_rate': 1.837823495343883e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 370/1773 [3:19:01<12:30:09, 32.08s/it] 21%|â–ˆâ–ˆ        | 371/1773 [3:19:33<12:25:50, 31.92s/it] 21%|â–ˆâ–ˆ        | 372/1773 [3:20:05<12:28:15, 32.05s/it] 21%|â–ˆâ–ˆ        | 373/1773 [3:20:37<12:25:25, 31.95s/it] 21%|â–ˆâ–ˆ        | 374/1773 [3:21:10<12:32:30, 32.27s/it] 21%|â–ˆâ–ˆ        | 375/1773 [3:21:42<12:28:05, 32.11s/it] 21%|â–ˆâ–ˆ        | 376/1773 [3:22:15<12:38:29, 32.58s/it] 21%|â–ˆâ–ˆâ–       | 377/1773 [3:22:47<12:32:25, 32.34s/it] 21%|â–ˆâ–ˆâ–       | 378/1773 [3:23:20<12:36:42, 32.55s/it] 21%|â–ˆâ–ˆâ–       | 379/1773 [3:23:52<12:31:25, 32.34s/it] 21%|â–ˆâ–ˆâ–       | 380/1773 [3:24:24<12:30:33, 32.33s/it]                                                       {'loss': 0.3303, 'learning_rate': 1.8277066806526746e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 380/1773 [3:24:24<12:30:33, 32.33s/it] 21%|â–ˆâ–ˆâ–       | 381/1773 [3:24:56<12:26:45, 32.19s/it] 22%|â–ˆâ–ˆâ–       | 382/1773 [3:25:28<12:22:42, 32.04s/it] 22%|â–ˆâ–ˆâ–       | 383/1773 [3:26:00<12:21:37, 32.01s/it] 22%|â–ˆâ–ˆâ–       | 384/1773 [3:26:31<12:17:26, 31.85s/it] 22%|â–ˆâ–ˆâ–       | 385/1773 [3:27:04<12:20:26, 32.01s/it] 22%|â–ˆâ–ˆâ–       | 386/1773 [3:27:35<12:17:32, 31.91s/it] 22%|â–ˆâ–ˆâ–       | 387/1773 [3:28:08<12:21:43, 32.11s/it] 22%|â–ˆâ–ˆâ–       | 388/1773 [3:28:40<12:18:21, 31.99s/it] 22%|â–ˆâ–ˆâ–       | 389/1773 [3:29:13<12:26:19, 32.36s/it] 22%|â–ˆâ–ˆâ–       | 390/1773 [3:29:45<12:22:02, 32.19s/it]                                                       {'loss': 0.347, 'learning_rate': 1.8173134186538505e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 390/1773 [3:29:45<12:22:02, 32.19s/it] 22%|â–ˆâ–ˆâ–       | 391/1773 [3:30:17<12:26:29, 32.41s/it] 22%|â–ˆâ–ˆâ–       | 392/1773 [3:30:49<12:22:53, 32.28s/it] 22%|â–ˆâ–ˆâ–       | 393/1773 [3:31:22<12:21:56, 32.26s/it] 22%|â–ˆâ–ˆâ–       | 394/1773 [3:31:53<12:17:52, 32.10s/it] 22%|â–ˆâ–ˆâ–       | 395/1773 [3:32:25<12:15:37, 32.03s/it] 22%|â–ˆâ–ˆâ–       | 396/1773 [3:32:57<12:14:31, 32.01s/it] 22%|â–ˆâ–ˆâ–       | 397/1773 [3:33:29<12:10:08, 31.84s/it] 22%|â–ˆâ–ˆâ–       | 398/1773 [3:34:01<12:12:36, 31.97s/it] 23%|â–ˆâ–ˆâ–Ž       | 399/1773 [3:34:33<12:10:37, 31.90s/it] 23%|â–ˆâ–ˆâ–Ž       | 400/1773 [3:35:05<12:15:09, 32.13s/it]                                                       {'loss': 0.3385, 'learning_rate': 1.806647180612491e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 400/1773 [3:35:05<12:15:09, 32.13s/it] 23%|â–ˆâ–ˆâ–Ž       | 401/1773 [3:35:37<12:11:31, 31.99s/it] 23%|â–ˆâ–ˆâ–Ž       | 402/1773 [3:36:11<12:22:07, 32.48s/it] 23%|â–ˆâ–ˆâ–Ž       | 403/1773 [3:36:42<12:16:22, 32.25s/it] 23%|â–ˆâ–ˆâ–Ž       | 404/1773 [3:37:15<12:21:44, 32.51s/it] 23%|â–ˆâ–ˆâ–Ž       | 405/1773 [3:37:47<12:16:29, 32.30s/it] 23%|â–ˆâ–ˆâ–Ž       | 406/1773 [3:38:20<12:16:40, 32.33s/it] 23%|â–ˆâ–ˆâ–Ž       | 407/1773 [3:38:52<12:12:56, 32.19s/it] 23%|â–ˆâ–ˆâ–Ž       | 408/1773 [3:39:23<12:09:49, 32.08s/it] 23%|â–ˆâ–ˆâ–Ž       | 409/1773 [3:39:55<12:08:51, 32.06s/it] 23%|â–ˆâ–ˆâ–Ž       | 410/1773 [3:40:27<12:05:48, 31.95s/it]                                                       {'loss': 0.3406, 'learning_rate': 1.7957115289654564e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 410/1773 [3:40:27<12:05:48, 31.95s/it] 23%|â–ˆâ–ˆâ–Ž       | 411/1773 [3:40:59<12:07:12, 32.04s/it] 23%|â–ˆâ–ˆâ–Ž       | 412/1773 [3:41:31<12:04:52, 31.96s/it] 23%|â–ˆâ–ˆâ–Ž       | 413/1773 [3:42:04<12:07:54, 32.11s/it] 23%|â–ˆâ–ˆâ–Ž       | 414/1773 [3:42:35<12:03:56, 31.96s/it] 23%|â–ˆâ–ˆâ–Ž       | 415/1773 [3:43:08<12:12:28, 32.36s/it] 23%|â–ˆâ–ˆâ–Ž       | 416/1773 [3:43:40<12:09:02, 32.23s/it] 24%|â–ˆâ–ˆâ–Ž       | 417/1773 [3:44:14<12:15:52, 32.56s/it] 24%|â–ˆâ–ˆâ–Ž       | 418/1773 [3:44:46<12:11:15, 32.38s/it] 24%|â–ˆâ–ˆâ–Ž       | 419/1773 [3:45:19<12:13:45, 32.51s/it] 24%|â–ˆâ–ˆâ–Ž       | 420/1773 [3:45:50<12:08:01, 32.28s/it]                                                       {'loss': 0.3377, 'learning_rate': 1.784510116131563e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–Ž       | 420/1773 [3:45:50<12:08:01, 32.28s/it] 24%|â–ˆâ–ˆâ–Ž       | 421/1773 [3:46:22<12:04:50, 32.17s/it] 24%|â–ˆâ–ˆâ–       | 422/1773 [3:46:54<12:03:07, 32.12s/it] 24%|â–ˆâ–ˆâ–       | 423/1773 [3:47:26<11:58:54, 31.95s/it] 24%|â–ˆâ–ˆâ–       | 424/1773 [3:47:58<12:01:53, 32.11s/it] 24%|â–ˆâ–ˆâ–       | 425/1773 [3:48:30<11:57:42, 31.95s/it] 24%|â–ˆâ–ˆâ–       | 426/1773 [3:49:02<11:59:05, 32.03s/it] 24%|â–ˆâ–ˆâ–       | 427/1773 [3:49:34<11:56:52, 31.96s/it] 24%|â–ˆâ–ˆâ–       | 428/1773 [3:50:07<12:03:36, 32.28s/it] 24%|â–ˆâ–ˆâ–       | 429/1773 [3:50:39<12:00:27, 32.16s/it] 24%|â–ˆâ–ˆâ–       | 430/1773 [3:51:13<12:11:51, 32.70s/it]                                                       {'loss': 0.3166, 'learning_rate': 1.773046683291705e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 430/1773 [3:51:13<12:11:51, 32.70s/it] 24%|â–ˆâ–ˆâ–       | 431/1773 [3:51:44<12:04:53, 32.41s/it] 24%|â–ˆâ–ˆâ–       | 432/1773 [3:52:17<12:08:19, 32.59s/it] 24%|â–ˆâ–ˆâ–       | 433/1773 [3:52:49<12:02:45, 32.36s/it] 24%|â–ˆâ–ˆâ–       | 434/1773 [3:53:21<12:00:55, 32.30s/it] 25%|â–ˆâ–ˆâ–       | 435/1773 [3:53:53<11:56:07, 32.11s/it] 25%|â–ˆâ–ˆâ–       | 436/1773 [3:54:25<11:53:36, 32.02s/it] 25%|â–ˆâ–ˆâ–       | 437/1773 [3:54:57<11:51:55, 31.97s/it] 25%|â–ˆâ–ˆâ–       | 438/1773 [3:55:28<11:48:04, 31.82s/it] 25%|â–ˆâ–ˆâ–       | 439/1773 [3:56:00<11:50:46, 31.97s/it] 25%|â–ˆâ–ˆâ–       | 440/1773 [3:56:32<11:48:15, 31.88s/it]                                                       {'loss': 0.3415, 'learning_rate': 1.76132505913933e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 440/1773 [3:56:32<11:48:15, 31.88s/it] 25%|â–ˆâ–ˆâ–       | 441/1773 [3:57:04<11:49:20, 31.95s/it] 25%|â–ˆâ–ˆâ–       | 442/1773 [3:57:36<11:45:43, 31.81s/it] 25%|â–ˆâ–ˆâ–       | 443/1773 [3:58:09<11:52:12, 32.13s/it] 25%|â–ˆâ–ˆâ–Œ       | 444/1773 [3:58:41<11:50:46, 32.09s/it] 25%|â–ˆâ–ˆâ–Œ       | 445/1773 [3:59:14<11:57:11, 32.40s/it] 25%|â–ˆâ–ˆâ–Œ       | 446/1773 [3:59:45<11:52:05, 32.20s/it] 25%|â–ˆâ–ˆâ–Œ       | 447/1773 [4:00:18<11:54:10, 32.32s/it] 25%|â–ˆâ–ˆâ–Œ       | 448/1773 [4:00:50<11:49:24, 32.12s/it] 25%|â–ˆâ–ˆâ–Œ       | 449/1773 [4:01:21<11:45:31, 31.97s/it] 25%|â–ˆâ–ˆâ–Œ       | 450/1773 [4:01:53<11:42:56, 31.88s/it]                                                       {'loss': 0.3324, 'learning_rate': 1.749349158601686e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 450/1773 [4:01:53<11:42:56, 31.88s/it] 25%|â–ˆâ–ˆâ–Œ       | 451/1773 [4:02:25<11:39:52, 31.76s/it] 25%|â–ˆâ–ˆâ–Œ       | 452/1773 [4:02:57<11:43:45, 31.96s/it] 26%|â–ˆâ–ˆâ–Œ       | 453/1773 [4:03:28<11:39:42, 31.81s/it] 26%|â–ˆâ–ˆâ–Œ       | 454/1773 [4:04:00<11:40:39, 31.87s/it] 26%|â–ˆâ–ˆâ–Œ       | 455/1773 [4:04:32<11:38:19, 31.79s/it] 26%|â–ˆâ–ˆâ–Œ       | 456/1773 [4:05:05<11:45:32, 32.14s/it] 26%|â–ˆâ–ˆâ–Œ       | 457/1773 [4:05:37<11:42:59, 32.05s/it] 26%|â–ˆâ–ˆâ–Œ       | 458/1773 [4:06:10<11:50:49, 32.43s/it] 26%|â–ˆâ–ˆâ–Œ       | 459/1773 [4:06:42<11:46:10, 32.25s/it] 26%|â–ˆâ–ˆâ–Œ       | 460/1773 [4:07:15<11:49:51, 32.44s/it]                                                       {'loss': 0.3149, 'learning_rate': 1.7371229815322687e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 460/1773 [4:07:15<11:49:51, 32.44s/it] 26%|â–ˆâ–ˆâ–Œ       | 461/1773 [4:07:47<11:45:17, 32.25s/it] 26%|â–ˆâ–ˆâ–Œ       | 462/1773 [4:08:19<11:44:38, 32.25s/it] 26%|â–ˆâ–ˆâ–Œ       | 463/1773 [4:08:51<11:40:11, 32.07s/it] 26%|â–ˆâ–ˆâ–Œ       | 464/1773 [4:09:22<11:36:58, 31.95s/it] 26%|â–ˆâ–ˆâ–Œ       | 465/1773 [4:09:54<11:35:20, 31.90s/it] 26%|â–ˆâ–ˆâ–‹       | 466/1773 [4:10:26<11:32:15, 31.78s/it] 26%|â–ˆâ–ˆâ–‹       | 467/1773 [4:10:58<11:34:20, 31.90s/it] 26%|â–ˆâ–ˆâ–‹       | 468/1773 [4:11:29<11:31:56, 31.81s/it] 26%|â–ˆâ–ˆâ–‹       | 469/1773 [4:12:01<11:33:52, 31.93s/it] 27%|â–ˆâ–ˆâ–‹       | 470/1773 [4:12:33<11:30:59, 31.82s/it]                                                       {'loss': 0.3191, 'learning_rate': 1.7246506113749018e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 470/1773 [4:12:33<11:30:59, 31.82s/it] 27%|â–ˆâ–ˆâ–‹       | 471/1773 [4:13:06<11:39:45, 32.25s/it] 27%|â–ˆâ–ˆâ–‹       | 472/1773 [4:13:38<11:35:21, 32.07s/it] 27%|â–ˆâ–ˆâ–‹       | 473/1773 [4:14:11<11:41:21, 32.37s/it] 27%|â–ˆâ–ˆâ–‹       | 474/1773 [4:14:43<11:37:48, 32.23s/it] 27%|â–ˆâ–ˆâ–‹       | 475/1773 [4:15:15<11:38:20, 32.28s/it] 27%|â–ˆâ–ˆâ–‹       | 476/1773 [4:15:47<11:33:00, 32.06s/it] 27%|â–ˆâ–ˆâ–‹       | 477/1773 [4:16:19<11:29:43, 31.93s/it] 27%|â–ˆâ–ˆâ–‹       | 478/1773 [4:16:50<11:28:36, 31.90s/it] 27%|â–ˆâ–ˆâ–‹       | 479/1773 [4:17:22<11:25:30, 31.79s/it] 27%|â–ˆâ–ˆâ–‹       | 480/1773 [4:17:54<11:28:05, 31.93s/it]                                                       {'loss': 0.326, 'learning_rate': 1.7119362137999018e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 480/1773 [4:17:54<11:28:05, 31.93s/it] 27%|â–ˆâ–ˆâ–‹       | 481/1773 [4:18:26<11:25:31, 31.84s/it] 27%|â–ˆâ–ˆâ–‹       | 482/1773 [4:18:58<11:26:42, 31.92s/it] 27%|â–ˆâ–ˆâ–‹       | 483/1773 [4:19:29<11:23:31, 31.79s/it] 27%|â–ˆâ–ˆâ–‹       | 484/1773 [4:20:03<11:32:02, 32.21s/it] 27%|â–ˆâ–ˆâ–‹       | 485/1773 [4:20:34<11:28:47, 32.09s/it] 27%|â–ˆâ–ˆâ–‹       | 486/1773 [4:21:07<11:34:36, 32.38s/it] 27%|â–ˆâ–ˆâ–‹       | 487/1773 [4:21:39<11:30:14, 32.20s/it] 28%|â–ˆâ–ˆâ–Š       | 488/1773 [4:22:12<11:30:50, 32.26s/it] 28%|â–ˆâ–ˆâ–Š       | 489/1773 [4:22:43<11:27:04, 32.11s/it] 28%|â–ˆâ–ˆâ–Š       | 490/1773 [4:23:15<11:24:31, 32.01s/it]                                                       {'loss': 0.322, 'learning_rate': 1.698984035312775e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 490/1773 [4:23:15<11:24:31, 32.01s/it] 28%|â–ˆâ–ˆâ–Š       | 491/1773 [4:23:47<11:24:06, 32.02s/it] 28%|â–ˆâ–ˆâ–Š       | 492/1773 [4:24:19<11:19:56, 31.85s/it] 28%|â–ˆâ–ˆâ–Š       | 493/1773 [4:24:51<11:22:06, 31.97s/it] 28%|â–ˆâ–ˆâ–Š       | 494/1773 [4:25:22<11:18:58, 31.85s/it] 28%|â–ˆâ–ˆâ–Š       | 495/1773 [4:25:55<11:22:48, 32.06s/it] 28%|â–ˆâ–ˆâ–Š       | 496/1773 [4:26:27<11:19:56, 31.95s/it] 28%|â–ˆâ–ˆâ–Š       | 497/1773 [4:27:00<11:29:23, 32.42s/it] 28%|â–ˆâ–ˆâ–Š       | 498/1773 [4:27:32<11:25:20, 32.25s/it] 28%|â–ˆâ–ˆâ–Š       | 499/1773 [4:28:05<11:28:38, 32.43s/it] 28%|â–ˆâ–ˆâ–Š       | 500/1773 [4:28:37<11:23:58, 32.24s/it]                                                       {'loss': 0.3184, 'learning_rate': 1.6857984018359212e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 500/1773 [4:28:37<11:23:58, 32.24s/it] 28%|â–ˆâ–ˆâ–Š       | 501/1773 [4:29:09<11:24:14, 32.28s/it] 28%|â–ˆâ–ˆâ–Š       | 502/1773 [4:29:41<11:20:18, 32.12s/it] 28%|â–ˆâ–ˆâ–Š       | 503/1773 [4:30:13<11:17:55, 32.03s/it] 28%|â–ˆâ–ˆâ–Š       | 504/1773 [4:30:45<11:16:38, 31.99s/it] 28%|â–ˆâ–ˆâ–Š       | 505/1773 [4:31:16<11:13:52, 31.89s/it] 29%|â–ˆâ–ˆâ–Š       | 506/1773 [4:31:48<11:15:21, 31.98s/it] 29%|â–ˆâ–ˆâ–Š       | 507/1773 [4:32:20<11:11:39, 31.83s/it] 29%|â–ˆâ–ˆâ–Š       | 508/1773 [4:32:52<11:13:31, 31.95s/it] 29%|â–ˆâ–ˆâ–Š       | 509/1773 [4:33:24<11:11:14, 31.86s/it] 29%|â–ˆâ–ˆâ–‰       | 510/1773 [4:33:57<11:16:46, 32.15s/it]                                                       {'loss': 0.3339, 'learning_rate': 1.6723837172638117e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 510/1773 [4:33:57<11:16:46, 32.15s/it] 29%|â–ˆâ–ˆâ–‰       | 511/1773 [4:34:28<11:13:45, 32.03s/it] 29%|â–ˆâ–ˆâ–‰       | 512/1773 [4:35:01<11:19:54, 32.35s/it] 29%|â–ˆâ–ˆâ–‰       | 513/1773 [4:35:33<11:16:03, 32.19s/it] 29%|â–ˆâ–ˆâ–‰       | 514/1773 [4:36:06<11:19:46, 32.40s/it] 29%|â–ˆâ–ˆâ–‰       | 515/1773 [4:36:38<11:16:12, 32.25s/it] 29%|â–ˆâ–ˆâ–‰       | 516/1773 [4:37:10<11:15:06, 32.22s/it] 29%|â–ˆâ–ˆâ–‰       | 517/1773 [4:37:42<11:12:04, 32.11s/it] 29%|â–ˆâ–ˆâ–‰       | 518/1773 [4:38:14<11:08:58, 31.98s/it] 29%|â–ˆâ–ˆâ–‰       | 519/1773 [4:38:46<11:07:46, 31.95s/it] 29%|â–ˆâ–ˆâ–‰       | 520/1773 [4:39:17<11:03:47, 31.79s/it]                                                       {'loss': 0.3069, 'learning_rate': 1.658744461992121e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 520/1773 [4:39:17<11:03:47, 31.79s/it] 29%|â–ˆâ–ˆâ–‰       | 521/1773 [4:39:49<11:04:58, 31.87s/it] 29%|â–ˆâ–ˆâ–‰       | 522/1773 [4:40:21<11:03:23, 31.82s/it] 29%|â–ˆâ–ˆâ–‰       | 523/1773 [4:40:54<11:10:04, 32.16s/it] 30%|â–ˆâ–ˆâ–‰       | 524/1773 [4:41:25<11:06:46, 32.03s/it] 30%|â–ˆâ–ˆâ–‰       | 525/1773 [4:41:58<11:11:53, 32.30s/it] 30%|â–ˆâ–ˆâ–‰       | 526/1773 [4:42:30<11:07:38, 32.12s/it] 30%|â–ˆâ–ˆâ–‰       | 527/1773 [4:43:03<11:09:52, 32.26s/it] 30%|â–ˆâ–ˆâ–‰       | 528/1773 [4:43:34<11:06:13, 32.11s/it] 30%|â–ˆâ–ˆâ–‰       | 529/1773 [4:44:06<11:03:37, 32.01s/it] 30%|â–ˆâ–ˆâ–‰       | 530/1773 [4:44:38<11:01:17, 31.92s/it]                                                       {'loss': 0.3288, 'learning_rate': 1.6448851914213136e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 530/1773 [4:44:38<11:01:17, 31.92s/it] 30%|â–ˆâ–ˆâ–‰       | 531/1773 [4:45:09<10:57:49, 31.78s/it] 30%|â–ˆâ–ˆâ–ˆ       | 532/1773 [4:45:42<11:00:34, 31.94s/it] 30%|â–ˆâ–ˆâ–ˆ       | 533/1773 [4:46:13<10:57:30, 31.82s/it] 30%|â–ˆâ–ˆâ–ˆ       | 534/1773 [4:46:46<11:01:51, 32.05s/it] 30%|â–ˆâ–ˆâ–ˆ       | 535/1773 [4:47:17<10:58:39, 31.92s/it] 30%|â–ˆâ–ˆâ–ˆ       | 536/1773 [4:47:51<11:05:47, 32.29s/it] 30%|â–ˆâ–ˆâ–ˆ       | 537/1773 [4:48:22<11:02:12, 32.15s/it] 30%|â–ˆâ–ˆâ–ˆ       | 538/1773 [4:48:55<11:07:26, 32.43s/it] 30%|â–ˆâ–ˆâ–ˆ       | 539/1773 [4:49:27<11:03:32, 32.26s/it] 30%|â–ˆâ–ˆâ–ˆ       | 540/1773 [4:50:00<11:03:54, 32.31s/it]                                                       {'loss': 0.3286, 'learning_rate': 1.6308105344351776e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 540/1773 [4:50:00<11:03:54, 32.31s/it] 31%|â–ˆâ–ˆâ–ˆ       | 541/1773 [4:50:31<10:59:42, 32.13s/it] 31%|â–ˆâ–ˆâ–ˆ       | 542/1773 [4:51:03<10:57:19, 32.04s/it] 31%|â–ˆâ–ˆâ–ˆ       | 543/1773 [4:51:35<10:55:54, 32.00s/it] 31%|â–ˆâ–ˆâ–ˆ       | 544/1773 [4:52:07<10:53:01, 31.88s/it] 31%|â–ˆâ–ˆâ–ˆ       | 545/1773 [4:52:39<10:56:16, 32.07s/it] 31%|â–ˆâ–ˆâ–ˆ       | 546/1773 [4:53:11<10:52:36, 31.91s/it] 31%|â–ˆâ–ˆâ–ˆ       | 547/1773 [4:53:43<10:54:57, 32.05s/it] 31%|â–ˆâ–ˆâ–ˆ       | 548/1773 [4:54:15<10:51:06, 31.89s/it] 31%|â–ˆâ–ˆâ–ˆ       | 549/1773 [4:54:48<11:00:42, 32.39s/it] 31%|â–ˆâ–ˆâ–ˆ       | 550/1773 [4:55:20<10:56:39, 32.22s/it]                                                       {'loss': 0.3167, 'learning_rate': 1.6165251918548134e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 550/1773 [4:55:20<10:56:39, 32.22s/it] 31%|â–ˆâ–ˆâ–ˆ       | 551/1773 [4:55:53<11:02:22, 32.52s/it] 31%|â–ˆâ–ˆâ–ˆ       | 552/1773 [4:56:25<10:57:19, 32.30s/it] 31%|â–ˆâ–ˆâ–ˆ       | 553/1773 [4:56:58<11:00:10, 32.47s/it] 31%|â–ˆâ–ˆâ–ˆ       | 554/1773 [4:57:30<10:55:46, 32.28s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 555/1773 [4:58:02<10:55:27, 32.29s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 556/1773 [4:58:34<10:51:20, 32.11s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 557/1773 [4:59:05<10:47:25, 31.95s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 558/1773 [4:59:37<10:47:19, 31.97s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 559/1773 [5:00:09<10:44:22, 31.85s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 560/1773 [5:00:41<10:45:39, 31.94s/it]                                                       {'loss': 0.3104, 'learning_rate': 1.602033934868598e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 560/1773 [5:00:41<10:45:39, 31.94s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 561/1773 [5:01:13<10:43:10, 31.84s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 562/1773 [5:01:46<10:48:28, 32.13s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 563/1773 [5:02:17<10:46:10, 32.04s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 564/1773 [5:02:51<10:54:26, 32.48s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 565/1773 [5:03:23<10:49:13, 32.25s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 566/1773 [5:03:56<10:53:56, 32.51s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 567/1773 [5:04:27<10:49:13, 32.30s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 568/1773 [5:05:00<10:48:01, 32.27s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 569/1773 [5:05:31<10:44:18, 32.11s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 570/1773 [5:06:03<10:41:10, 31.98s/it]                                                       {'loss': 0.3086, 'learning_rate': 1.5873416034386485e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 570/1773 [5:06:03<10:41:10, 31.98s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 571/1773 [5:06:35<10:41:08, 32.00s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 572/1773 [5:07:07<10:37:34, 31.85s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 573/1773 [5:07:39<10:39:07, 31.96s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 574/1773 [5:08:10<10:36:18, 31.84s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 575/1773 [5:08:43<10:41:32, 32.13s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 576/1773 [5:09:15<10:37:34, 31.96s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 577/1773 [5:09:48<10:45:51, 32.40s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 578/1773 [5:10:20<10:42:32, 32.26s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 579/1773 [5:10:53<10:47:04, 32.52s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 580/1773 [5:11:25<10:42:31, 32.31s/it]                                                       {'loss': 0.2886, 'learning_rate': 1.5724531046843117e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 580/1773 [5:11:25<10:42:31, 32.31s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 581/1773 [5:11:57<10:42:19, 32.33s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 582/1773 [5:12:29<10:38:28, 32.16s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 583/1773 [5:13:01<10:35:46, 32.06s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 584/1773 [5:13:33<10:33:44, 31.98s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 585/1773 [5:14:04<10:30:42, 31.85s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 586/1773 [5:14:37<10:33:02, 32.00s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 587/1773 [5:15:08<10:29:14, 31.83s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 588/1773 [5:15:40<10:31:21, 31.97s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 589/1773 [5:16:12<10:27:57, 31.82s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 590/1773 [5:16:45<10:35:55, 32.25s/it]                                                       {'loss': 0.305, 'learning_rate': 1.5573734112432273e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 590/1773 [5:16:45<10:35:55, 32.25s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 591/1773 [5:17:17<10:32:59, 32.13s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 592/1773 [5:17:50<10:39:42, 32.50s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 593/1773 [5:18:22<10:35:01, 32.29s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 594/1773 [5:18:55<10:36:51, 32.41s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 595/1773 [5:19:27<10:32:18, 32.21s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 596/1773 [5:19:59<10:29:48, 32.11s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 597/1773 [5:20:30<10:28:07, 32.05s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 598/1773 [5:21:02<10:25:30, 31.94s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 599/1773 [5:21:34<10:26:22, 32.01s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 600/1773 [5:22:06<10:22:59, 31.87s/it]                                                       {'loss': 0.2989, 'learning_rate': 1.542107559610508e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 600/1773 [5:22:06<10:22:59, 31.87s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 601/1773 [5:22:38<10:24:49, 31.99s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 602/1773 [5:23:10<10:22:34, 31.90s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 603/1773 [5:23:43<10:28:44, 32.24s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 604/1773 [5:24:15<10:24:59, 32.08s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 605/1773 [5:24:48<10:30:07, 32.37s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 606/1773 [5:25:19<10:26:51, 32.23s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 607/1773 [5:25:52<10:27:02, 32.27s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 608/1773 [5:26:23<10:22:08, 32.04s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 609/1773 [5:26:55<10:19:54, 31.95s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 610/1773 [5:27:27<10:19:32, 31.96s/it]                                                       {'loss': 0.3053, 'learning_rate': 1.5266606484565896e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 610/1773 [5:27:27<10:19:32, 31.96s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 611/1773 [5:27:59<10:16:28, 31.83s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 612/1773 [5:28:31<10:18:07, 31.94s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 613/1773 [5:29:03<10:16:06, 31.87s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 614/1773 [5:29:35<10:20:42, 32.13s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 615/1773 [5:30:07<10:17:34, 32.00s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 616/1773 [5:30:41<10:26:04, 32.47s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 617/1773 [5:31:12<10:22:43, 32.32s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 618/1773 [5:31:46<10:27:53, 32.62s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 619/1773 [5:32:18<10:22:43, 32.38s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 620/1773 [5:32:50<10:22:03, 32.37s/it]                                                       {'loss': 0.3171, 'learning_rate': 1.5110378369243171e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 620/1773 [5:32:50<10:22:03, 32.37s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 621/1773 [5:33:22<10:18:07, 32.19s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 622/1773 [5:33:54<10:15:31, 32.09s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 623/1773 [5:34:25<10:13:46, 32.02s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 624/1773 [5:34:57<10:10:47, 31.90s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 625/1773 [5:35:29<10:12:40, 32.02s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 626/1773 [5:36:01<10:09:29, 31.88s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 627/1773 [5:36:33<10:10:35, 31.97s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 628/1773 [5:37:05<10:08:40, 31.90s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 629/1773 [5:37:38<10:14:16, 32.22s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 630/1773 [5:38:10<10:11:04, 32.08s/it]                                                       {'loss': 0.3203, 'learning_rate': 1.4952443429058334e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 630/1773 [5:38:10<10:11:04, 32.08s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 631/1773 [5:38:43<10:18:34, 32.50s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 632/1773 [5:39:15<10:14:38, 32.32s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 633/1773 [5:39:48<10:18:37, 32.56s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 634/1773 [5:40:20<10:13:54, 32.34s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 635/1773 [5:40:52<10:14:33, 32.40s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 636/1773 [5:41:24<10:11:28, 32.27s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 637/1773 [5:41:56<10:09:10, 32.17s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 638/1773 [5:42:28<10:06:52, 32.08s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 639/1773 [5:43:00<10:03:26, 31.93s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 640/1773 [5:43:32<10:05:18, 32.06s/it]                                                       {'loss': 0.3248, 'learning_rate': 1.4792854412998455e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 640/1773 [5:43:32<10:05:18, 32.06s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 641/1773 [5:44:04<10:02:21, 31.93s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 642/1773 [5:44:36<10:06:07, 32.16s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 643/1773 [5:45:08<10:02:53, 32.01s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 644/1773 [5:45:42<10:11:18, 32.49s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 645/1773 [5:46:13<10:06:39, 32.27s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 646/1773 [5:46:46<10:10:06, 32.48s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 647/1773 [5:47:18<10:05:31, 32.27s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 648/1773 [5:47:50<10:02:58, 32.16s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 649/1773 [5:48:22<10:00:29, 32.05s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 650/1773 [5:48:54<9:58:31, 31.98s/it]                                                       {'loss': 0.3171, 'learning_rate': 1.46316646224985e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 650/1773 [5:48:54<9:58:31, 31.98s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 651/1773 [5:49:26<9:59:16, 32.05s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 652/1773 [5:49:57<9:55:50, 31.89s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 653/1773 [5:50:30<9:57:06, 31.99s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 654/1773 [5:51:01<9:54:22, 31.87s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 655/1773 [5:51:34<9:59:36, 32.18s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 656/1773 [5:52:06<9:55:51, 32.01s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 657/1773 [5:52:39<10:03:44, 32.46s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 658/1773 [5:53:11<10:00:11, 32.30s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 659/1773 [5:53:44<10:03:03, 32.48s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 660/1773 [5:54:16<9:58:38, 32.27s/it]                                                       {'loss': 0.3134, 'learning_rate': 1.4468927893639106e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 660/1773 [5:54:16<9:58:38, 32.27s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 661/1773 [5:54:48<9:58:49, 32.31s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 662/1773 [5:55:20<9:55:44, 32.17s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 663/1773 [5:55:52<9:52:48, 32.04s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 664/1773 [5:56:24<9:52:01, 32.03s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 665/1773 [5:56:55<9:48:17, 31.86s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 666/1773 [5:57:28<9:50:14, 31.99s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 667/1773 [5:57:59<9:47:19, 31.86s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 668/1773 [5:58:31<9:47:56, 31.92s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 669/1773 [5:59:03<9:45:50, 31.84s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 670/1773 [5:59:36<9:53:26, 32.28s/it]                                                      {'loss': 0.3088, 'learning_rate': 1.4304698579165747e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 670/1773 [5:59:36<9:53:26, 32.28s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 671/1773 [6:00:08<9:50:49, 32.17s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 672/1773 [6:00:41<9:56:19, 32.50s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 673/1773 [6:01:13<9:51:39, 32.27s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 674/1773 [6:01:46<9:54:42, 32.47s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 675/1773 [6:02:18<9:50:47, 32.28s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 676/1773 [6:02:50<9:51:02, 32.33s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 677/1773 [6:03:22<9:46:53, 32.13s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 678/1773 [6:03:54<9:43:47, 31.99s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 679/1773 [6:04:26<9:43:10, 31.98s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 680/1773 [6:04:57<9:39:54, 31.83s/it]                                                      {'loss': 0.3043, 'learning_rate': 1.4139031530335373e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 680/1773 [6:04:57<9:39:54, 31.83s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 681/1773 [6:05:29<9:40:40, 31.91s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 682/1773 [6:06:01<9:37:52, 31.78s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 683/1773 [6:06:34<9:42:52, 32.08s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 684/1773 [6:07:05<9:40:55, 32.01s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 685/1773 [6:07:39<9:48:58, 32.48s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 686/1773 [6:08:11<9:44:41, 32.27s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 687/1773 [6:08:44<9:47:19, 32.45s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 688/1773 [6:09:15<9:42:35, 32.22s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 689/1773 [6:09:47<9:41:22, 32.18s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 690/1773 [6:10:19<9:39:05, 32.08s/it]                                                      {'loss': 0.2912, 'learning_rate': 1.3971982078596526e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 690/1773 [6:10:19<9:39:05, 32.08s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 691/1773 [6:10:51<9:35:52, 31.93s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 692/1773 [6:11:23<9:35:41, 31.95s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 693/1773 [6:11:54<9:32:58, 31.83s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 694/1773 [6:12:27<9:35:04, 31.98s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 695/1773 [6:12:58<9:33:02, 31.90s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 696/1773 [6:13:31<9:35:26, 32.06s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 697/1773 [6:14:02<9:32:18, 31.91s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 698/1773 [6:14:36<9:39:10, 32.33s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 699/1773 [6:15:08<9:36:49, 32.22s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 700/1773 [6:15:41<9:40:03, 32.44s/it]                                                      {'loss': 0.2964, 'learning_rate': 1.3803606017109087e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 700/1773 [6:15:41<9:40:03, 32.44s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 701/1773 [6:16:12<9:35:46, 32.23s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 702/1773 [6:16:45<9:36:04, 32.27s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 703/1773 [6:17:16<9:31:55, 32.07s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 704/1773 [6:17:48<9:28:44, 31.92s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 705/1773 [6:18:20<9:28:28, 31.94s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 706/1773 [6:18:51<9:25:39, 31.81s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 707/1773 [6:19:24<9:27:18, 31.93s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 708/1773 [6:19:55<9:23:57, 31.77s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 709/1773 [6:20:28<9:29:52, 32.14s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 710/1773 [6:21:00<9:27:44, 32.05s/it]                                                      {'loss': 0.2919, 'learning_rate': 1.363395958210983e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 710/1773 [6:21:00<9:27:44, 32.05s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 711/1773 [6:21:33<9:33:30, 32.40s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 712/1773 [6:22:05<9:29:56, 32.23s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 713/1773 [6:22:38<9:33:24, 32.46s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 714/1773 [6:23:10<9:29:19, 32.26s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 715/1773 [6:23:41<9:25:59, 32.10s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 716/1773 [6:24:13<9:24:44, 32.06s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 717/1773 [6:24:45<9:22:21, 31.95s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 718/1773 [6:25:17<9:23:03, 32.02s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 719/1773 [6:25:49<9:19:37, 31.86s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 720/1773 [6:26:21<9:20:02, 31.91s/it]                                                      {'loss': 0.3016, 'learning_rate': 1.346309943412995e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 720/1773 [6:26:21<9:20:02, 31.91s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 721/1773 [6:26:53<9:19:02, 31.88s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 722/1773 [6:27:25<9:23:52, 32.19s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 723/1773 [6:27:57<9:21:17, 32.07s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 724/1773 [6:28:31<9:27:45, 32.47s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 725/1773 [6:29:02<9:23:07, 32.24s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 726/1773 [6:29:35<9:25:08, 32.39s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 727/1773 [6:30:07<9:20:58, 32.18s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 728/1773 [6:30:38<9:17:42, 32.02s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 729/1773 [6:31:10<9:16:32, 31.98s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 730/1773 [6:31:42<9:13:13, 31.83s/it]                                                      {'loss': 0.2929, 'learning_rate': 1.3291082639070915e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 730/1773 [6:31:42<9:13:13, 31.83s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 731/1773 [6:32:14<9:14:48, 31.95s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 732/1773 [6:32:46<9:12:22, 31.84s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 733/1773 [6:33:18<9:15:07, 32.03s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 734/1773 [6:33:50<9:12:31, 31.91s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 735/1773 [6:34:23<9:20:24, 32.39s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 736/1773 [6:34:55<9:16:41, 32.21s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 737/1773 [6:35:28<9:18:49, 32.36s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 738/1773 [6:36:00<9:15:16, 32.19s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 739/1773 [6:36:31<9:13:09, 32.10s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 740/1773 [6:37:03<9:11:19, 32.02s/it]                                                      {'loss': 0.2762, 'learning_rate': 1.3117966649144938e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 740/1773 [6:37:03<9:11:19, 32.02s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 741/1773 [6:37:35<9:09:24, 31.94s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 742/1773 [6:38:07<9:10:30, 32.04s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 743/1773 [6:38:39<9:07:22, 31.89s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 744/1773 [6:39:11<9:09:11, 32.02s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 745/1773 [6:39:43<9:06:41, 31.91s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 746/1773 [6:40:15<9:08:19, 32.03s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 747/1773 [6:40:47<9:04:54, 31.87s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 748/1773 [6:41:20<9:12:55, 32.37s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 749/1773 [6:41:52<9:08:26, 32.14s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 750/1773 [6:42:25<9:12:33, 32.41s/it]                                                      {'loss': 0.3127, 'learning_rate': 1.2943809283686373e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 750/1773 [6:42:25<9:12:33, 32.41s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 751/1773 [6:42:57<9:09:17, 32.25s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 752/1773 [6:43:29<9:09:01, 32.26s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 753/1773 [6:44:01<9:05:40, 32.10s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 754/1773 [6:44:33<9:04:07, 32.04s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 755/1773 [6:45:04<9:02:18, 31.96s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 756/1773 [6:45:36<8:59:22, 31.82s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 757/1773 [6:46:08<9:00:38, 31.93s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 758/1773 [6:46:39<8:57:49, 31.79s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 759/1773 [6:47:12<9:00:34, 31.99s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 760/1773 [6:47:43<8:58:04, 31.87s/it]                                                      {'loss': 0.2889, 'learning_rate': 1.2768668709840547e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 760/1773 [6:47:43<8:58:04, 31.87s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 761/1773 [6:48:17<9:06:15, 32.39s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 762/1773 [6:48:49<9:02:13, 32.18s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 763/1773 [6:49:22<9:06:53, 32.49s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 764/1773 [6:49:54<9:03:00, 32.29s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 765/1773 [6:50:26<9:03:05, 32.33s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 766/1773 [6:50:58<8:59:30, 32.15s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 767/1773 [6:51:30<8:57:08, 32.04s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 768/1773 [6:52:01<8:55:09, 31.95s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 769/1773 [6:52:33<8:52:55, 31.85s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 770/1773 [6:53:05<8:54:41, 31.99s/it]                                                      {'loss': 0.2999, 'learning_rate': 1.25926034231364e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 770/1773 [6:53:05<8:54:41, 31.99s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 771/1773 [6:53:37<8:51:45, 31.84s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 772/1773 [6:54:09<8:53:50, 32.00s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 773/1773 [6:54:41<8:50:29, 31.83s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 774/1773 [6:55:14<8:58:16, 32.33s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 775/1773 [6:55:46<8:54:56, 32.16s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 776/1773 [6:56:19<8:57:35, 32.35s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 777/1773 [6:56:51<8:54:54, 32.22s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 778/1773 [6:57:23<8:54:00, 32.20s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 779/1773 [6:57:55<8:51:29, 32.08s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 780/1773 [6:58:26<8:48:45, 31.95s/it]                                                      {'loss': 0.3037, 'learning_rate': 1.2415672227949467e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 780/1773 [6:58:26<8:48:45, 31.95s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 781/1773 [6:58:58<8:47:27, 31.90s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 782/1773 [6:59:30<8:45:07, 31.79s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 783/1773 [7:00:02<8:46:58, 31.94s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 784/1773 [7:00:33<8:43:59, 31.79s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 785/1773 [7:01:06<8:46:52, 32.00s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 786/1773 [7:01:37<8:43:45, 31.84s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 787/1773 [7:02:11<8:50:51, 32.30s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 788/1773 [7:02:42<8:47:24, 32.13s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 789/1773 [7:03:15<8:50:33, 32.35s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 790/1773 [7:03:47<8:47:18, 32.19s/it]                                                      {'loss': 0.2814, 'learning_rate': 1.2237934217861665e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 790/1773 [7:03:47<8:47:18, 32.19s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 791/1773 [7:04:19<8:46:57, 32.20s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 792/1773 [7:04:51<8:44:04, 32.05s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 793/1773 [7:05:23<8:41:06, 31.90s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 794/1773 [7:05:55<8:41:12, 31.94s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 795/1773 [7:06:26<8:38:48, 31.83s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 796/1773 [7:06:58<8:40:47, 31.98s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 797/1773 [7:07:30<8:38:04, 31.85s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 798/1773 [7:08:02<8:40:21, 32.02s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 799/1773 [7:08:34<8:37:23, 31.87s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 800/1773 [7:09:07<8:44:32, 32.35s/it]                                                      {'loss': 0.2961, 'learning_rate': 1.2059448755924542e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 800/1773 [7:09:07<8:44:32, 32.35s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 801/1773 [7:09:39<8:42:14, 32.24s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 802/1773 [7:10:12<8:45:17, 32.46s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 803/1773 [7:10:44<8:42:03, 32.29s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 804/1773 [7:11:17<8:42:50, 32.37s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 805/1773 [7:11:48<8:38:28, 32.14s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 806/1773 [7:12:20<8:36:14, 32.03s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 807/1773 [7:12:52<8:34:36, 31.96s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 808/1773 [7:13:24<8:31:55, 31.83s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 809/1773 [7:13:56<8:35:01, 32.06s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 810/1773 [7:14:28<8:31:22, 31.86s/it]                                                      {'loss': 0.2908, 'learning_rate': 1.1880275454832493e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 810/1773 [7:14:28<8:31:22, 31.86s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 811/1773 [7:15:00<8:32:39, 31.97s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 812/1773 [7:15:32<8:30:57, 31.90s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 813/1773 [7:16:05<8:35:45, 32.23s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 814/1773 [7:16:36<8:33:01, 32.10s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 815/1773 [7:17:10<8:39:13, 32.52s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 816/1773 [7:17:42<8:35:36, 32.33s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 817/1773 [7:18:14<8:37:19, 32.47s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 818/1773 [7:18:46<8:33:51, 32.28s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 819/1773 [7:19:18<8:31:40, 32.18s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 820/1773 [7:19:50<8:29:36, 32.08s/it]                                                      {'loss': 0.3014, 'learning_rate': 1.17004741570126e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 820/1773 [7:19:50<8:29:36, 32.08s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 821/1773 [7:20:22<8:27:27, 31.98s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 822/1773 [7:20:54<8:27:38, 32.03s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 823/1773 [7:21:25<8:24:27, 31.86s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 824/1773 [7:21:58<8:25:26, 31.96s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 825/1773 [7:22:29<8:23:03, 31.84s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 826/1773 [7:23:02<8:26:29, 32.09s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 827/1773 [7:23:34<8:23:42, 31.95s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 828/1773 [7:24:07<8:31:28, 32.47s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 829/1773 [7:24:39<8:28:13, 32.30s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 830/1773 [7:25:12<8:29:56, 32.45s/it]                                                      {'loss': 0.2892, 'learning_rate': 1.1520104914637753e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 830/1773 [7:25:12<8:29:56, 32.45s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 831/1773 [7:25:44<8:26:41, 32.27s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 832/1773 [7:26:16<8:26:00, 32.26s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 833/1773 [7:26:48<8:22:31, 32.08s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 834/1773 [7:27:19<8:20:25, 31.98s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 835/1773 [7:27:51<8:19:16, 31.94s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 836/1773 [7:28:23<8:17:03, 31.83s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 837/1773 [7:28:55<8:19:00, 31.99s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 838/1773 [7:29:27<8:15:40, 31.81s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 839/1773 [7:29:59<8:17:10, 31.94s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 840/1773 [7:30:30<8:14:22, 31.79s/it]                                                      {'loss': 0.29, 'learning_rate': 1.1339227969569711e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 840/1773 [7:30:30<8:14:22, 31.79s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 841/1773 [7:31:04<8:20:56, 32.25s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 842/1773 [7:31:36<8:19:04, 32.16s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 843/1773 [7:32:09<8:23:03, 32.46s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 844/1773 [7:32:41<8:19:39, 32.27s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 845/1773 [7:33:13<8:20:39, 32.37s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 846/1773 [7:33:45<8:18:14, 32.25s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 847/1773 [7:34:17<8:14:33, 32.04s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 848/1773 [7:34:48<8:12:29, 31.94s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 849/1773 [7:35:20<8:09:59, 31.82s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 850/1773 [7:35:52<8:11:12, 31.93s/it]                                                      {'loss': 0.2857, 'learning_rate': 1.1157903733238803e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 850/1773 [7:35:52<8:11:12, 31.93s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 851/1773 [7:36:24<8:09:04, 31.83s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 852/1773 [7:36:56<8:11:05, 31.99s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 853/1773 [7:37:28<8:08:48, 31.88s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 854/1773 [7:38:01<8:14:53, 32.31s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 855/1773 [7:38:33<8:12:29, 32.19s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 856/1773 [7:39:06<8:16:17, 32.47s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 857/1773 [7:39:38<8:12:45, 32.28s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 858/1773 [7:40:10<8:12:16, 32.28s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 859/1773 [7:40:42<8:09:04, 32.11s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 860/1773 [7:41:13<8:06:27, 31.97s/it]                                                      {'loss': 0.2993, 'learning_rate': 1.0976192766467008e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 860/1773 [7:41:13<8:06:27, 31.97s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 861/1773 [7:41:45<8:06:00, 31.97s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 862/1773 [7:42:17<8:03:32, 31.85s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 863/1773 [7:42:49<8:05:13, 31.99s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 864/1773 [7:43:21<8:02:41, 31.86s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 865/1773 [7:43:53<8:04:26, 32.01s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 866/1773 [7:44:25<8:02:20, 31.91s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 867/1773 [7:44:58<8:09:16, 32.40s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 868/1773 [7:45:30<8:05:30, 32.19s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 869/1773 [7:46:03<8:09:43, 32.50s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 870/1773 [7:46:35<8:06:29, 32.33s/it]                                                      {'loss': 0.2968, 'learning_rate': 1.0794155759241138e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 870/1773 [7:46:35<8:06:29, 32.33s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 871/1773 [7:47:08<8:08:36, 32.50s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 872/1773 [7:47:40<8:05:03, 32.30s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 873/1773 [7:48:12<8:04:06, 32.27s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 874/1773 [7:48:44<8:00:58, 32.10s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 875/1773 [7:49:16<7:58:16, 31.96s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 876/1773 [7:49:48<7:58:25, 32.00s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 877/1773 [7:50:19<7:55:45, 31.86s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 878/1773 [7:50:52<7:57:22, 32.00s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 879/1773 [7:51:23<7:55:26, 31.91s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 880/1773 [7:51:56<7:58:16, 32.13s/it]                                                      {'loss': 0.2991, 'learning_rate': 1.0611853510442858e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 880/1773 [7:51:56<7:58:16, 32.13s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 881/1773 [7:52:28<7:55:59, 32.02s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 882/1773 [7:53:01<8:02:59, 32.53s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 883/1773 [7:53:33<8:00:27, 32.39s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 884/1773 [7:54:06<8:01:56, 32.53s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 885/1773 [7:54:38<7:57:28, 32.26s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 886/1773 [7:55:11<7:58:21, 32.36s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 887/1773 [7:55:42<7:54:20, 32.12s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 888/1773 [7:56:14<7:51:52, 31.99s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 889/1773 [7:56:46<7:50:39, 31.95s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 890/1773 [7:57:17<7:48:04, 31.81s/it]                                                      {'loss': 0.293, 'learning_rate': 1.0429346907542368e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 890/1773 [7:57:17<7:48:04, 31.81s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 891/1773 [7:57:49<7:48:21, 31.86s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 892/1773 [7:58:20<7:45:48, 31.72s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 893/1773 [7:58:52<7:45:58, 31.77s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 894/1773 [7:59:24<7:43:47, 31.66s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 895/1773 [7:59:56<7:46:42, 31.89s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 896/1773 [8:00:28<7:44:57, 31.81s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 897/1773 [8:01:01<7:51:15, 32.28s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 898/1773 [8:01:33<7:48:32, 32.13s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 899/1773 [8:02:06<7:51:34, 32.37s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 900/1773 [8:02:38<7:48:50, 32.22s/it]                                                      {'loss': 0.2839, 'learning_rate': 1.0246696906262484e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 900/1773 [8:02:38<7:48:50, 32.22s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 901/1773 [8:03:10<7:49:55, 32.33s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 902/1773 [8:03:42<7:46:45, 32.15s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 903/1773 [8:04:14<7:44:58, 32.07s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 904/1773 [8:04:46<7:42:33, 31.94s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 905/1773 [8:05:17<7:41:04, 31.87s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 906/1773 [8:05:49<7:40:46, 31.89s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 907/1773 [8:06:20<7:36:47, 31.65s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 908/1773 [8:06:53<7:38:34, 31.81s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 909/1773 [8:07:24<7:36:34, 31.71s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 910/1773 [8:07:56<7:38:35, 31.88s/it]                                                      {'loss': 0.2974, 'learning_rate': 1.00639645102199e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 910/1773 [8:07:56<7:38:35, 31.88s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 911/1773 [8:08:28<7:36:47, 31.80s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 912/1773 [8:09:01<7:42:50, 32.25s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 913/1773 [8:09:33<7:39:43, 32.07s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 914/1773 [8:10:06<7:42:53, 32.33s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 915/1773 [8:10:37<7:39:20, 32.12s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 916/1773 [8:11:10<7:41:10, 32.29s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 917/1773 [8:11:42<7:38:02, 32.11s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 918/1773 [8:12:13<7:35:42, 31.98s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 919/1773 [8:12:45<7:33:35, 31.87s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 920/1773 [8:13:17<7:31:21, 31.75s/it]                                                      {'loss': 0.2844, 'learning_rate': 9.88121075055051e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 920/1773 [8:13:17<7:31:21, 31.75s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 921/1773 [8:13:49<7:32:14, 31.85s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 922/1773 [8:14:20<7:30:02, 31.73s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 923/1773 [8:14:52<7:31:08, 31.85s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 924/1773 [8:15:24<7:28:53, 31.72s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 925/1773 [8:15:56<7:33:06, 32.06s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 926/1773 [8:16:28<7:30:48, 31.93s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 927/1773 [8:17:02<7:36:35, 32.38s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 928/1773 [8:17:33<7:32:48, 32.15s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 929/1773 [8:18:06<7:35:05, 32.35s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 930/1773 [8:18:38<7:31:56, 32.17s/it]                                                      {'loss': 0.2781, 'learning_rate': 9.69849666552547e-06, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 930/1773 [8:18:38<7:31:56, 32.17s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 931/1773 [8:19:10<7:32:06, 32.22s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 932/1773 [8:19:42<7:28:22, 31.99s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 933/1773 [8:20:13<7:26:35, 31.90s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 934/1773 [8:20:45<7:25:51, 31.88s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 935/1773 [8:21:16<7:23:19, 31.74s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 936/1773 [8:21:49<7:24:06, 31.84s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 937/1773 [8:22:20<7:22:01, 31.72s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 938/1773 [8:22:53<7:26:20, 32.07s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 939/1773 [8:23:25<7:24:19, 31.97s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 940/1773 [8:23:58<7:30:00, 32.41s/it]                                                      {'loss': 0.2753, 'learning_rate': 9.515883280164941e-06, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 940/1773 [8:23:58<7:30:00, 32.41s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 941/1773 [8:24:30<7:26:54, 32.23s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 942/1773 [8:25:03<7:28:55, 32.41s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 943/1773 [8:25:34<7:25:39, 32.22s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 944/1773 [8:26:07<7:25:01, 32.21s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 945/1773 [8:26:38<7:21:45, 32.01s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 946/1773 [8:27:10<7:19:05, 31.86s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 947/1773 [8:27:41<7:16:55, 31.74s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 948/1773 [8:28:12<7:13:40, 31.54s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 949/1773 [8:28:44<7:14:14, 31.62s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 950/1773 [8:29:15<7:12:28, 31.53s/it]                                                      {'loss': 0.2899, 'learning_rate': 9.333431585856204e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 950/1773 [8:29:15<7:12:28, 31.53s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 951/1773 [8:29:48<7:16:25, 31.86s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 952/1773 [8:30:20<7:15:20, 31.82s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 953/1773 [8:30:53<7:20:03, 32.20s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 954/1773 [8:31:24<7:16:39, 31.99s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 955/1773 [8:31:57<7:18:43, 32.18s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 956/1773 [8:32:29<7:16:01, 32.02s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 957/1773 [8:33:00<7:14:34, 31.95s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 958/1773 [8:33:32<7:12:44, 31.86s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 959/1773 [8:34:03<7:10:41, 31.75s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 960/1773 [8:34:35<7:10:07, 31.74s/it]                                                      {'loss': 0.2689, 'learning_rate': 9.151202519983045e-06, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 960/1773 [8:34:35<7:10:07, 31.74s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 961/1773 [8:35:06<7:07:47, 31.61s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 962/1773 [8:35:38<7:08:17, 31.69s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 963/1773 [8:36:10<7:05:56, 31.55s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 964/1773 [8:36:42<7:10:49, 31.95s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 965/1773 [8:37:14<7:09:20, 31.88s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 966/1773 [8:37:47<7:12:52, 32.18s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 967/1773 [8:38:19<7:09:46, 31.99s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 968/1773 [8:38:51<7:09:02, 31.98s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 969/1773 [8:39:22<7:06:54, 31.86s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 970/1773 [8:39:54<7:05:08, 31.77s/it]                                                      {'loss': 0.2732, 'learning_rate': 8.969256945573157e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 970/1773 [8:39:54<7:05:08, 31.77s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 971/1773 [8:40:25<7:04:12, 31.74s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 972/1773 [8:40:57<7:01:32, 31.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 973/1773 [8:41:28<7:02:08, 31.66s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 974/1773 [8:42:00<7:00:23, 31.57s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 975/1773 [8:42:32<7:03:17, 31.83s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 976/1773 [8:43:04<7:01:30, 31.73s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 977/1773 [8:43:37<7:06:03, 32.12s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 978/1773 [8:44:09<7:04:12, 32.02s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 979/1773 [8:44:41<7:05:56, 32.19s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 980/1773 [8:45:13<7:02:37, 31.98s/it]                                                      {'loss': 0.2926, 'learning_rate': 8.787655630970403e-06, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 980/1773 [8:45:13<7:02:37, 31.98s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 981/1773 [8:45:45<7:02:31, 32.01s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 982/1773 [8:46:16<7:00:01, 31.86s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 983/1773 [8:46:48<6:58:06, 31.75s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 984/1773 [8:47:19<6:57:27, 31.75s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 985/1773 [8:47:51<6:55:16, 31.62s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 986/1773 [8:48:23<6:55:48, 31.70s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 987/1773 [8:48:54<6:54:41, 31.66s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 988/1773 [8:49:27<6:57:12, 31.89s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 989/1773 [8:49:58<6:55:11, 31.77s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 990/1773 [8:50:31<7:00:08, 32.19s/it]                                                      {'loss': 0.2749, 'learning_rate': 8.606459229538645e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 990/1773 [8:50:31<7:00:08, 32.19s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 991/1773 [8:51:03<6:57:11, 32.01s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 992/1773 [8:51:35<6:58:59, 32.19s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 993/1773 [8:52:07<6:55:50, 31.99s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 994/1773 [8:52:39<6:54:57, 31.96s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 995/1773 [8:53:10<6:52:51, 31.84s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 996/1773 [8:53:42<6:50:41, 31.71s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 997/1773 [8:54:14<6:50:08, 31.71s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 998/1773 [8:54:45<6:47:54, 31.58s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 999/1773 [8:55:17<6:48:48, 31.69s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1000/1773 [8:55:48<6:47:05, 31.60s/it]                                                       {'loss': 0.2927, 'learning_rate': 8.425728259404053e-06, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1000/1773 [8:55:48<6:47:05, 31.60s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1001/1773 [8:56:21<6:49:41, 31.84s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1002/1773 [8:56:52<6:47:42, 31.73s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1003/1773 [8:57:25<6:52:50, 32.17s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1004/1773 [8:57:57<6:50:54, 32.06s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1005/1773 [8:58:30<6:52:53, 32.26s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1006/1773 [8:59:01<6:50:01, 32.07s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1007/1773 [8:59:34<6:50:29, 32.15s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1008/1773 [9:00:05<6:47:28, 31.96s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1009/1773 [9:00:37<6:45:48, 31.87s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1010/1773 [9:01:09<6:44:11, 31.78s/it]                                                       {'loss': 0.2822, 'learning_rate': 8.24552308324251e-06, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1010/1773 [9:01:09<6:44:11, 31.78s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1011/1773 [9:01:40<6:41:23, 31.61s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1012/1773 [9:02:12<6:42:53, 31.77s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1013/1773 [9:02:43<6:40:50, 31.65s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1014/1773 [9:03:15<6:41:32, 31.74s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1015/1773 [9:03:47<6:40:19, 31.69s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1016/1773 [9:04:19<6:43:45, 32.00s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1017/1773 [9:04:51<6:42:04, 31.91s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1018/1773 [9:05:24<6:46:00, 32.27s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1019/1773 [9:05:56<6:42:35, 32.04s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1020/1773 [9:06:28<6:43:38, 32.16s/it]                                                       {'loss': 0.298, 'learning_rate': 8.065903888119e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1020/1773 [9:06:28<6:43:38, 32.16s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1021/1773 [9:07:00<6:40:47, 31.98s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1022/1773 [9:07:31<6:39:02, 31.88s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1023/1773 [9:08:03<6:37:06, 31.77s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1024/1773 [9:08:34<6:35:35, 31.69s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1025/1773 [9:09:07<6:36:38, 31.82s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1026/1773 [9:09:38<6:34:09, 31.66s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1027/1773 [9:10:10<6:35:06, 31.78s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1028/1773 [9:10:41<6:33:12, 31.67s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1029/1773 [9:11:14<6:37:44, 32.08s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1030/1773 [9:11:46<6:35:33, 31.94s/it]                                                       {'loss': 0.2778, 'learning_rate': 7.886930665385579e-06, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1030/1773 [9:11:46<6:35:33, 31.94s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1031/1773 [9:12:19<6:37:35, 32.15s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1032/1773 [9:12:50<6:35:17, 32.01s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1033/1773 [9:13:22<6:35:21, 32.06s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1034/1773 [9:13:54<6:33:21, 31.94s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1035/1773 [9:14:25<6:30:30, 31.75s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1036/1773 [9:14:57<6:29:27, 31.71s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1037/1773 [9:15:28<6:27:00, 31.55s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1038/1773 [9:16:00<6:27:47, 31.66s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1039/1773 [9:16:31<6:26:12, 31.57s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1040/1773 [9:17:04<6:28:57, 31.84s/it]                                                       {'loss': 0.2695, 'learning_rate': 7.708663190644794e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1040/1773 [9:17:04<6:28:57, 31.84s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1041/1773 [9:17:36<6:27:34, 31.77s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1042/1773 [9:18:09<6:31:59, 32.17s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1043/1773 [9:18:40<6:29:29, 32.01s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1044/1773 [9:19:13<6:31:28, 32.22s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1045/1773 [9:19:45<6:28:31, 32.02s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1046/1773 [9:20:16<6:26:42, 31.92s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1047/1773 [9:20:48<6:25:01, 31.82s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1048/1773 [9:21:19<6:23:48, 31.76s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1049/1773 [9:21:51<6:23:42, 31.80s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1050/1773 [9:22:23<6:20:58, 31.62s/it]                                                       {'loss': 0.2625, 'learning_rate': 7.53116100378508e-06, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1050/1773 [9:22:23<6:20:58, 31.62s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1051/1773 [9:22:55<6:22:22, 31.78s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1052/1773 [9:23:26<6:20:58, 31.70s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1053/1773 [9:23:59<6:25:34, 32.13s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1054/1773 [9:24:31<6:23:48, 32.03s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1055/1773 [9:25:04<6:25:38, 32.23s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1056/1773 [9:25:36<6:23:08, 32.06s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1057/1773 [9:26:08<6:24:37, 32.23s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1058/1773 [9:26:40<6:22:00, 32.06s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1059/1773 [9:27:12<6:20:16, 31.96s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1060/1773 [9:27:43<6:18:50, 31.88s/it]                                                       {'loss': 0.2855, 'learning_rate': 7.354483389094947e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1060/1773 [9:27:43<6:18:50, 31.88s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1061/1773 [9:28:15<6:17:08, 31.78s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1062/1773 [9:28:47<6:16:47, 31.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1063/1773 [9:29:18<6:14:12, 31.62s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1064/1773 [9:29:50<6:14:14, 31.67s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1065/1773 [9:30:21<6:12:39, 31.58s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1066/1773 [9:30:54<6:16:22, 31.94s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1067/1773 [9:31:25<6:14:26, 31.82s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1068/1773 [9:31:58<6:17:56, 32.16s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1069/1773 [9:32:30<6:15:12, 31.98s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1070/1773 [9:33:02<6:15:35, 32.06s/it]                                                       {'loss': 0.2706, 'learning_rate': 7.178689355462487e-06, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1070/1773 [9:33:02<6:15:35, 32.06s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1071/1773 [9:33:34<6:13:15, 31.90s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1072/1773 [9:34:05<6:11:40, 31.81s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1073/1773 [9:34:37<6:10:25, 31.75s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1074/1773 [9:35:08<6:08:30, 31.63s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1075/1773 [9:35:40<6:08:35, 31.68s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1076/1773 [9:36:11<6:06:58, 31.59s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1077/1773 [9:36:43<6:07:29, 31.68s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1078/1773 [9:37:15<6:05:45, 31.58s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1079/1773 [9:37:47<6:09:55, 31.98s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1080/1773 [9:38:19<6:08:46, 31.93s/it]                                                       {'loss': 0.2749, 'learning_rate': 7.003837616666906e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1080/1773 [9:38:19<6:08:46, 31.93s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1081/1773 [9:38:52<6:10:55, 32.16s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1082/1773 [9:39:24<6:08:11, 31.97s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1083/1773 [9:39:56<6:08:05, 32.01s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1084/1773 [9:40:27<6:05:27, 31.82s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1085/1773 [9:40:58<6:03:36, 31.71s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1086/1773 [9:41:30<6:03:12, 31.72s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1087/1773 [9:42:02<6:01:25, 31.61s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1088/1773 [9:42:33<6:01:46, 31.69s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1089/1773 [9:43:05<6:00:42, 31.64s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1090/1773 [9:43:37<6:02:38, 31.86s/it]                                                       {'loss': 0.2624, 'learning_rate': 6.829986571768585e-06, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1090/1773 [9:43:37<6:02:38, 31.86s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1091/1773 [9:44:09<6:01:11, 31.78s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1092/1773 [9:44:42<6:05:03, 32.16s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1093/1773 [9:45:14<6:02:34, 31.99s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1094/1773 [9:45:46<6:03:55, 32.16s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1095/1773 [9:46:18<6:02:07, 32.05s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1096/1773 [9:46:50<6:01:50, 32.07s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1097/1773 [9:47:22<5:59:46, 31.93s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1098/1773 [9:47:53<5:58:06, 31.83s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1099/1773 [9:48:25<5:57:11, 31.80s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1100/1773 [9:48:56<5:55:23, 31.68s/it]                                                       {'loss': 0.2872, 'learning_rate': 6.657194285604276e-06, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1100/1773 [9:48:56<5:55:23, 31.68s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1101/1773 [9:49:28<5:56:10, 31.80s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1102/1773 [9:50:00<5:53:56, 31.65s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1103/1773 [9:50:32<5:54:23, 31.74s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1104/1773 [9:51:03<5:52:49, 31.64s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1105/1773 [9:51:36<5:56:10, 31.99s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1106/1773 [9:52:08<5:54:49, 31.92s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1107/1773 [9:52:40<5:57:14, 32.18s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1108/1773 [9:53:12<5:54:54, 32.02s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1109/1773 [9:53:45<5:56:12, 32.19s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1110/1773 [9:54:16<5:53:15, 31.97s/it]                                                       {'loss': 0.2695, 'learning_rate': 6.4855184693939076e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1110/1773 [9:54:16<5:53:15, 31.97s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1111/1773 [9:54:48<5:51:33, 31.86s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1112/1773 [9:55:19<5:49:49, 31.75s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1113/1773 [9:55:51<5:47:59, 31.64s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1114/1773 [9:56:23<5:48:20, 31.72s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1115/1773 [9:56:54<5:46:42, 31.61s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1116/1773 [9:57:26<5:46:46, 31.67s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1117/1773 [9:57:57<5:45:28, 31.60s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1118/1773 [9:58:30<5:49:06, 31.98s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1119/1773 [9:59:02<5:48:06, 31.94s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1120/1773 [9:59:35<5:50:22, 32.19s/it]                                                       {'loss': 0.2756, 'learning_rate': 6.315016461465522e-06, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1120/1773 [9:59:35<5:50:22, 32.19s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1121/1773 [10:00:06<5:47:32, 31.98s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1122/1773 [10:00:38<5:48:03, 32.08s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1123/1773 [10:01:10<5:46:11, 31.96s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1124/1773 [10:01:42<5:44:18, 31.83s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1125/1773 [10:02:13<5:43:16, 31.78s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1126/1773 [10:02:45<5:41:09, 31.64s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1127/1773 [10:03:17<5:41:31, 31.72s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1128/1773 [10:03:48<5:39:34, 31.59s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1129/1773 [10:04:20<5:40:21, 31.71s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1130/1773 [10:04:51<5:38:43, 31.61s/it]                                                        {'loss': 0.2765, 'learning_rate': 6.145745208104721e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1130/1773 [10:04:51<5:38:43, 31.61s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1131/1773 [10:05:24<5:43:44, 32.13s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1132/1773 [10:05:56<5:41:14, 31.94s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1133/1773 [10:06:29<5:43:32, 32.21s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1134/1773 [10:07:01<5:41:29, 32.06s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1135/1773 [10:07:33<5:40:37, 32.03s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1136/1773 [10:08:04<5:38:24, 31.88s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1137/1773 [10:08:36<5:36:53, 31.78s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1138/1773 [10:09:07<5:35:57, 31.74s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1139/1773 [10:09:38<5:33:38, 31.57s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1140/1773 [10:10:10<5:34:38, 31.72s/it]                                                        {'loss': 0.2854, 'learning_rate': 5.977761244535088e-06, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1140/1773 [10:10:10<5:34:38, 31.72s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1141/1773 [10:10:42<5:33:22, 31.65s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1142/1773 [10:11:14<5:35:32, 31.91s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1143/1773 [10:11:46<5:34:22, 31.85s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1144/1773 [10:12:19<5:36:57, 32.14s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1145/1773 [10:12:51<5:34:51, 31.99s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1146/1773 [10:13:23<5:36:22, 32.19s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1147/1773 [10:13:55<5:34:14, 32.04s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1148/1773 [10:14:27<5:34:06, 32.07s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1149/1773 [10:14:59<5:31:42, 31.90s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1150/1773 [10:15:30<5:30:08, 31.80s/it]                                                        {'loss': 0.266, 'learning_rate': 5.811120676035859e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1150/1773 [10:15:30<5:30:08, 31.80s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1151/1773 [10:16:02<5:28:57, 31.73s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1152/1773 [10:16:33<5:27:21, 31.63s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1153/1773 [10:17:05<5:28:06, 31.75s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1154/1773 [10:17:37<5:26:42, 31.67s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1155/1773 [10:18:09<5:27:16, 31.77s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1156/1773 [10:18:40<5:25:32, 31.66s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1157/1773 [10:19:13<5:29:13, 32.07s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1158/1773 [10:19:45<5:27:27, 31.95s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1159/1773 [10:20:17<5:29:04, 32.16s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1160/1773 [10:20:49<5:26:20, 31.94s/it]                                                        {'loss': 0.2813, 'learning_rate': 5.645879159203251e-06, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1160/1773 [10:20:49<5:26:20, 31.94s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1161/1773 [10:21:21<5:25:07, 31.88s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1162/1773 [10:21:52<5:23:52, 31.80s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1163/1773 [10:22:24<5:23:40, 31.84s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1164/1773 [10:22:56<5:23:35, 31.88s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1165/1773 [10:23:27<5:21:31, 31.73s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1166/1773 [10:24:00<5:22:32, 31.88s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1167/1773 [10:24:31<5:21:05, 31.79s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1168/1773 [10:25:04<5:24:10, 32.15s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1169/1773 [10:25:36<5:22:12, 32.01s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1170/1773 [10:26:09<5:25:56, 32.43s/it]                                                        {'loss': 0.2545, 'learning_rate': 5.482091883361571e-06, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1170/1773 [10:26:09<5:25:56, 32.43s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1171/1773 [10:26:41<5:23:55, 32.28s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1172/1773 [10:27:15<5:26:21, 32.58s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1173/1773 [10:27:46<5:23:03, 32.31s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1174/1773 [10:28:19<5:23:04, 32.36s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1175/1773 [10:28:51<5:21:17, 32.24s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1176/1773 [10:29:23<5:20:02, 32.16s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1177/1773 [10:29:54<5:18:09, 32.03s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1178/1773 [10:30:26<5:16:35, 31.93s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1179/1773 [10:30:58<5:16:08, 31.93s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1180/1773 [10:31:29<5:13:49, 31.75s/it]                                                        {'loss': 0.2637, 'learning_rate': 5.319813552130456e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1180/1773 [10:31:29<5:13:49, 31.75s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1181/1773 [10:32:01<5:13:32, 31.78s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1182/1773 [10:32:33<5:11:57, 31.67s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1183/1773 [10:33:05<5:14:12, 31.95s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1184/1773 [10:33:38<5:15:36, 32.15s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1185/1773 [10:34:12<5:21:21, 32.79s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1186/1773 [10:34:44<5:18:39, 32.57s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1187/1773 [10:35:18<5:20:50, 32.85s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1188/1773 [10:35:51<5:21:03, 32.93s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1189/1773 [10:36:24<5:21:36, 33.04s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1190/1773 [10:36:58<5:22:32, 33.20s/it]                                                        {'loss': 0.2642, 'learning_rate': 5.159098365154294e-06, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1190/1773 [10:36:58<5:22:32, 33.20s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1191/1773 [10:37:31<5:22:38, 33.26s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1192/1773 [10:38:05<5:25:03, 33.57s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1193/1773 [10:38:38<5:21:56, 33.30s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1194/1773 [10:39:12<5:22:05, 33.38s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1195/1773 [10:39:43<5:16:49, 32.89s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1196/1773 [10:40:16<5:15:34, 32.81s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1197/1773 [10:40:48<5:13:04, 32.61s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1198/1773 [10:41:23<5:19:15, 33.31s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1199/1773 [10:41:55<5:14:21, 32.86s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1200/1773 [10:42:27<5:12:50, 32.76s/it]                                                        {'loss': 0.2559, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1200/1773 [10:42:27<5:12:50, 32.76s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1201/1773 [10:42:59<5:08:38, 32.37s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1202/1773 [10:43:31<5:07:38, 32.33s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1203/1773 [10:44:04<5:08:38, 32.49s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1204/1773 [10:44:37<5:10:28, 32.74s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1205/1773 [10:45:09<5:08:08, 32.55s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1206/1773 [10:45:42<5:07:06, 32.50s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1207/1773 [10:46:13<5:03:18, 32.15s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1208/1773 [10:46:45<5:02:58, 32.17s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1209/1773 [10:47:18<5:02:34, 32.19s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1210/1773 [10:47:50<5:01:54, 32.18s/it]                                                        {'loss': 0.2651, 'learning_rate': 4.842571594229129e-06, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1210/1773 [10:47:50<5:01:54, 32.18s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1211/1773 [10:48:21<4:59:09, 31.94s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1212/1773 [10:48:53<4:58:39, 31.94s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1213/1773 [10:49:26<5:00:03, 32.15s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1214/1773 [10:49:58<5:00:33, 32.26s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1215/1773 [10:50:30<4:58:07, 32.06s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1216/1773 [10:51:03<4:59:56, 32.31s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1217/1773 [10:51:34<4:57:25, 32.10s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1218/1773 [10:52:07<4:58:59, 32.32s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1219/1773 [10:52:39<4:56:03, 32.06s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1220/1773 [10:53:10<4:54:33, 31.96s/it]                                                        {'loss': 0.2611, 'learning_rate': 4.6868657276503695e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1220/1773 [10:53:10<4:54:33, 31.96s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1221/1773 [10:53:42<4:53:48, 31.94s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1222/1773 [10:54:14<4:53:08, 31.92s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1223/1773 [10:54:46<4:52:22, 31.90s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1224/1773 [10:55:18<4:51:19, 31.84s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1225/1773 [10:55:50<4:51:13, 31.89s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1226/1773 [10:56:21<4:50:02, 31.81s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1227/1773 [10:56:54<4:51:16, 32.01s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1228/1773 [10:57:25<4:49:13, 31.84s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1229/1773 [10:57:58<4:51:40, 32.17s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1230/1773 [10:58:30<4:49:36, 32.00s/it]                                                        {'loss': 0.2695, 'learning_rate': 4.532934404758308e-06, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1230/1773 [10:58:30<4:49:36, 32.00s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1231/1773 [10:59:03<4:51:32, 32.27s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1232/1773 [10:59:35<4:50:54, 32.26s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1233/1773 [11:00:08<4:53:30, 32.61s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1234/1773 [11:00:40<4:50:45, 32.37s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1235/1773 [11:01:13<4:51:59, 32.56s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1236/1773 [11:01:45<4:50:09, 32.42s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1237/1773 [11:02:17<4:49:05, 32.36s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1238/1773 [11:02:49<4:47:00, 32.19s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1239/1773 [11:03:21<4:46:14, 32.16s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1240/1773 [11:03:53<4:44:12, 31.99s/it]                                                        {'loss': 0.2793, 'learning_rate': 4.380829037364358e-06, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1240/1773 [11:03:53<4:44:12, 31.99s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1241/1773 [11:04:24<4:42:18, 31.84s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1242/1773 [11:04:56<4:40:47, 31.73s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1243/1773 [11:05:27<4:38:31, 31.53s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1244/1773 [11:05:59<4:40:09, 31.78s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1245/1773 [11:06:31<4:39:20, 31.74s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1246/1773 [11:07:03<4:39:45, 31.85s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1247/1773 [11:07:35<4:38:33, 31.78s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1248/1773 [11:08:07<4:39:52, 31.99s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1249/1773 [11:08:40<4:41:27, 32.23s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1250/1773 [11:09:14<4:44:40, 32.66s/it]                                                        {'loss': 0.2721, 'learning_rate': 4.230600427425626e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1250/1773 [11:09:14<4:44:40, 32.66s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1251/1773 [11:09:46<4:43:04, 32.54s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1252/1773 [11:10:19<4:43:25, 32.64s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1253/1773 [11:10:51<4:41:05, 32.43s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1254/1773 [11:11:23<4:40:57, 32.48s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1255/1773 [11:11:55<4:37:46, 32.17s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1256/1773 [11:12:27<4:38:37, 32.33s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1257/1773 [11:12:59<4:35:41, 32.06s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1258/1773 [11:13:31<4:36:10, 32.18s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1259/1773 [11:14:03<4:34:01, 31.99s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1260/1773 [11:14:35<4:33:15, 31.96s/it]                                                        {'loss': 0.2593, 'learning_rate': 4.082298750077485e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1260/1773 [11:14:35<4:33:15, 31.96s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1261/1773 [11:15:06<4:31:23, 31.80s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1262/1773 [11:15:38<4:29:56, 31.70s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1263/1773 [11:16:09<4:29:27, 31.70s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1264/1773 [11:16:41<4:27:48, 31.57s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1265/1773 [11:17:12<4:27:57, 31.65s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1266/1773 [11:17:44<4:26:35, 31.55s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1267/1773 [11:18:16<4:27:08, 31.68s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1268/1773 [11:18:47<4:25:40, 31.57s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1269/1773 [11:19:20<4:29:12, 32.05s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1270/1773 [11:19:52<4:27:19, 31.89s/it]                                                        {'loss': 0.266, 'learning_rate': 3.935973536875459e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1270/1773 [11:19:52<4:27:19, 31.89s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1271/1773 [11:20:24<4:28:17, 32.07s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1272/1773 [11:20:56<4:26:09, 31.87s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1273/1773 [11:21:28<4:25:48, 31.90s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1274/1773 [11:21:59<4:23:50, 31.72s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1275/1773 [11:22:30<4:22:05, 31.58s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1276/1773 [11:23:02<4:21:17, 31.54s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1277/1773 [11:23:33<4:19:41, 31.41s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1278/1773 [11:24:05<4:20:19, 31.56s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1279/1773 [11:24:36<4:18:58, 31.46s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1280/1773 [11:25:08<4:19:44, 31.61s/it]                                                        {'loss': 0.2691, 'learning_rate': 3.791673659252121e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1280/1773 [11:25:08<4:19:44, 31.61s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1281/1773 [11:25:39<4:18:59, 31.58s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1282/1773 [11:26:12<4:21:11, 31.92s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1283/1773 [11:26:44<4:19:57, 31.83s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1284/1773 [11:27:16<4:21:19, 32.06s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1285/1773 [11:27:48<4:19:23, 31.89s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1286/1773 [11:28:20<4:19:35, 31.98s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1287/1773 [11:28:52<4:18:20, 31.89s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1288/1773 [11:29:23<4:17:41, 31.88s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1289/1773 [11:29:55<4:16:15, 31.77s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1290/1773 [11:30:27<4:15:16, 31.71s/it]                                                        {'loss': 0.2826, 'learning_rate': 3.6494473121943973e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1290/1773 [11:30:27<4:15:16, 31.71s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1291/1773 [11:30:58<4:14:23, 31.67s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1292/1773 [11:31:29<4:13:06, 31.57s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1293/1773 [11:32:01<4:13:00, 31.63s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1294/1773 [11:32:32<4:11:24, 31.49s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1295/1773 [11:33:04<4:11:23, 31.55s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1296/1773 [11:33:35<4:10:07, 31.46s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1297/1773 [11:34:08<4:12:45, 31.86s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1298/1773 [11:34:40<4:11:32, 31.77s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1299/1773 [11:35:12<4:13:02, 32.03s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1300/1773 [11:35:44<4:11:41, 31.93s/it]                                                        {'loss': 0.2584, 'learning_rate': 3.509341998146859e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1300/1773 [11:35:44<4:11:41, 31.93s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1301/1773 [11:36:16<4:12:17, 32.07s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1302/1773 [11:36:48<4:10:01, 31.85s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1303/1773 [11:37:19<4:08:28, 31.72s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1304/1773 [11:37:51<4:07:10, 31.62s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1305/1773 [11:38:22<4:06:03, 31.55s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1306/1773 [11:38:54<4:06:33, 31.68s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1307/1773 [11:39:25<4:04:56, 31.54s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1308/1773 [11:39:57<4:04:56, 31.61s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1309/1773 [11:40:28<4:03:40, 31.51s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1310/1773 [11:41:00<4:03:34, 31.56s/it]                                                        {'loss': 0.2734, 'learning_rate': 3.3714045111462724e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1310/1773 [11:41:00<4:03:34, 31.56s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1311/1773 [11:41:31<4:02:15, 31.46s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1312/1773 [11:42:04<4:04:34, 31.83s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1313/1773 [11:42:35<4:03:19, 31.74s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1314/1773 [11:43:08<4:04:35, 31.97s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1315/1773 [11:43:39<4:02:41, 31.79s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1316/1773 [11:44:11<4:02:38, 31.86s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1317/1773 [11:44:43<4:00:59, 31.71s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1318/1773 [11:45:14<3:59:32, 31.59s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1319/1773 [11:45:45<3:58:41, 31.54s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1320/1773 [11:46:18<3:59:51, 31.77s/it]                                                        {'loss': 0.2706, 'learning_rate': 3.235680921192793e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1320/1773 [11:46:18<3:59:51, 31.77s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1321/1773 [11:46:49<3:59:24, 31.78s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1322/1773 [11:47:21<3:59:13, 31.83s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1323/1773 [11:47:54<4:00:44, 32.10s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1324/1773 [11:48:26<3:59:12, 31.97s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1325/1773 [11:48:59<4:01:17, 32.32s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1326/1773 [11:49:31<4:00:07, 32.23s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1327/1773 [11:50:03<3:59:23, 32.21s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1328/1773 [11:50:36<3:59:48, 32.33s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1329/1773 [11:51:08<3:59:37, 32.38s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1330/1773 [11:51:40<3:57:14, 32.13s/it]                                                        {'loss': 0.2703, 'learning_rate': 3.1022165588629337e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1330/1773 [11:51:40<3:57:14, 32.13s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1331/1773 [11:52:12<3:57:45, 32.27s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1332/1773 [11:52:44<3:56:08, 32.13s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1333/1773 [11:53:17<3:56:53, 32.30s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1334/1773 [11:53:48<3:54:51, 32.10s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1335/1773 [11:54:21<3:54:50, 32.17s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1336/1773 [11:54:52<3:53:14, 32.02s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1337/1773 [11:55:25<3:53:29, 32.13s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1338/1773 [11:55:57<3:53:05, 32.15s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1339/1773 [11:56:29<3:52:06, 32.09s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1340/1773 [11:57:00<3:50:08, 31.89s/it]                                                        {'loss': 0.2681, 'learning_rate': 2.9710560001695466e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1340/1773 [11:57:00<3:50:08, 31.89s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1341/1773 [11:57:33<3:50:45, 32.05s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1342/1773 [11:58:05<3:49:58, 32.02s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1343/1773 [11:58:38<3:52:01, 32.37s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1344/1773 [11:59:09<3:49:09, 32.05s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1345/1773 [11:59:42<3:50:09, 32.27s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1346/1773 [12:00:14<3:48:13, 32.07s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1347/1773 [12:00:46<3:49:10, 32.28s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1348/1773 [12:01:18<3:46:35, 31.99s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1349/1773 [12:01:50<3:46:40, 32.08s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1350/1773 [12:02:21<3:44:37, 31.86s/it]                                                        {'loss': 0.2595, 'learning_rate': 2.8422430516737733e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1350/1773 [12:02:21<3:44:37, 31.86s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1351/1773 [12:02:53<3:43:34, 31.79s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1352/1773 [12:03:24<3:42:12, 31.67s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1353/1773 [12:03:56<3:41:09, 31.59s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1354/1773 [12:04:27<3:40:31, 31.58s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1355/1773 [12:04:58<3:38:42, 31.39s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1356/1773 [12:05:30<3:38:52, 31.49s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1357/1773 [12:06:01<3:37:25, 31.36s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1358/1773 [12:06:33<3:37:36, 31.46s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1359/1773 [12:07:04<3:36:28, 31.37s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1360/1773 [12:07:37<3:38:38, 31.76s/it]                                                        {'loss': 0.245, 'learning_rate': 2.7158207358540212e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1360/1773 [12:07:37<3:38:38, 31.76s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1361/1773 [12:08:08<3:37:25, 31.66s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1362/1773 [12:08:40<3:38:03, 31.83s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1363/1773 [12:09:12<3:36:33, 31.69s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1364/1773 [12:09:43<3:35:41, 31.64s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1365/1773 [12:10:14<3:34:10, 31.50s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1366/1773 [12:10:46<3:33:14, 31.44s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1367/1773 [12:11:17<3:33:21, 31.53s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1368/1773 [12:11:48<3:31:50, 31.38s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1369/1773 [12:12:20<3:31:41, 31.44s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1370/1773 [12:12:51<3:30:40, 31.37s/it]                                                        {'loss': 0.2653, 'learning_rate': 2.591831276736806e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1370/1773 [12:12:51<3:30:40, 31.37s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1371/1773 [12:13:23<3:31:42, 31.60s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1372/1773 [12:13:55<3:30:43, 31.53s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1373/1773 [12:14:27<3:32:11, 31.83s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1374/1773 [12:14:59<3:30:34, 31.67s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1375/1773 [12:15:31<3:30:46, 31.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1376/1773 [12:16:02<3:28:59, 31.59s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1377/1773 [12:16:33<3:28:00, 31.52s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1378/1773 [12:17:04<3:27:09, 31.47s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1379/1773 [12:17:36<3:26:09, 31.39s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1380/1773 [12:18:07<3:25:52, 31.43s/it]                                                        {'loss': 0.2606, 'learning_rate': 2.4703160857942697e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1380/1773 [12:18:07<3:25:52, 31.43s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1381/1773 [12:18:38<3:24:54, 31.36s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1382/1773 [12:19:10<3:24:24, 31.37s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1383/1773 [12:19:41<3:23:28, 31.30s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1384/1773 [12:20:13<3:25:02, 31.63s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1385/1773 [12:20:45<3:24:15, 31.59s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1386/1773 [12:21:17<3:25:59, 31.94s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1387/1773 [12:21:49<3:25:27, 31.94s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1388/1773 [12:22:23<3:27:30, 32.34s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1389/1773 [12:22:57<3:30:27, 32.88s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1390/1773 [12:23:29<3:29:20, 32.80s/it]                                                        {'loss': 0.2734, 'learning_rate': 2.351315748113078e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1390/1773 [12:23:29<3:29:20, 32.80s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1391/1773 [12:24:01<3:26:27, 32.43s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1392/1773 [12:24:34<3:26:39, 32.55s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1393/1773 [12:25:05<3:23:57, 32.20s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1394/1773 [12:25:38<3:24:03, 32.30s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1395/1773 [12:26:09<3:21:51, 32.04s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1396/1773 [12:26:41<3:21:43, 32.10s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1397/1773 [12:27:13<3:19:54, 31.90s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1398/1773 [12:27:45<3:19:20, 31.89s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1399/1773 [12:28:16<3:17:43, 31.72s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1400/1773 [12:28:47<3:16:07, 31.55s/it]                                                        {'loss': 0.2718, 'learning_rate': 2.2348700088393436e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1400/1773 [12:28:47<3:16:07, 31.55s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1401/1773 [12:29:19<3:16:05, 31.63s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1402/1773 [12:29:50<3:14:25, 31.44s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1403/1773 [12:30:22<3:15:13, 31.66s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1404/1773 [12:30:54<3:14:32, 31.63s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1405/1773 [12:31:27<3:16:14, 32.00s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1406/1773 [12:31:58<3:14:35, 31.81s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1407/1773 [12:32:30<3:15:07, 31.99s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1408/1773 [12:33:02<3:13:29, 31.81s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1409/1773 [12:33:34<3:12:52, 31.79s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1410/1773 [12:34:05<3:11:31, 31.66s/it]                                                        {'loss': 0.2525, 'learning_rate': 2.121017759904056e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1410/1773 [12:34:05<3:11:31, 31.66s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1411/1773 [12:34:36<3:10:01, 31.50s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1412/1773 [12:35:08<3:10:21, 31.64s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1413/1773 [12:35:39<3:08:50, 31.47s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1414/1773 [12:36:11<3:09:45, 31.72s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1415/1773 [12:36:43<3:08:49, 31.65s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1416/1773 [12:37:15<3:09:41, 31.88s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1417/1773 [12:37:47<3:08:10, 31.71s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1418/1773 [12:38:19<3:08:05, 31.79s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1419/1773 [12:38:50<3:06:39, 31.64s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1420/1773 [12:39:21<3:06:04, 31.63s/it]                                                        {'loss': 0.2529, 'learning_rate': 2.009797027033501e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1420/1773 [12:39:21<3:06:04, 31.63s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1421/1773 [12:39:53<3:04:59, 31.53s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1422/1773 [12:40:24<3:03:46, 31.42s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1423/1773 [12:40:56<3:03:44, 31.50s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1424/1773 [12:41:27<3:02:29, 31.37s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1425/1773 [12:41:59<3:03:04, 31.56s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1426/1773 [12:42:30<3:01:58, 31.47s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1427/1773 [12:43:03<3:03:33, 31.83s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1428/1773 [12:43:34<3:02:07, 31.67s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1429/1773 [12:44:06<3:02:58, 31.91s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1430/1773 [12:44:38<3:01:23, 31.73s/it]                                                        {'loss': 0.258, 'learning_rate': 1.9012449570489744e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1430/1773 [12:44:38<3:01:23, 31.73s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1431/1773 [12:45:09<3:00:51, 31.73s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1432/1773 [12:45:41<2:59:24, 31.57s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1433/1773 [12:46:12<2:58:36, 31.52s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1434/1773 [12:46:44<2:58:06, 31.52s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1435/1773 [12:47:15<2:56:37, 31.35s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1436/1773 [12:47:46<2:56:42, 31.46s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1437/1773 [12:48:17<2:55:46, 31.39s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1438/1773 [12:48:50<2:56:23, 31.59s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1439/1773 [12:49:21<2:55:44, 31.57s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1440/1773 [12:49:54<2:57:11, 31.93s/it]                                                        {'loss': 0.2464, 'learning_rate': 1.795397805460053e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1440/1773 [12:49:54<2:57:11, 31.93s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1441/1773 [12:50:25<2:55:30, 31.72s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1442/1773 [12:50:57<2:55:51, 31.88s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1443/1773 [12:51:28<2:54:12, 31.67s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1444/1773 [12:52:00<2:53:38, 31.67s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1445/1773 [12:52:31<2:52:37, 31.58s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1446/1773 [12:53:03<2:52:03, 31.57s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1447/1773 [12:53:35<2:51:31, 31.57s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1448/1773 [12:54:06<2:50:12, 31.42s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1449/1773 [12:54:37<2:50:15, 31.53s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1450/1773 [12:55:09<2:49:00, 31.40s/it]                                                        {'loss': 0.258, 'learning_rate': 1.692290924355533e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1450/1773 [12:55:09<2:49:00, 31.40s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1451/1773 [12:55:40<2:48:58, 31.49s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1452/1773 [12:56:11<2:48:04, 31.42s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1453/1773 [12:56:44<2:49:39, 31.81s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1454/1773 [12:57:16<2:48:17, 31.65s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1455/1773 [12:57:48<2:49:08, 31.91s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1456/1773 [12:58:19<2:47:39, 31.73s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1457/1773 [12:58:51<2:47:20, 31.77s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1458/1773 [12:59:23<2:46:08, 31.64s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1459/1773 [12:59:54<2:45:36, 31.65s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1460/1773 [13:00:26<2:44:38, 31.56s/it]                                                        {'loss': 0.2582, 'learning_rate': 1.5919587505961387e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1460/1773 [13:00:26<2:44:38, 31.56s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1461/1773 [13:00:57<2:43:40, 31.47s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1462/1773 [13:01:29<2:43:45, 31.59s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1463/1773 [13:02:00<2:42:22, 31.43s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1464/1773 [13:02:32<2:42:29, 31.55s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1465/1773 [13:03:03<2:41:14, 31.41s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1466/1773 [13:03:34<2:41:18, 31.53s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1467/1773 [13:04:06<2:40:22, 31.45s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1468/1773 [13:04:38<2:41:08, 31.70s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1469/1773 [13:05:10<2:40:25, 31.66s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1470/1773 [13:05:42<2:41:29, 31.98s/it]                                                        {'loss': 0.2636, 'learning_rate': 1.4944347943128722e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1470/1773 [13:05:42<2:41:29, 31.98s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1471/1773 [13:06:14<2:39:59, 31.79s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1472/1773 [13:06:46<2:40:03, 31.91s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1473/1773 [13:07:17<2:38:37, 31.73s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1474/1773 [13:07:49<2:38:03, 31.72s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1475/1773 [13:08:20<2:36:47, 31.57s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1476/1773 [13:08:51<2:35:51, 31.49s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1477/1773 [13:09:23<2:35:25, 31.51s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1478/1773 [13:09:54<2:34:09, 31.35s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1479/1773 [13:10:26<2:34:14, 31.48s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1480/1773 [13:10:57<2:33:25, 31.42s/it]                                                        {'loss': 0.2565, 'learning_rate': 1.3997516277149158e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1480/1773 [13:10:57<2:33:25, 31.42s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1481/1773 [13:11:29<2:33:26, 31.53s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1482/1773 [13:12:00<2:32:48, 31.51s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1483/1773 [13:12:33<2:34:18, 31.92s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1484/1773 [13:13:04<2:32:53, 31.74s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1485/1773 [13:13:37<2:33:23, 31.96s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1486/1773 [13:14:08<2:31:55, 31.76s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1487/1773 [13:14:40<2:31:15, 31.73s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1488/1773 [13:15:11<2:30:03, 31.59s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1489/1773 [13:15:43<2:29:15, 31.53s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1490/1773 [13:16:14<2:28:40, 31.52s/it]                                                        {'loss': 0.251, 'learning_rate': 1.3079408742107702e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1490/1773 [13:16:14<2:28:40, 31.52s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1491/1773 [13:16:45<2:27:29, 31.38s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1492/1773 [13:17:17<2:27:15, 31.44s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1493/1773 [13:17:48<2:26:18, 31.35s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1494/1773 [13:18:20<2:27:08, 31.64s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1495/1773 [13:18:52<2:26:23, 31.60s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1496/1773 [13:19:24<2:26:51, 31.81s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1497/1773 [13:19:55<2:25:42, 31.68s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1498/1773 [13:20:27<2:25:26, 31.73s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1499/1773 [13:20:58<2:24:06, 31.56s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1500/1773 [13:21:30<2:23:27, 31.53s/it]                                                        {'loss': 0.27, 'learning_rate': 1.2190331978463e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1500/1773 [13:21:30<2:23:27, 31.53s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1501/1773 [13:22:01<2:22:47, 31.50s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1502/1773 [13:22:32<2:21:36, 31.35s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1503/1773 [13:23:04<2:21:29, 31.44s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1504/1773 [13:23:35<2:20:26, 31.33s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1505/1773 [13:24:07<2:20:58, 31.56s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1506/1773 [13:24:38<2:20:08, 31.49s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1507/1773 [13:25:11<2:20:48, 31.76s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1508/1773 [13:25:42<2:19:47, 31.65s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1509/1773 [13:26:14<2:19:30, 31.71s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1510/1773 [13:26:45<2:18:18, 31.55s/it]                                                        {'loss': 0.266, 'learning_rate': 1.133058293063184e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1510/1773 [13:26:45<2:18:18, 31.55s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1511/1773 [13:27:17<2:17:50, 31.57s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1512/1773 [13:27:48<2:17:03, 31.51s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1513/1773 [13:28:19<2:15:51, 31.35s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1514/1773 [13:28:51<2:15:51, 31.47s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1515/1773 [13:29:22<2:14:54, 31.37s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1516/1773 [13:29:54<2:15:21, 31.60s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1517/1773 [13:30:25<2:14:22, 31.49s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1518/1773 [13:30:58<2:15:18, 31.84s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1519/1773 [13:31:30<2:14:24, 31.75s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1520/1773 [13:32:02<2:14:25, 31.88s/it]                                                        {'loss': 0.2706, 'learning_rate': 1.0500448747812209e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1520/1773 [13:32:02<2:14:25, 31.88s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1521/1773 [13:32:33<2:12:58, 31.66s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1522/1773 [13:33:04<2:12:19, 31.63s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1523/1773 [13:33:36<2:11:21, 31.53s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1524/1773 [13:34:07<2:10:45, 31.51s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1525/1773 [13:34:39<2:10:20, 31.53s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1526/1773 [13:35:10<2:09:07, 31.37s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1527/1773 [13:35:41<2:08:54, 31.44s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1528/1773 [13:36:12<2:07:57, 31.34s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1529/1773 [13:36:45<2:08:44, 31.66s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1530/1773 [13:37:16<2:08:10, 31.65s/it]                                                        {'loss': 0.2567, 'learning_rate': 9.700206688077707e-07, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1530/1773 [13:37:17<2:08:10, 31.65s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1531/1773 [13:37:49<2:08:38, 31.89s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1532/1773 [13:38:20<2:07:32, 31.76s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1533/1773 [13:38:52<2:07:10, 31.79s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1534/1773 [13:39:24<2:05:59, 31.63s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1535/1773 [13:39:55<2:05:23, 31.61s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1536/1773 [13:40:26<2:04:29, 31.52s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1537/1773 [13:40:58<2:03:31, 31.41s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1538/1773 [13:41:29<2:03:26, 31.52s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1539/1773 [13:42:00<2:02:13, 31.34s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1540/1773 [13:42:32<2:02:35, 31.57s/it]                                                        {'loss': 0.2532, 'learning_rate': 8.930124025775677e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1540/1773 [13:42:32<2:02:35, 31.57s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1541/1773 [13:43:04<2:01:37, 31.46s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1542/1773 [13:43:36<2:02:33, 31.83s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1543/1773 [13:44:08<2:01:39, 31.73s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1544/1773 [13:44:40<2:01:52, 31.93s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1545/1773 [13:45:11<2:00:32, 31.72s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1546/1773 [13:45:43<2:00:23, 31.82s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1547/1773 [13:46:15<1:59:09, 31.64s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1548/1773 [13:46:46<1:58:29, 31.60s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1549/1773 [13:47:17<1:57:37, 31.51s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1550/1773 [13:47:48<1:56:29, 31.34s/it]                                                        {'loss': 0.2644, 'learning_rate': 8.19045796225969e-07, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1550/1773 [13:47:48<1:56:29, 31.34s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1551/1773 [13:48:20<1:56:30, 31.49s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1552/1773 [13:48:51<1:55:30, 31.36s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1553/1773 [13:49:23<1:55:28, 31.49s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1554/1773 [13:49:54<1:54:46, 31.45s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1555/1773 [13:50:27<1:55:42, 31.85s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1556/1773 [13:50:59<1:54:38, 31.70s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1557/1773 [13:51:31<1:54:39, 31.85s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1558/1773 [13:52:02<1:53:33, 31.69s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1559/1773 [13:52:34<1:52:50, 31.64s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1560/1773 [13:53:05<1:51:53, 31.52s/it]                                                        {'loss': 0.2549, 'learning_rate': 7.481455539986437e-07, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1560/1773 [13:53:05<1:51:53, 31.52s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1561/1773 [13:53:36<1:51:04, 31.44s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1562/1773 [13:54:08<1:50:49, 31.52s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1563/1773 [13:54:39<1:49:46, 31.37s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1564/1773 [13:55:10<1:49:29, 31.43s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1565/1773 [13:55:42<1:48:42, 31.36s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1566/1773 [13:56:14<1:48:57, 31.58s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1567/1773 [13:56:45<1:48:11, 31.51s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1568/1773 [13:57:17<1:48:32, 31.77s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1569/1773 [13:57:49<1:47:26, 31.60s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1570/1773 [13:58:20<1:46:56, 31.61s/it]                                                        {'loss': 0.2843, 'learning_rate': 6.803353560005488e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1570/1773 [13:58:20<1:46:56, 31.61s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1571/1773 [13:58:52<1:46:05, 31.51s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1572/1773 [13:59:23<1:45:17, 31.43s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1573/1773 [13:59:54<1:44:56, 31.48s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1574/1773 [14:00:25<1:43:53, 31.32s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1575/1773 [14:00:57<1:43:38, 31.41s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1576/1773 [14:01:28<1:42:46, 31.30s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1577/1773 [14:02:01<1:43:43, 31.75s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1578/1773 [14:02:32<1:42:44, 31.61s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1579/1773 [14:03:04<1:42:44, 31.78s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1580/1773 [14:03:35<1:41:39, 31.60s/it]                                                        {'loss': 0.2662, 'learning_rate': 6.156378502869742e-07, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1580/1773 [14:03:35<1:41:39, 31.60s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1581/1773 [14:04:07<1:40:57, 31.55s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1582/1773 [14:04:38<1:40:14, 31.49s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1583/1773 [14:05:09<1:39:19, 31.37s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1584/1773 [14:05:41<1:39:06, 31.47s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1585/1773 [14:06:12<1:38:06, 31.31s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1586/1773 [14:06:43<1:37:49, 31.39s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1587/1773 [14:07:15<1:37:02, 31.31s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1588/1773 [14:07:47<1:37:29, 31.62s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1589/1773 [14:08:18<1:36:46, 31.56s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1590/1773 [14:08:51<1:36:58, 31.80s/it]                                                        {'loss': 0.2672, 'learning_rate': 5.540746452992684e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1590/1773 [14:08:51<1:36:58, 31.80s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1591/1773 [14:09:22<1:35:57, 31.64s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1592/1773 [14:09:53<1:35:15, 31.58s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1593/1773 [14:10:24<1:34:13, 31.41s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1594/1773 [14:10:56<1:33:30, 31.34s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1595/1773 [14:11:27<1:33:14, 31.43s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1596/1773 [14:11:58<1:32:19, 31.29s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1597/1773 [14:12:30<1:31:56, 31.34s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1598/1773 [14:13:01<1:31:17, 31.30s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1599/1773 [14:13:33<1:31:43, 31.63s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1600/1773 [14:14:05<1:31:03, 31.58s/it]                                                        {'loss': 0.2601, 'learning_rate': 4.956663026478015e-07, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1600/1773 [14:14:05<1:31:03, 31.58s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1601/1773 [14:14:37<1:31:06, 31.78s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1602/1773 [14:15:08<1:30:07, 31.62s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1603/1773 [14:15:40<1:29:45, 31.68s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1604/1773 [14:16:11<1:28:45, 31.51s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1605/1773 [14:16:42<1:28:01, 31.44s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1606/1773 [14:17:14<1:27:17, 31.36s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1607/1773 [14:17:45<1:26:24, 31.23s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1608/1773 [14:18:16<1:26:12, 31.35s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1609/1773 [14:18:47<1:25:26, 31.26s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1610/1773 [14:19:19<1:25:25, 31.44s/it]                                                        {'loss': 0.2562, 'learning_rate': 4.404323302445457e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1610/1773 [14:19:19<1:25:25, 31.44s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1611/1773 [14:19:50<1:24:46, 31.40s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1612/1773 [14:20:23<1:25:12, 31.75s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1613/1773 [14:20:54<1:24:22, 31.64s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1614/1773 [14:21:27<1:24:16, 31.80s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1615/1773 [14:21:58<1:23:29, 31.71s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1616/1773 [14:22:30<1:23:01, 31.73s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1617/1773 [14:23:01<1:22:03, 31.56s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1618/1773 [14:23:32<1:21:17, 31.47s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1619/1773 [14:24:03<1:20:34, 31.39s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1620/1773 [14:24:33<1:18:25, 30.76s/it]                                                        {'loss': 0.2683, 'learning_rate': 3.883911757876058e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1620/1773 [14:24:33<1:18:25, 30.76s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1621/1773 [14:25:02<1:16:46, 30.31s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1622/1773 [14:25:31<1:15:28, 29.99s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1623/1773 [14:26:00<1:14:24, 29.77s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1624/1773 [14:26:30<1:13:36, 29.64s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1625/1773 [14:26:59<1:12:50, 29.53s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1626/1773 [14:27:29<1:12:21, 29.53s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1627/1773 [14:27:58<1:11:44, 29.48s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1628/1773 [14:28:27<1:11:09, 29.45s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–[rank1]:[E ProcessGroupNCCL.cpp:523] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=327, OpType=ALLGATHER, NumelIn=1, NumelOut=5, Timeout(ms)=1800000) ran for 1800341 milliseconds before timing out.
[rank1]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E ProcessGroupNCCL.cpp:1182] [Rank 1] NCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=327, OpType=ALLGATHER, NumelIn=1, NumelOut=5, Timeout(ms)=1800000) ran for 1800341 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f23d9efdd87 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f23db0a56e6 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f23db0a8c3d in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f23db0a9839 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3e95 (0x7f2424db4e95 in /home/data_llm/anaconda3/envs/moellava/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f2426816609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f24265e1353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 1] NCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=327, OpType=ALLGATHER, NumelIn=1, NumelOut=5, Timeout(ms)=1800000) ran for 1800341 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f23d9efdd87 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f23db0a56e6 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f23db0a8c3d in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f23db0a9839 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3e95 (0x7f2424db4e95 in /home/data_llm/anaconda3/envs/moellava/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f2426816609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f24265e1353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f23d9efdd87 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7f23dadffb11 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd3e95 (0x7f2424db4e95 in /home/data_llm/anaconda3/envs/moellava/bin/../lib/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f2426816609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f24265e1353 in /lib/x86_64-linux-gnu/libc.so.6)

[rank2]:[E ProcessGroupNCCL.cpp:523] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=327, OpType=ALLGATHER, NumelIn=1, NumelOut=5, Timeout(ms)=1800000) ran for 1800373 milliseconds before timing out.
[rank2]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E ProcessGroupNCCL.cpp:1182] [Rank 2] NCCL watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=327, OpType=ALLGATHER, NumelIn=1, NumelOut=5, Timeout(ms)=1800000) ran for 1800373 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f7e26a78d87 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f7e27c206e6 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f7e27c23c3d in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f7e27c24839 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3e95 (0x7f7e7192fe95 in /home/data_llm/anaconda3/envs/moellava/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f7e73391609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f7e7315c353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 2] NCCL watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=327, OpType=ALLGATHER, NumelIn=1, NumelOut=5, Timeout(ms)=1800000) ran for 1800373 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f7e26a78d87 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f7e27c206e6 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f7e27c23c3d in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f7e27c24839 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3e95 (0x7f7e7192fe95 in /home/data_llm/anaconda3/envs/moellava/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f7e73391609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f7e7315c353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f7e26a78d87 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7f7e2797ab11 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd3e95 (0x7f7e7192fe95 in /home/data_llm/anaconda3/envs/moellava/bin/../lib/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f7e73391609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f7e7315c353 in /lib/x86_64-linux-gnu/libc.so.6)

[rank3]:[E ProcessGroupNCCL.cpp:523] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=327, OpType=ALLGATHER, NumelIn=1, NumelOut=5, Timeout(ms)=1800000) ran for 1800662 milliseconds before timing out.
[rank3]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E ProcessGroupNCCL.cpp:1182] [Rank 3] NCCL watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=327, OpType=ALLGATHER, NumelIn=1, NumelOut=5, Timeout(ms)=1800000) ran for 1800662 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f59339a0d87 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f5934b486e6 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f5934b4bc3d in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f5934b4c839 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3e95 (0x7f597e857e95 in /home/data_llm/anaconda3/envs/moellava/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f59802b9609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f5980084353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 3] NCCL watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=327, OpType=ALLGATHER, NumelIn=1, NumelOut=5, Timeout(ms)=1800000) ran for 1800662 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f59339a0d87 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f5934b486e6 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f5934b4bc3d in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f5934b4c839 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3e95 (0x7f597e857e95 in /home/data_llm/anaconda3/envs/moellava/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f59802b9609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f5980084353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f59339a0d87 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7f59348a2b11 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd3e95 (0x7f597e857e95 in /home/data_llm/anaconda3/envs/moellava/bin/../lib/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f59802b9609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f5980084353 in /lib/x86_64-linux-gnu/libc.so.6)

[rank4]:[E ProcessGroupNCCL.cpp:523] [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=327, OpType=ALLGATHER, NumelIn=1, NumelOut=5, Timeout(ms)=1800000) ran for 1800855 milliseconds before timing out.
[rank4]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank4]:[E ProcessGroupNCCL.cpp:1182] [Rank 4] NCCL watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=327, OpType=ALLGATHER, NumelIn=1, NumelOut=5, Timeout(ms)=1800000) ran for 1800855 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f02e1cdbd87 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f02e2e836e6 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f02e2e86c3d in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f02e2e87839 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3e95 (0x7f032cb92e95 in /home/data_llm/anaconda3/envs/moellava/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f032e5f4609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f032e3bf353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 4] NCCL watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=327, OpType=ALLGATHER, NumelIn=1, NumelOut=5, Timeout(ms)=1800000) ran for 1800855 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f02e1cdbd87 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x7f02e2e836e6 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x7f02e2e86c3d in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f02e2e87839 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3e95 (0x7f032cb92e95 in /home/data_llm/anaconda3/envs/moellava/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f032e5f4609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f032e3bf353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1186 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f02e1cdbd87 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xdf6b11 (0x7f02e2bddb11 in /home/data_llm/anaconda3/envs/moellava/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd3e95 (0x7f032cb92e95 in /home/data_llm/anaconda3/envs/moellava/bin/../lib/libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f032e5f4609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f032e3bf353 in /lib/x86_64-linux-gnu/libc.so.6)

[2024-04-23 14:16:35,415] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1080349
[2024-04-23 14:16:39,463] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1080350
[2024-04-23 14:16:39,495] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1080351
[2024-04-23 14:16:39,521] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1080352
[2024-04-23 14:16:39,521] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1080353
[2024-04-23 14:16:39,547] [ERROR] [launch.py:321:sigkill_handler] ['/home/data_llm/anaconda3/envs/moellava/bin/python', '-u', '/home/data_llm/FoodHealthMMLLM/moellava/train/train_xformers.py', '--local_rank=4', '--moe_enable', 'True', '--num_experts', '4', '--top_k_experts', '2', '--capacity_factor', '1.5', '--moe_mode', 'sparse', '--use_residual', 'False', '--router_aux_loss_coef', '0.01', '--train_modules', 'fc1', 'fc2', 'wg', '--deepspeed', '../../zero2_offload.json', '--model_name_or_path', '/mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-nutv2', '--version', 'phi', '--data_path', '/mnt/data_llm/json_file/101_train_prompt1.json', '/mnt/data_llm/json_file/172_train_prompt1.json', '--image_folder', '/media/LLM_data/food_recognition_dataset', '--image_tower', 'openai/clip-vit-large-patch14-336', '--image_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--check_point_file_name', '/mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-nutv2-moe-v1.json', '--output_dir', '/mnt/data_llm/model/checkpoints/checkpoints-phi-2.7b-nutv2-moe-v1', '--num_train_epochs', '1', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'epoch', '--save_total_limit', '30', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--tf32', 'False', '--model_max_length', '512', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '16', '--lazy_preprocess', 'True', '--report_to', 'tensorboard', '--cache_dir', '/media/fast_data/huggingface/hub/'] exits with return code = -6
